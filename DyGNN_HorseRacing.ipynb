{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnmmudHdhqBX",
        "outputId": "40e3e7a7-b50a-421d-db8e-39b4f7a0048e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math as math\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from typing import Dict, Optional, Tuple, Any, List, Union\n",
        "import copy\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import logging\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass, field\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import spearmanr\n",
        "import sys as sys\n",
        "import pickle as pkl\n",
        "from datetime import datetime\n",
        "import json\n",
        "import collections\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !pip install torchviz\n",
        "# from torchviz import make_dot\n",
        "\n",
        "# !pip install memory_profiler\n",
        "# %load_ext memory_profiler\n",
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.13-cp311-cp311-linux_x86_64.whl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr3ikEdNjOzq"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jszvF6QC2ulN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_device(prefer_device: Optional[str] = None) -> torch.device:\n",
        "    \"\"\"\n",
        "    Detect and return available compute device with priority order:\n",
        "    TPU > GPU > CPU\n",
        "\n",
        "    Args:\n",
        "        prefer_device: Optional preferred device ('tpu', 'gpu', 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        torch.device: Best available device\n",
        "    \"\"\"\n",
        "    device_order = []\n",
        "\n",
        "    # Determine detection order based on preference\n",
        "    if prefer_device:\n",
        "        device_order.append(prefer_device.lower())\n",
        "    device_order += ['tpu', 'cuda', 'mps', 'cpu']\n",
        "\n",
        "    for device_type in device_order:\n",
        "        try:\n",
        "            if device_type == 'tpu':\n",
        "                import torch_xla\n",
        "                import torch_xla.core.xla_model as xm\n",
        "\n",
        "                device = xm.xla_device()\n",
        "                print(f\"Using TPU: {device}\")\n",
        "                return device\n",
        "\n",
        "            elif device_type == 'cuda' and torch.cuda.is_available():\n",
        "                device = torch.device(\"cuda\")\n",
        "                print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
        "                return device\n",
        "\n",
        "            elif device_type == 'mps' and torch.backends.mps.is_available():\n",
        "                device = torch.device(\"mps\")\n",
        "                print(\"Using Apple MPS\")\n",
        "                return device\n",
        "\n",
        "            elif device_type == 'cpu':\n",
        "                device = torch.device(\"cpu\")\n",
        "                print(\"Using CPU\")\n",
        "                return device\n",
        "\n",
        "        except ImportError:\n",
        "            continue\n",
        "\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "def set_seed(seed: int, deterministic: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Set random seeds for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        seed: Random seed value\n",
        "        deterministic: Enable deterministic algorithms (may impact performance)\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        if deterministic:\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Seeds initialized to {seed} with {'deterministic' if deterministic else 'normal'} mode\")\n",
        "\n",
        "def save_training_checkpoint(\n",
        "    session_dir: str,\n",
        "    batch_idx: int,\n",
        "    model: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    stats: Dict[str, Any],\n",
        "):\n",
        "    \"\"\"\n",
        "    Saves training checkpoint, statistics, and generates plots.\n",
        "\n",
        "    Args:\n",
        "        session_dir: Directory created by create_directory_training_session\n",
        "        batch_idx: Current batch_idx number\n",
        "        model: Model to save\n",
        "        optimizer: Optimizer to save\n",
        "        stats: Dictionary containing training statistics\n",
        "    \"\"\"\n",
        "    checkpoint_dir = os.path.join(session_dir, \"checkpoints\")\n",
        "    plots_dir = os.path.join(session_dir, \"plots\")\n",
        "    os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Save model checkpoint\n",
        "    checkpoint = {\n",
        "        'batch_idx': batch_idx,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'stats': stats,\n",
        "        'model_type': type(model).__name__,\n",
        "    }\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_{batch_idx}.pt\")\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # 2. Save statistics to JSON\n",
        "    stats_path = os.path.join(session_dir, \"training_stats.json\")\n",
        "    with open(stats_path, 'w') as f:\n",
        "        json.dump(stats, f, indent=4)\n",
        "\n",
        "    # 3. Generate and save plots\n",
        "    metrics_to_plot = [\n",
        "                        ('loss', 'Training Loss'),\n",
        "                        ('predicted_earnings', 'Predicted Earnings (Probabilistic)'),\n",
        "                        ('predicted_earnings_greedy', 'Predicted Earnings (Greedy)'),\n",
        "                        ('predicted_earnings_value', 'Predicted Earnings (Value)'),\n",
        "                        ('roi_kelly', 'ROI Kelly Betting Method'),\n",
        "                        ('accuracy', 'Accuracy'),\n",
        "                        ('hrn', 'HRN'),\n",
        "                        ('spearman', 'Spearman Correlation'),\n",
        "                        # ('computation_depth', 'Computation Depth'),\n",
        "                        # ('n_nodes', 'Number of Nodes'),\n",
        "                        ('batch_time', 'Batch Time'),\n",
        "                        ('lr', 'Learning Rate'),\n",
        "                        ]\n",
        "\n",
        "    plot_path = os.path.join(plots_dir, f\"training_plots_{batch_idx}.png\")\n",
        "    plot_training_stats(training_stats = stats,\n",
        "                       window_size = 50,\n",
        "                       config = None,\n",
        "                       plots_dir = plot_path,\n",
        "                       metrics = metrics_to_plot)\n",
        "    print('Successfully saved model')\n",
        "\n",
        "\n",
        "def load_training_checkpoint(\n",
        "    checkpoint_path: str,\n",
        "    model: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        ") -> Tuple[int, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Loads a training checkpoint from a specific file path.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Full path to the checkpoint file\n",
        "        model: Model instance to load weights into\n",
        "        optimizer: Optimizer instance to load state into\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - int: The batch index of the loaded checkpoint\n",
        "        - dict: The statistics dictionary from the checkpoint\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If specified checkpoint doesn't exist\n",
        "        TypeError: If saved model type doesn't match current model type\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
        "\n",
        "    # Determine device to load on\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:  # Model has no parameters\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    # Load checkpoint\n",
        "    with torch.serialization.safe_globals([collections.defaultdict, list]):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Verify model compatibility\n",
        "    saved_model_type = checkpoint.get(\"model_type\")\n",
        "    current_model_type = type(model).__name__\n",
        "    if saved_model_type != current_model_type:\n",
        "        raise TypeError(\n",
        "            f\"Model type mismatch: Saved model '{saved_model_type}', \"\n",
        "            f\"Current model '{current_model_type}'\"\n",
        "        )\n",
        "\n",
        "    # Load states\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    if optimizer is not None:\n",
        "      optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "    return checkpoint[\"batch_idx\"], checkpoint[\"stats\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxwaAifUVbKg"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class PlotConfig:\n",
        "    \"\"\"Configuration for plotting training statistics\"\"\"\n",
        "    figsize: tuple = (10, 6)\n",
        "    linewidth: float = 2.0\n",
        "    fontsize: int = 12\n",
        "    dpi: int = 100\n",
        "    colors: List[str] = field(default_factory=lambda: ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
        "                                                       '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']) # Use default_factory to create a new list for each instance\n",
        "\n",
        "def smooth_data(data: np.ndarray, window_size: int = 20) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Smooth data using a simple moving average with proper edge handling.\n",
        "\n",
        "    Args:\n",
        "        data: Input data array\n",
        "        window_size: Size of the moving average window\n",
        "\n",
        "    Returns:\n",
        "        Smoothed data array\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to float64 only if data is numeric (int or float)\n",
        "    # and not already float64, use float32 to avoid overflow with very large integers\n",
        "    if not isinstance(data, np.ndarray) and (isinstance(data[0], int) or isinstance(data[0], float)) and not isinstance(data[0], np.float64):\n",
        "        try:\n",
        "            data = np.array(data, dtype=np.float32) # Use float32\n",
        "        except OverflowError:\n",
        "            # If float32 still overflows, use float64 with scaling if necessary\n",
        "            data = np.array(data, dtype=np.float64) / 1e10  # Scale down\n",
        "\n",
        "    if window_size > len(data):\n",
        "        raise ValueError(\"Window size cannot be larger than data length\")\n",
        "\n",
        "    # Create a masked array where NaNs are masked\n",
        "    masked_data = np.ma.masked_array(data, np.isnan(data))\n",
        "\n",
        "    # Perform convolution on the masked array\n",
        "    result = np.convolve(masked_data.filled(0), np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "    # Adjust the result to account for the masked values\n",
        "    mask = np.convolve(~masked_data.mask, np.ones(window_size), mode='valid')\n",
        "    result = np.ma.masked_array(result, mask == 0)\n",
        "\n",
        "    return result\n",
        "\n",
        "def plot_metric(ax: plt.Axes, x: np.ndarray, y: np.ndarray,\n",
        "                label: str, color: str, config: PlotConfig) -> None:\n",
        "    \"\"\"\n",
        "    Helper function to plot a single metric.\n",
        "    \"\"\"\n",
        "    ax.plot(x, y, label=label, color=color,\n",
        "            linewidth=config.linewidth)\n",
        "    ax.set_xlabel('Training Step', fontsize=config.fontsize)\n",
        "    ax.set_ylabel(label, fontsize=config.fontsize)\n",
        "    ax.set_title(f'{label} Over Training Steps', fontsize=config.fontsize+2)\n",
        "    ax.legend(fontsize=config.fontsize-2)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "def plot_training_stats(training_stats: Dict[str, List[float]],\n",
        "                       window_size: int = 50,\n",
        "                       config: Optional[PlotConfig] = None,\n",
        "                       plots_dir: str = None,\n",
        "                       metrics: Dict[str, List] = None) -> None:\n",
        "    \"\"\"\n",
        "    Plot training statistics with proper smoothing and visualization.\n",
        "\n",
        "    Args:\n",
        "        training_stats: Dictionary containing training metrics\n",
        "        window_size: Smoothing window size\n",
        "        config: Plot configuration object\n",
        "    \"\"\"\n",
        "\n",
        "    if config is None:\n",
        "        config = PlotConfig()\n",
        "\n",
        "    # Smooth all metrics\n",
        "    smoothed_stats = {\n",
        "        key: smooth_data(values, window_size)\n",
        "        for key, values in training_stats.items()\n",
        "    }\n",
        "\n",
        "    # Create training steps array\n",
        "    training_steps = np.arange(len(smoothed_stats['loss']))\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(math.ceil(len(metrics)/3), 3, figsize=(16, 22), dpi=config.dpi)\n",
        "\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    id = 0\n",
        "    for idx, (metric, label) in enumerate(metrics):\n",
        "        if metric in smoothed_stats:\n",
        "            plot_metric(axes[id], np.arange(len(smoothed_stats[metric])),\n",
        "                       smoothed_stats[metric], label,\n",
        "                       config.colors[idx % len(config.colors)], config)\n",
        "            id+=1\n",
        "    # Adjust layout and show\n",
        "    plt.tight_layout()\n",
        "    if plots_dir:\n",
        "        plt.savefig(plots_dir)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKHj-XwgecPj"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3c8Qguc7TMT"
      },
      "outputs": [],
      "source": [
        "class HorseRacingDataset(Dataset):\n",
        "    \"\"\"Dataset class for horse racing prediction tasks\"\"\"\n",
        "\n",
        "    def __init__(self, data_config):\n",
        "        \"\"\"\n",
        "        Initialize dataset with configuration\n",
        "\n",
        "        Args:\n",
        "            data_config: Data configuration parameters\n",
        "        \"\"\"\n",
        "        self.config = data_config\n",
        "        self.embedding_dict: Dict = {}\n",
        "\n",
        "        # Load and preprocess data\n",
        "        self._load_data()\n",
        "        self._preprocess_data()\n",
        "        self._validate_data()\n",
        "\n",
        "        logger.info(f\"Dataset initialized with {len(self)} races\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Total number of races in dataset\"\"\"\n",
        "        return len(self.df_races)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple:\n",
        "        \"\"\"Get item with robust error handling and validation\"\"\"\n",
        "        for attempt in range(100):  # Max 100 attempts to find valid sample\n",
        "            try:\n",
        "                i = self._get_item(idx)\n",
        "                return i\n",
        "            except (KeyError, IndexError, ValueError) as e:\n",
        "                logger.warning(f\"Error processing index {idx}: {str(e)}\")\n",
        "                idx = (idx + 1) % len(self)\n",
        "\n",
        "        raise RuntimeError(f\"Failed to find valid sample after 100 attempts starting from index {idx}\")\n",
        "\n",
        "    def _load_data(self) -> None:\n",
        "        \"\"\"Load and filter raw data files\"\"\"\n",
        "        try:\n",
        "            self.df_races = self._load_csv(\"df_races_input_w_datetime.csv\", sort_by = ['crid']).drop(columns = self.config.cols_races_to_drop)\n",
        "            self.df_results = self._load_csv(\"df_results.csv\", sort_by = ['crid', 'hid']).drop(columns = self.config.cols_results_to_drop)\n",
        "            self.df_race_horse = self._load_csv(\"df_race_horse_input.csv\", sort_by = ['crid', 'hid']).drop(columns = self.config.cols_horses_to_drop)\n",
        "            # self.df_races = self._load_csv(\"df_races_input_w_datetime_10000.csv\", sort_by = ['crid']).drop(columns = self.config.cols_races_to_drop)\n",
        "            # self.df_results = self._load_csv(\"df_results_10000.csv\", sort_by = ['crid', 'hid']).drop(columns = self.config.cols_results_to_drop)\n",
        "            # self.df_race_horse = self._load_csv(\"df_race_horse_input_10000.csv\", sort_by = ['crid', 'hid']).drop(columns = self.config.cols_horses_to_drop)\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logger.error(f\"Data loading failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _load_csv(self, filename: str, sort_by: list) -> pd.DataFrame:\n",
        "        \"\"\"Load and filter CSV file with memory optimization\"\"\"\n",
        "        filepath = os.path.join(self.config.data_folder, filename)\n",
        "        df = pd.read_csv(filepath, sep=\";\",\n",
        "                        dtype={'crid': 'int32', 'hid': 'int32'})\n",
        "        # display(df)\n",
        "        return df.sort_values(sort_by).reset_index(drop=True)\n",
        "\n",
        "    def _preprocess_data(self) -> None:\n",
        "        \"\"\"Preprocess and merge datasets\"\"\"\n",
        "        if self.config.input_solution:\n",
        "          # Merge results to test the functionning of the model\n",
        "          cols_to_merge = ['hrn', 'hid', 'res_win'] + [f\"position_{i}\" for i in range(1, 41)]\n",
        "          self.df_results['hrn'] = self.df_results['hrn'] - 1\n",
        "          self.df_race_horse = self.df_race_horse.merge(\n",
        "              self.df_results[cols_to_merge],\n",
        "              on=['hrn', 'hid'],\n",
        "              how='left',\n",
        "              validate='one_to_one'\n",
        "          ).fillna(0)\n",
        "          self.df_results['hrn'] = self.df_results['hrn'] + 1\n",
        "\n",
        "        # Drop the crid that are lower than start_crid\n",
        "        self.df_races = self.df_races[self.df_races['crid'] >= self.config.start_crid]\n",
        "        self.df_race_horse = self.df_race_horse[self.df_race_horse['crid'] >= self.config.start_crid]\n",
        "        self.df_results = self.df_results[self.df_results['crid'] >= self.config.start_crid]\n",
        "\n",
        "        # Get dataframe, that is not standardized\n",
        "        self.df_target = self.df_results.copy()\n",
        "\n",
        "        # Prepare feature columns\n",
        "        self._setup_feature_columns()\n",
        "        self._standardize_features()\n",
        "\n",
        "        # Create lookup indices\n",
        "        self.crid_to_race_horse = self._create_crid_groups(self.df_race_horse)\n",
        "        self.crid_to_results = self._create_crid_groups(self.df_results)\n",
        "\n",
        "    def _setup_feature_columns(self) -> None:\n",
        "        \"\"\"Identify feature columns for each dataframe\"\"\"\n",
        "        id_columns = [\"rid\", \"hid\", \"crid\", 'date']\n",
        "\n",
        "        self.features = {\n",
        "            'races': [c for c in self.df_races if c not in id_columns],\n",
        "            'race_horse': [c for c in self.df_race_horse if c not in id_columns],\n",
        "            'results': [c for c in self.df_results if c not in id_columns]\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Feature counts - Races: {len(self.features['races'])}, \"\n",
        "                   f\"Race Horses: {len(self.features['race_horse'])}, \"\n",
        "                   f\"Results: {len(self.features['results'])}\")\n",
        "\n",
        "    def _standardize_features(self) -> None:\n",
        "        \"\"\"Apply z-score standardization to feature columns\"\"\"\n",
        "        self.df_races = self._zscore_standardize(self.df_races, self.features['races'])\n",
        "        self.df_race_horse = self._zscore_standardize(self.df_race_horse, self.features['race_horse'])\n",
        "        self.df_results = self._zscore_standardize(self.df_results, self.features['results'])\n",
        "\n",
        "    @staticmethod\n",
        "    def _zscore_standardize(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Safe z-score standardization with error handling\"\"\"\n",
        "        df = df.copy()\n",
        "        for col in columns:\n",
        "            try:\n",
        "                mean = df[col].mean()\n",
        "                std = df[col].std(ddof=0)\n",
        "                df[col] = (df[col] - mean) / (std + 1e-8)\n",
        "            except TypeError:\n",
        "                logger.error(f\"Non-numeric data in column {col}\")\n",
        "                raise\n",
        "        return df.fillna(0)\n",
        "\n",
        "    def _create_crid_groups(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Create efficient crid to indices mapping\"\"\"\n",
        "        return df.groupby('crid', sort=False).indices\n",
        "\n",
        "    def _validate_data(self) -> None:\n",
        "        \"\"\"Validate dataset consistency\"\"\"\n",
        "        if len(self.df_races) == 0:\n",
        "            raise ValueError(\"No races loaded in dataset\")\n",
        "\n",
        "        if not all(c in self.df_race_horse for c in ['hrn', 'hid']):\n",
        "            raise ValueError(\"Missing required columns in race horse data\")\n",
        "\n",
        "    def _get_item(self, idx: int) -> Tuple:\n",
        "        \"\"\"Core item retrieval logic\"\"\"\n",
        "        crid = self.df_races.iloc[idx]['crid']\n",
        "\n",
        "        # Get race features\n",
        "        race_features = self.df_races.iloc[idx][self.features['races']].values.astype(np.float32)\n",
        "        # Get horse data\n",
        "        horse_data, results_data = self._get_horse_data(crid)\n",
        "\n",
        "        # Get target information\n",
        "        target_data = self.df_target.loc[self.crid_to_results.get(crid, [])]\n",
        "\n",
        "        targets = {\n",
        "            'position': target_data['position'].values.astype(np.float32),\n",
        "            'decimalPrice': target_data['decimalPrice'].values.astype(np.float32),\n",
        "            'hids': target_data['hid'].values.astype(np.int32),\n",
        "            'hrn': target_data['hrn'].values.astype(np.int32),\n",
        "            'crid': target_data['crid'].values.astype(np.int32),\n",
        "            'date': self.df_races.iloc[idx]['date'].astype(np.int32)\n",
        "        }\n",
        "\n",
        "        return (race_features, horse_data, results_data) + tuple(targets.values())\n",
        "\n",
        "    def _get_horse_data(self, crid: int) -> Tuple:\n",
        "        \"\"\"Retrieve horse data for a given CRID\"\"\"\n",
        "        horse_indices = self.crid_to_race_horse.get(crid, np.array([], dtype=int))\n",
        "        results_indices = self.crid_to_results.get(crid, np.array([], dtype=int))\n",
        "\n",
        "        if len(horse_indices) != len(results_indices):\n",
        "            logger.warning(f\"Mismatched data lengths for CRID {crid}: \"\n",
        "                          f\"{len(horse_indices)} horses vs {len(results_indices)} results\")\n",
        "            return np.empty((0, len(self.features['race_horse']))), np.empty((0, len(self.features['results'])))\n",
        "\n",
        "        return (\n",
        "            self.df_race_horse.iloc[horse_indices][self.features['race_horse']].values.astype(np.float32),\n",
        "            self.df_results.iloc[results_indices][self.features['results']].values.astype(np.float32)\n",
        "        )\n",
        "\n",
        "def collate_fn(batch: List, dataset: HorseRacingDataset) -> Dict:\n",
        "    \"\"\"Efficient batch collation with padding and masking\"\"\"\n",
        "\n",
        "    def pad_array(arr: np.ndarray, target_length: int, pad_value: float = 0) -> np.ndarray:\n",
        "      \"\"\"Pad array to target length\"\"\"\n",
        "      pad_width = (0, target_length - len(arr))\n",
        "      return np.pad(arr, (pad_width, (0, 0)) if arr.ndim == 2 else pad_width,\n",
        "                  constant_values=pad_value)\n",
        "\n",
        "    batch_elements = len(batch)\n",
        "    max_horses = max(len(item[1]) for item in batch)\n",
        "\n",
        "    # Initialize storage\n",
        "    batch_dict = {\n",
        "        'race_data': [],\n",
        "        'horse_data': [],\n",
        "        'results_data': [],\n",
        "        'positions': [],\n",
        "        'prices': [],\n",
        "        'hids': [],\n",
        "        'hrn': [],\n",
        "        'crid': [],\n",
        "        'date': []\n",
        "    }\n",
        "\n",
        "    # Process each sample\n",
        "    for sample in batch:\n",
        "        race, horses, results, pos, price, hids, hrn, crid, date = sample\n",
        "        num_horses = len(horses)\n",
        "\n",
        "        # Pad features\n",
        "        batch_dict['race_data'].append(pad_array(np.tile(race, (num_horses, 1)), max_horses))\n",
        "        batch_dict['horse_data'].append(pad_array(horses, max_horses))\n",
        "        batch_dict['results_data'].append(pad_array(results, max_horses))\n",
        "\n",
        "        # Pad targets\n",
        "        batch_dict['positions'].append(pad_array(pos, max_horses, -1))\n",
        "        batch_dict['prices'].append(pad_array(price, max_horses, -1))\n",
        "        batch_dict['hids'].append(pad_array(hids, max_horses, -1))\n",
        "        batch_dict['hrn'].append(pad_array(hrn, max_horses, -1))\n",
        "        batch_dict['crid'].append(pad_array(crid, max_horses, -1))\n",
        "        batch_dict['date'].append(date)\n",
        "\n",
        "    # Convert to tensors\n",
        "    tensor_batch = {\n",
        "        k: torch.tensor(np.stack(v), dtype=torch.float32)\n",
        "        for k, v in batch_dict.items()\n",
        "    }\n",
        "    tensors_with_nan = []\n",
        "    for name, tensor in tensor_batch.items():\n",
        "        if torch.isnan(tensor).any():\n",
        "            tensors_with_nan.append(name)\n",
        "    if tensors_with_nan:\n",
        "        print(f\"Tensors with NaNs in {tensors_with_nan}\")\n",
        "    return tensor_batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b3jxHZjeyZ_"
      },
      "source": [
        "### Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47d_QgTHFgiD"
      },
      "outputs": [],
      "source": [
        "def spearman_rank_correlation(logits: torch.Tensor,\n",
        "                             positions_arrival: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Spearman's rank correlation between predicted logits and actual positions,\n",
        "    ignoring invalid entries (-1 or 40). Handles variable participant counts per race.\n",
        "\n",
        "    Args:\n",
        "        logits: Tensor of shape [batch_size, num_horses] with prediction scores\n",
        "        positions_arrival: Tensor of shape [batch_size, num_horses] with actual positions\n",
        "\n",
        "    Returns:\n",
        "        Mean Spearman's rho across batch (valid races only) as torch.Tensor\n",
        "    \"\"\"\n",
        "    device = logits.device\n",
        "    batch_size = logits.size(0)\n",
        "    correlations = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Filter valid entries for this race\n",
        "        mask = (positions_arrival[i] != -1.0) & (positions_arrival[i] != 40.0)\n",
        "        race_logits = logits[i][mask].detach().cpu().numpy()\n",
        "        race_positions = positions_arrival[i][mask].cpu().numpy()\n",
        "\n",
        "        # Skip races with <2 valid participants\n",
        "        if len(race_logits) < 2:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Generate predicted ranks from logits (higher logit = better rank)\n",
        "            pred_ranks = (-race_logits).argsort().argsort()  # Double argsort for rank\n",
        "\n",
        "            # Calculate Spearman correlation\n",
        "            rho, _ = spearmanr(pred_ranks, race_positions)\n",
        "\n",
        "            # Handle NaN/edge cases\n",
        "            if np.isnan(rho):\n",
        "                rho = 0.0\n",
        "        except:\n",
        "            rho = 0.0\n",
        "\n",
        "        correlations.append(rho)\n",
        "\n",
        "    # Return average across valid races\n",
        "    if not correlations:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "    return torch.tensor(np.mean(correlations), device=device)\n",
        "\n",
        "def loss_function_classificationV2(\n",
        "    logits: torch.Tensor,\n",
        "    batch: dict\n",
        "    ) -> tuple:\n",
        "    \"\"\"\n",
        "    Loss function with vectorized operations and reduced memory footprint.\n",
        "\n",
        "    Args:\n",
        "        logits: Model outputs (batch_size, num_tokens, 40)\n",
        "        decimal_prices: Decimal odds (batch_size, num_tokens)\n",
        "        positions_arrival: Target positions (batch_size, num_tokens)\n",
        "        crids: Race identifiers (batch_size)\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing loss tensor and various metrics\n",
        "    \"\"\"\n",
        "    decimal_prices = batch['prices']\n",
        "    positions_arrival = batch['positions']\n",
        "    device = logits.device\n",
        "    batch_size, num_tokens, n_rankings = logits.shape\n",
        "\n",
        "    # Mask invalid positions and calculate outputs\n",
        "    valid_mask = (positions_arrival != -1.0) & (positions_arrival != 40.0)\n",
        "    logits = logits.masked_fill(~valid_mask.unsqueeze(-1), -1e6)\n",
        "    output = torch.sigmoid(logits)\n",
        "\n",
        "    # Create indices tensor using vectorized operations\n",
        "    positions = positions_arrival.clamp(min=1, max=n_rankings).long() - 1\n",
        "\n",
        "    positions_expanded = positions.unsqueeze(-1)  # (batch_size, num_tokens, 1)\n",
        "    rankings_range = torch.arange(n_rankings, device=device).view(1, 1, -1)  # (1, 1, n_rankings)\n",
        "    indices = (rankings_range >= positions_expanded).float()  # (batch_size, num_tokens, n_rankings)\n",
        "    indices[valid_mask == False] = 1.0  # (batch_size, num_tokens, n_rankings\n",
        "\n",
        "    # Calculate position weights, to put as much attention on each race, independently of the amount of racers.\n",
        "    weights_by_position = torch.arange(n_rankings, 0, -1, device=device).float()\n",
        "    weights_position = valid_mask.unsqueeze(-1) * weights_by_position.view(1, 1, -1)\n",
        "    weights_position = (n_rankings**2 / 2) * weights_position / (weights_position.sum(dim=(1,2), keepdim=True) + 1e-5)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = torch.nn.functional.binary_cross_entropy(\n",
        "        output, indices, weight=weights_position\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Calculate accuracy\n",
        "        valid_positions = valid_mask & (positions_arrival == 1.0)\n",
        "        race_output_max = output[..., 0].masked_fill(~valid_mask, -float('inf'))\n",
        "        predicted_winners = race_output_max.argmax(dim=1)\n",
        "\n",
        "        # Create mask of correct targets\n",
        "        correct_target_mask = (positions_arrival == 1.0) & valid_mask\n",
        "\n",
        "        # Check if predictions match any correct target\n",
        "        batch_indices = torch.arange(batch_size, device=device)\n",
        "        correct_predictions = correct_target_mask[batch_indices, predicted_winners]\n",
        "\n",
        "        # Filter for batches with valid positions\n",
        "        valid_batches = valid_positions.any(dim=1)\n",
        "        accuracy = correct_predictions[valid_batches].float().mean()\n",
        "\n",
        "        # Total winnings\n",
        "        winnings = (output[valid_positions][:,0] / decimal_prices[valid_positions]).sum()\n",
        "\n",
        "        # Total betted amount\n",
        "        total_betted = output[valid_mask][:, 0].sum()\n",
        "\n",
        "        # Predicted earnings\n",
        "        predicted_earnings = winnings - total_betted\n",
        "\n",
        "        # Greedy earnings\n",
        "        greedy_winnings = ((positions_arrival.gather(1, predicted_winners.unsqueeze(1))==1) / decimal_prices.gather(1, predicted_winners.unsqueeze(1))).sum()\n",
        "        greedy_earnings = greedy_winnings - batch_size  # Subtract total bets\n",
        "\n",
        "        # Calculate Spearman correlation\n",
        "        spearman_results = spearmanr_kpi(output.detach().clone(), positions_arrival)\n",
        "\n",
        "    return (\n",
        "        loss,\n",
        "        accuracy.detach(),\n",
        "        predicted_earnings / batch_size,\n",
        "        greedy_earnings / batch_size,\n",
        "        total_betted / batch_size,\n",
        "        winnings / batch_size,\n",
        "        torch.tensor(spearman_results, device=device)\n",
        "    )\n",
        "\n",
        "def loss_function_first_horse_classification(\n",
        "    logits: torch.Tensor,\n",
        "    batch: dict\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Focused on predicting first-place finishes using softmax cross-entropy.\n",
        "    Horses with position -1 are excluded from the softmax, while horses with\n",
        "    position 40 are included in the softmax but do not contribute to the loss.\n",
        "\n",
        "    Args:\n",
        "        logits: Winner prediction scores (batch_size, num_horses)\n",
        "        decimal_prices: Decimal odds (batch_size, num_horses)\n",
        "        positions_arrival: Target positions (batch_size, num_horses)\n",
        "        crids: Race identifiers (batch_size)\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing loss tensor and metrics\n",
        "    \"\"\"\n",
        "\n",
        "    decimal_prices = batch['prices']\n",
        "    positions_arrival = batch['positions']\n",
        "\n",
        "    device = logits.device\n",
        "    batch_size, num_horses, n_ranks = logits.shape\n",
        "    logits = logits.squeeze(-1)\n",
        "\n",
        "    # Mask out logits for horses with position -1 (excluded from softmax)\n",
        "    mask_out = (positions_arrival == -1.0)\n",
        "    logits = logits.masked_fill(mask_out, -1e9)\n",
        "\n",
        "    # Mask for valid horses (excluding -1 and 40) to determine valid winners\n",
        "    valid_mask = (positions_arrival != -1.0)\n",
        "    winner_mask = (positions_arrival == 1.0)\n",
        "    valid_races = winner_mask.sum(dim=1) == 1  # Races with exactly one valid winner\n",
        "\n",
        "    # Convert to class indices for valid winners\n",
        "    winner_indices = winner_mask.float().argmax(dim=1)  # (batch_size,)\n",
        "\n",
        "    # Cross-entropy loss only for valid races\n",
        "    if valid_races.any():\n",
        "        logits_valid = logits[valid_races]\n",
        "        targets_valid = winner_indices[valid_races].long()\n",
        "        loss = F.cross_entropy(logits_valid, targets_valid, reduction = 'none')\n",
        "\n",
        "        n_horses_participated = valid_mask[valid_races].sum(dim=1)\n",
        "\n",
        "        # Scale loss for different amount of participants per race\n",
        "        loss = (loss / torch.log(n_horses_participated)).mean()\n",
        "    else:\n",
        "        loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def loss_function_plackett_luce(\n",
        "    logits: torch.Tensor,\n",
        "    decimal_prices: torch.Tensor,\n",
        "    positions_arrival: torch.Tensor,\n",
        "    crids: torch.Tensor,\n",
        "    penalty_weight: float = 1,\n",
        "    avg_bet_per_race: float = 0.5,\n",
        "    printit: bool = False\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Plackett-Luce loss implementation for horse racing predictions.\n",
        "    Handles up to 40 positions with dynamic computation graph optimization.\n",
        "    \"\"\"\n",
        "    device = logits.device\n",
        "    batch_size, num_horses, n_ranks = logits.shape\n",
        "    logits = logits.squeeze(-1)\n",
        "    # Mask and prepare valid rankings\n",
        "    valid_mask = (positions_arrival != -1.0) & (positions_arrival != 40.0)\n",
        "    positions = positions_arrival.clamp(min=1, max=40).long()\n",
        "\n",
        "    # ====================== Plackett-Luce Loss Core ========================\n",
        "    # Sort logits by actual positions for each race\n",
        "    adjusted_positions = torch.where(valid_mask, positions, torch.full_like(positions, 41))\n",
        "    sorted_indices = adjusted_positions.argsort(dim=1)\n",
        "\n",
        "    # Prepare sorted tensors with valid masking\n",
        "    sorted_logits = logits.gather(1, sorted_indices)\n",
        "    sorted_valid = valid_mask.gather(1, sorted_indices)\n",
        "    sorted_logits_masked = sorted_logits.masked_fill(~sorted_valid, -float('inf'))\n",
        "\n",
        "    # Compute reverse cumulative logsumexp for stability\n",
        "    reversed_logits = torch.flip(sorted_logits_masked, dims=[1])\n",
        "    reverse_cumsum = torch.logcumsumexp(reversed_logits, dim=1)\n",
        "    cum_logsumexp = torch.flip(reverse_cumsum, dims=[1])\n",
        "\n",
        "    # Calculate per-position log probabilities\n",
        "    log_probs = sorted_logits_masked - cum_logsumexp\n",
        "    valid_log_probs = log_probs * sorted_valid.float()\n",
        "\n",
        "    # Normalize by number of participants per race\n",
        "    participants_per_race = valid_mask.sum(dim=1, dtype=torch.float)  # (batch_size,)\n",
        "    race_log_likelihood = torch.nansum(valid_log_probs, dim=1) / participants_per_race\n",
        "\n",
        "    # Filter valid races (handle potential 0/0 from empty races)\n",
        "    valid_races = (valid_mask.any(dim=1)) & (participants_per_race > 0)\n",
        "    loss = -race_log_likelihood[valid_races].mean() if valid_races.any() else torch.tensor(0.0, device=device)\n",
        "    # ========================================================================\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Prediction metrics (similar to original)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        winner_mask = (positions_arrival == 1.0) & valid_mask\n",
        "\n",
        "        # Accuracy calculation\n",
        "        valid_winner_races = winner_mask.any(dim=1)\n",
        "        if valid_winner_races.any():\n",
        "            pred_winners = probs.argmax(dim=1)\n",
        "            correct = winner_mask[torch.arange(batch_size), pred_winners]\n",
        "            accuracy = correct[valid_winner_races].float().mean()\n",
        "        else:\n",
        "            accuracy = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # Betting metrics\n",
        "        if valid_winner_races.any():\n",
        "            # Probabilistic betting\n",
        "            selected_probs = probs[winner_mask]\n",
        "            selected_decimalprices = decimal_prices[winner_mask]\n",
        "            winnings_prob = (selected_probs / selected_decimalprices).sum()\n",
        "            total_betted_prob = batch_size\n",
        "\n",
        "            # Greedy betting\n",
        "            pred_winners = probs.argmax(dim=1)\n",
        "            winnings_greedy = (positions_arrival[torch.arange(batch_size), pred_winners] == 1.0)\n",
        "            decimalPrice_greedy = decimal_prices[torch.arange(batch_size), pred_winners]\n",
        "            winnings_greedy = (winnings_greedy.float() / decimalPrice_greedy).sum()\n",
        "            total_betted_greedy = batch_size\n",
        "        else:\n",
        "            winnings_prob = torch.tensor(0.0, device=device)\n",
        "            total_betted_prob = torch.tensor(0.0, device=device)\n",
        "            winnings_greedy = torch.tensor(0.0, device=device)\n",
        "            total_betted_greedy = torch.tensor(0.0, device=device)\n",
        "\n",
        "        predicted_earnings_prob = winnings_prob - total_betted_prob\n",
        "        predicted_earnings_greedy = winnings_greedy - total_betted_greedy\n",
        "\n",
        "        # Ranking correlation\n",
        "        spearman = spearman_rank_correlation(logits, positions_arrival)\n",
        "\n",
        "    return (\n",
        "        loss,\n",
        "        accuracy.detach(),\n",
        "        predicted_earnings_prob / batch_size,\n",
        "        predicted_earnings_greedy / batch_size,\n",
        "        torch.tensor(1.0, device = device),\n",
        "        winnings_prob / batch_size,\n",
        "        spearman.to(device)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo-J62NOes4d"
      },
      "source": [
        "### Embedding Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClSmn-r3iAms"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "@dataclass\n",
        "class EmbeddingState:\n",
        "    embeddings: deque\n",
        "    node_counts: deque\n",
        "\n",
        "class CustomEmbeddingManager(nn.Module):\n",
        "    \"\"\"Manages dynamic embeddings with computation graph optimization\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int = 128,\n",
        "                 max_nodes: int = 100,\n",
        "                 max_depth_nodes: int = 10,\n",
        "                 max_sequence_length: int = 5):\n",
        "        super().__init__()\n",
        "\n",
        "        # State management\n",
        "        self.embedding_states: Dict[int, EmbeddingState] = defaultdict(\n",
        "            lambda: EmbeddingState(embeddings=deque(maxlen=max_sequence_length),\n",
        "                                   node_counts=deque(maxlen=max_sequence_length)))\n",
        "\n",
        "        # Configuration\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_nodes = max_nodes\n",
        "        self.max_depth_nodes = max_depth_nodes\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "    def update_embeddings(self,\n",
        "                         horse_ids: torch.Tensor,\n",
        "                         new_embeddings: torch.Tensor,\n",
        "                         additional_nodes: int) -> None:\n",
        "        \"\"\"\n",
        "        Update embeddings in a vectorized manner\n",
        "        Args:\n",
        "            horse_ids: (batch_size, num_tokens) tensor of horse IDs\n",
        "            new_embeddings: (batch_size, num_tokens, embedding_dim) tensor\n",
        "            additional_nodes: (batch_size, num_tokens) tensor of node counts\n",
        "        \"\"\"\n",
        "        self.device = new_embeddings.device\n",
        "        batch_size, num_tokens = horse_ids.shape\n",
        "\n",
        "        # Flatten and filter valid IDs\n",
        "        mask = horse_ids != -1\n",
        "        valid_ids = horse_ids[mask].long()\n",
        "        valid_embeddings = new_embeddings[mask]\n",
        "\n",
        "        # Vectorized update\n",
        "        for hid, emb in zip(valid_ids, valid_embeddings):\n",
        "            state = self.embedding_states[hid.item()]\n",
        "            if len(state.embeddings) == self.max_sequence_length:\n",
        "              # Detach and remove reference to popped embedding\n",
        "              popped_emb = state.embeddings.popleft()\n",
        "              popped_emb = popped_emb.detach()\n",
        "              state.node_counts.popleft()\n",
        "            state.embeddings.append(emb)\n",
        "            state.node_counts.append(additional_nodes + 1)\n",
        "\n",
        "    def get_detach_flags(self, horse_ids: torch.Tensor) -> Tuple[List[bool], int, List[int]]:\n",
        "        \"\"\"\n",
        "        Calculate detachment flags considering:\n",
        "        1. Each individual node_count <= max_depth_nodes\n",
        "        2. Total of selected node_counts <= max_nodes\n",
        "        3. Prioritizes smallest node_counts first\n",
        "        \"\"\"\n",
        "        batch_size, num_tokens = horse_ids.shape\n",
        "        device = horse_ids.device\n",
        "\n",
        "        # Initialize\n",
        "        detach_flags = []\n",
        "        node_counts = []\n",
        "        hid_location = []\n",
        "\n",
        "        # Collect all valid candidates with their positions\n",
        "        valid_mask = horse_ids != -1\n",
        "        valid_indices = torch.nonzero(valid_mask, as_tuple=False)\n",
        "\n",
        "        for idx in valid_indices:\n",
        "            i, j = idx.tolist()\n",
        "            hid = horse_ids[i, j].item()\n",
        "            state = self.embedding_states.get(hid)\n",
        "\n",
        "            if not state:\n",
        "                continue\n",
        "\n",
        "            # Collect individual node counts with their positions\n",
        "            for count in state.node_counts:\n",
        "              node_counts.append(count)\n",
        "              detach_flags.append(True)\n",
        "              hid_location.append(hid)\n",
        "\n",
        "        # Sort candidates by node count (smallest first)\n",
        "        sorted_indices = sorted(range(len(node_counts)), key=lambda i: node_counts[i])\n",
        "\n",
        "        # Select candidates until we reach max_nodes\n",
        "        total = 0\n",
        "        selected = set()\n",
        "        for i, index in enumerate(sorted_indices):\n",
        "            count = node_counts[index]\n",
        "            if total + count > self.max_nodes or count > self.max_depth_nodes:\n",
        "                break\n",
        "            total += count\n",
        "            detach_flags[index] = False\n",
        "\n",
        "        return detach_flags, total, hid_location\n",
        "\n",
        "    def get_embeddings(self,\n",
        "                      horse_ids: torch.Tensor) -> Tuple[Optional[torch.Tensor],\n",
        "                                                       Optional[torch.Tensor],\n",
        "                                                       List[int]]:\n",
        "        \"\"\"\n",
        "        Retrieve embeddings with optimized detachment\n",
        "        Args:\n",
        "            horse_ids: (batch_size, num_tokens) tensor of horse IDs\n",
        "        Returns:\n",
        "            embeddings: (total_sequences, seq_len, embedding_dim) padded embeddings\n",
        "            lengths: (total_sequences,) tensor of sequence lengths\n",
        "            valid_ids: List of valid horse IDs\n",
        "        \"\"\"\n",
        "        detach_flags, total_nodes, hid_location = self.get_detach_flags(horse_ids)\n",
        "        valid_mask = horse_ids != -1\n",
        "        valid_ids = horse_ids[valid_mask].tolist()\n",
        "\n",
        "        # Batch retrieval of embeddings\n",
        "        sequences = []\n",
        "        integrated_ids = []\n",
        "        i = 0\n",
        "        for hid in valid_ids:\n",
        "            state = self.embedding_states.get(hid)\n",
        "            if state and len(state.embeddings) > 0:\n",
        "                intermediate_sequence = []\n",
        "                for inter_state in state.embeddings:\n",
        "                    # if detach_flags[i]:\n",
        "                    #     inter_state = inter_state.detach()\n",
        "                    if hid_location[i] != hid:\n",
        "                        logger.warning(f\"In get_embeddings crid sequence {hid} doesn't coincide with detach_flags sequence {hid_location[i]} \")\n",
        "                    i += 1\n",
        "                    intermediate_sequence.append(inter_state)\n",
        "                sequences.append(torch.stack(intermediate_sequence))\n",
        "                integrated_ids.append(hid)\n",
        "        if not sequences:\n",
        "            return None, None, [], 0\n",
        "\n",
        "        # Pad sequences efficiently\n",
        "        lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long, device=self.device)\n",
        "        padded = torch.nn.utils.rnn.pad_sequence(sequences,\n",
        "                                                batch_first=True,\n",
        "                                                padding_value=0.0)\n",
        "\n",
        "        return padded, lengths, integrated_ids, total_nodes\n",
        "\n",
        "    def reset_embeddings(self) -> None:\n",
        "        \"\"\"Safely reset all embeddings and associated computation graphs\"\"\"\n",
        "        # First detach and clear gradients\n",
        "        for hid, state in self.embedding_states.items():\n",
        "            with torch.no_grad():\n",
        "                # Detach all embeddings from computation graph\n",
        "                state.embeddings = [t.detach() for t in state.embeddings]\n",
        "\n",
        "                # Remove gradient information\n",
        "                for t in state.embeddings:\n",
        "                    t.grad = None\n",
        "                    t.requires_grad_(False)\n",
        "\n",
        "                # Clear lists\n",
        "                state.embeddings.clear()\n",
        "                state.node_counts.clear()\n",
        "\n",
        "        # Then clear the dictionary\n",
        "        self.embedding_states.clear()\n",
        "\n",
        "        # Finally force CUDA cleanup\n",
        "        if torch.cuda.is_initialized():\n",
        "            torch.cuda.synchronize()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijyZl9FVeo3p"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad-NiLDECK2S"
      },
      "outputs": [],
      "source": [
        "\n",
        "def token_ids_to_adjacency(token_ids: torch.Tensor, self_loop: bool = False) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Converts token IDs to a batch of adjacency matrices where ALL valid nodes are neighbors.\n",
        "\n",
        "    Args:\n",
        "        token_ids: (B, N) tensor where:\n",
        "            - B = batch size\n",
        "            - N = number of nodes\n",
        "            - -1 indicates padding/no neighbor\n",
        "\n",
        "    Returns:\n",
        "        adj: (B, N, N) adjacency matrix where valid nodes are connected\n",
        "    \"\"\"\n",
        "    B, N = token_ids.shape\n",
        "    device = token_ids.device\n",
        "\n",
        "    # Create mask of valid nodes\n",
        "    valid_nodes = (token_ids != -1)\n",
        "\n",
        "    # Create adjacency matrix\n",
        "    adj = (valid_nodes.unsqueeze(-1).type(torch.float32) @ valid_nodes.unsqueeze(-2).type(torch.float32))  # (B, N, N)\n",
        "\n",
        "    # Remove self-loops if needed\n",
        "    if not self_loop:\n",
        "        identity = torch.eye(N, device=device).unsqueeze(0)\n",
        "        adj = adj * (1 - identity)\n",
        "\n",
        "    return adj\n",
        "\n",
        "def batch_normalize_adjacency(adj: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Batch-aware adjacency normalization\"\"\"\n",
        "    # Compute degree\n",
        "    degree = adj.sum(dim=-1).clamp(min=1)\n",
        "    deg_inv_sqrt = torch.pow(degree, -0.5)\n",
        "\n",
        "    # Normalized adjacency (D^(-0.5)AD^(-0.5))\n",
        "    norm_adj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)\n",
        "    return norm_adj\n",
        "\n",
        "class GraphConvolution_GCNII2(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, residual=False, variant=False):\n",
        "        super(GraphConvolution_GCNII, self).__init__()\n",
        "        self.variant = variant\n",
        "        if self.variant:\n",
        "            self.in_features = 2*in_features\n",
        "        else:\n",
        "            self.in_features = in_features\n",
        "\n",
        "        self.out_features = out_features\n",
        "        self.residual = residual\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(self.in_features,self.out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.out_features)\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj , h0 , lamda, alpha, l):\n",
        "        theta = math.log(lamda/l+1)\n",
        "        hi = torch.spmm(adj, input)\n",
        "        # hi = torch.mm(adj, input)\n",
        "\n",
        "        if self.variant:\n",
        "            support = torch.cat([hi,h0],1)\n",
        "            r = (1-alpha)*hi+alpha*h0\n",
        "        else:\n",
        "            support = (1-alpha)*hi+alpha*h0\n",
        "            r = support\n",
        "        output = theta*torch.mm(support, self.weight)+(1-theta)*r\n",
        "        if self.residual:\n",
        "            output = output+input\n",
        "        return output\n",
        "class GraphConvolution_GCNII(nn.Module):\n",
        "    def __init__(self, in_features, out_features, residual=False, variant=False):\n",
        "        super(GraphConvolution_GCNII, self).__init__()\n",
        "        self.variant = variant\n",
        "        if self.variant:\n",
        "            self.in_features = 2 * in_features  # Concatenate along feature dim\n",
        "        else:\n",
        "            self.in_features = in_features\n",
        "\n",
        "        self.out_features = out_features\n",
        "        self.residual = residual\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(self.in_features, self.out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.out_features)\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x, adj, h0, lamda, alpha, l):\n",
        "        # Shapes:\n",
        "        # x:      [B, N, in_features]\n",
        "        # adj:    [B, N, N]\n",
        "        # h0:     [B, N, in_features]\n",
        "        # Returns: [B, N, out_features]\n",
        "\n",
        "        theta = math.log(lamda / l + 1)\n",
        "\n",
        "        hi = torch.bmm(adj, x)  # Shape: [B, N, in_features]\n",
        "\n",
        "        if self.variant:\n",
        "            support = torch.cat([hi, h0], dim=2)  # [B, N, 2*in_features]\n",
        "            r = (1 - alpha) * hi + alpha * h0  # [B, N, in_features]\n",
        "        else:\n",
        "            support = (1 - alpha) * hi + alpha * h0  # [B, N, in_features]\n",
        "            r = support\n",
        "\n",
        "        # Batched matrix multiplication\n",
        "        output = theta * torch.matmul(support, self.weight) + (1 - theta) * r\n",
        "\n",
        "        if self.residual:\n",
        "            # Ensure input & output dimensions match\n",
        "            output = output + x  # [B, N, out_features] += [B, N, in_features]\n",
        "\n",
        "        return output\n",
        "\n",
        "class GCNII(nn.Module):\n",
        "    def __init__(self, nlayers,emb_dim, dropout, lamda, alpha, variant, residual, self_loop):\n",
        "        super(GCNII, self).__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(nlayers):\n",
        "            self.convs.append(GraphConvolution_GCNII(emb_dim, emb_dim,variant=variant, residual = residual))\n",
        "        self.act_fn = nn.GELU()\n",
        "        self.dropout = dropout\n",
        "        self.alpha = alpha\n",
        "        self.lamda = lamda\n",
        "        self.residual = residual\n",
        "        self.self_loop = self_loop\n",
        "    def forward(self, input) : #x, hids):\n",
        "        x, hids = input\n",
        "        adj = token_ids_to_adjacency(hids, self.self_loop)\n",
        "        adj = batch_normalize_adjacency(adj) #.to_sparse()\n",
        "\n",
        "        first_x = x\n",
        "        for i,con in enumerate(self.convs):\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = self.act_fn(con(x,adj,first_x,self.lamda,self.alpha,i+1))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        return x, hids\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise feed-forward network with GELU activation, dropout, and configurable hidden dimension.\"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_layer_dim: int, dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden_layer_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_layer_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"Graph convolution layer with input/output transformations and neighbor aggregation.\n",
        "\n",
        "    Args:\n",
        "        emb_dim: Dimension of node embeddings (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.linear_self = nn.Linear(emb_dim, emb_dim)\n",
        "        self.linear_neigh = nn.Linear(emb_dim, emb_dim)\n",
        "        self.eps = 1e-6  # For numerical stability\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x: torch.Tensor, token_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Node features (batch_size, num_nodes, emb_dim)\n",
        "            token_ids: Token ids (batch_size, max_number_tokens)\n",
        "                          -1 indicates padding/no token\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes, emb_dim = x.shape\n",
        "\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "        # Self transformation\n",
        "        self_emb = self.linear_self(x)  # (B, N, D)\n",
        "\n",
        "        # Neighbor transformation and aggregation\n",
        "        hids_mask = token_ids != -1\n",
        "        valid_counts = hids_mask.sum(dim=-1, keepdim=True)  # (B, N, 1)\n",
        "        transformed_neigh = self.linear_neigh(x)  # (B, N, D)\n",
        "\n",
        "        # Masked sum and normalize\n",
        "        summed_neigh = (transformed_neigh * hids_mask[..., None]).sum(dim=-2)\n",
        "        normalized_neigh = summed_neigh / (valid_counts + self.eps)\n",
        "\n",
        "        # Combine and activate\n",
        "        return F.gelu(self_emb + normalized_neigh.unsqueeze(1))\n",
        "\n",
        "\n",
        "class GCNBlock(nn.Module):\n",
        "    \"\"\"Multi-head graph convolution block with residual connection and layer normalization.\n",
        "\n",
        "    Args:\n",
        "        config: Dictionary containing:\n",
        "            - emb_dim: Embedding dimension (int)\n",
        "            - n_heads: Number of attention heads (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim, n_heads, dropout: float = 0.0, ff_layer: bool = False, multiple_ff: float = 4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([\n",
        "            GraphConvolution(emb_dim, dropout = dropout)\n",
        "            for _ in range(n_heads)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "        self.output_proj = nn.Linear(emb_dim * n_heads, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ff_layer = ff_layer\n",
        "        self.multiple_ff = multiple_ff\n",
        "        if ff_layer:\n",
        "            self.feed_forward = FeedForward(\n",
        "                emb_dim = emb_dim,\n",
        "                hidden_layer_dim = emb_dim * multiple_ff,\n",
        "                dropout = dropout\n",
        "            )\n",
        "    def forward(self, data: tuple) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: Tuple containing: (x, token_ids)\n",
        "            x: Node features (B, N, D)\n",
        "            token_ids: Token ids (B, N, K)\n",
        "        \"\"\"\n",
        "\n",
        "        x, token_ids = data\n",
        "\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        # x[token_ids!=-1] = self.norm(x[token_ids!=-1])\n",
        "\n",
        "        # Process all heads in parallel\n",
        "        head_outputs = [head(x, token_ids) for head in self.heads]\n",
        "\n",
        "        if len(self.heads)>1:\n",
        "          combined = torch.cat(head_outputs, dim=-1)  # (B, N, D*H)\n",
        "\n",
        "          # Project back to original dimension\n",
        "          # return self.dropout(self.output_proj(combined)) + residual, token_ids\n",
        "          if self.ff_layer:\n",
        "            return self.feed_forward(self.output_proj(combined)) + residual, token_ids\n",
        "          else:\n",
        "            return self.output_proj(combined) + residual, token_ids\n",
        "\n",
        "        else:\n",
        "          # return self.dropout(head_outputs[0]) + residual, token_ids\n",
        "          if self.ff_layer:\n",
        "            return self.feed_forward(head_outputs[0]) + residual, token_ids\n",
        "          else:\n",
        "            return head_outputs[0] + residual, token_ids\n",
        "\n",
        "class GNN_Transformer(nn.Module):\n",
        "    \"\"\"Transformer Model with padding mask handling and configurable parameters\"\"\"\n",
        "\n",
        "    def __init__(self, emb_dim, nhead, num_layers, multiple_ff, dropout):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config: Dictionary containing model parameters:\n",
        "                - emb_dim: Input dimension size (int)\n",
        "                - nhead: Number of attention heads (int)\n",
        "                - num_layers: Number of transformer layers (int, optional)\n",
        "                - dim_feedforward: Feedforward dimension (int, optional)\n",
        "                - dropout: Dropout probability (float, optional)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        # Transformer components\n",
        "        encoder_layer = TransformerEncoderLayer(\n",
        "            d_model = emb_dim,\n",
        "            nhead = nhead,\n",
        "            dim_feedforward = self.emb_dim * multiple_ff,\n",
        "            dropout = dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, input: Tuple) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Process variable-length sequences through Transformer\n",
        "\n",
        "        Args:\n",
        "            x:(B, N, emb_dim) padded sequences\n",
        "            token_ids: Token ids (B, N) (when -1 then pad this element)\n",
        "\n",
        "        Returns:\n",
        "            output_sequences: (B, N, emb_dim) processed sequences\n",
        "        \"\"\"\n",
        "        x, token_ids = input\n",
        "\n",
        "        # Validate inputs\n",
        "        # self._validate_inputs(x, token_ids)\n",
        "\n",
        "        # Create padding mask (True indicates padding positions)\n",
        "        padding_mask = self._create_padding_mask(token_ids)\n",
        "\n",
        "        # Process through transformer\n",
        "        output = self.transformer_encoder(\n",
        "            x,\n",
        "            src_key_padding_mask=padding_mask\n",
        "        )\n",
        "\n",
        "        return output, token_ids\n",
        "\n",
        "    def _create_padding_mask(self,\n",
        "                            token_ids: torch.Tensor,\n",
        "                            ) -> torch.Tensor:\n",
        "        \"\"\"Creates padding mask for transformer input\"\"\"\n",
        "        mask = token_ids == -1\n",
        "        return mask\n",
        "\n",
        "    def _validate_inputs(self,\n",
        "                        padded_sequences: torch.Tensor,\n",
        "                        sequence_lengths: torch.Tensor):\n",
        "        \"\"\"Validates input dimensions and types\"\"\"\n",
        "        if padded_sequences.dim() != 3:\n",
        "            raise ValueError(f\"Input sequences must be 3D tensor (batch, seq, features), \"\n",
        "                           f\"got {padded_sequences.dim()}D\")\n",
        "\n",
        "        if sequence_lengths.dim() != 1:\n",
        "            raise ValueError(f\"Lengths must be 1D tensor, got {sequence_lengths.dim()}D\")\n",
        "\n",
        "        if padded_sequences.size(0) != sequence_lengths.size(0):\n",
        "            raise ValueError(\"Batch size mismatch between sequences and lengths\")\n",
        "\n",
        "\n",
        "class LSTM_Model(nn.Module):\n",
        "    \"\"\"LSTM Model with padded sequence handling and configurable parameters\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config: Dictionary containing model parameters:\n",
        "                - emb_dim: Input and hidden dimension size (int)\n",
        "                - num_layers: Number of LSTM layers (int, optional)\n",
        "                - dropout: Dropout probability (float, optional)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "                input: Tuple) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Process variable-length sequences through LSTM\n",
        "\n",
        "        Args:\n",
        "            padded_sequences: (batch_size, max_seq_len, emb_dim) padded sequences\n",
        "            sequence_lengths: (batch_size,) lengths of valid sequences\n",
        "\n",
        "        Returns:\n",
        "            output_sequences: (batch_size, max_seq_len, emb_dim) processed sequences\n",
        "        \"\"\"\n",
        "        padded_sequences, sequence_lengths = input\n",
        "\n",
        "        # Validate inputs\n",
        "        self._validate_inputs(padded_sequences, sequence_lengths)\n",
        "\n",
        "        # Convert lengths to CPU tensor for packing\n",
        "        lengths_cpu = sequence_lengths.cpu()\n",
        "\n",
        "        # Pack padded sequences\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(\n",
        "            input=padded_sequences,\n",
        "            lengths=lengths_cpu,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Process through LSTM\n",
        "        packed_output, _ = self.lstm(packed_input)\n",
        "\n",
        "        # Unpack sequences\n",
        "        output_sequences, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "            packed_output,\n",
        "            batch_first=True,\n",
        "            total_length=padded_sequences.size(1))\n",
        "\n",
        "        return output_sequences\n",
        "\n",
        "    def _validate_inputs(self,\n",
        "                        sequences: torch.Tensor,\n",
        "                        lengths: torch.Tensor) -> None:\n",
        "        \"\"\"Validate input dimensions and lengths\"\"\"\n",
        "        if sequences.dim() != 3:\n",
        "            raise ValueError(f\"Input sequences must be 3D tensor (batch, seq, features), got {sequences.shape}\")\n",
        "\n",
        "        if lengths.dim() != 1:\n",
        "            raise ValueError(f\"Sequence lengths must be 1D tensor, got {lengths.shape}\")\n",
        "\n",
        "        if sequences.size(0) != lengths.size(0):\n",
        "            raise ValueError(f\"Batch size mismatch between sequences ({sequences.size(0)}) and lengths ({lengths.size(0)})\")\n",
        "\n",
        "        if (lengths < 0).any() or (lengths > sequences.size(1)).any():\n",
        "            raise ValueError(\"Invalid sequence lengths detected\")\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Transformer Model with padding mask handling and configurable parameters\"\"\"\n",
        "\n",
        "    def __init__(self, emb_dim, nhead, num_layers, multiple_ff, dropout, max_seq_len):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config: Dictionary containing model parameters:\n",
        "                - emb_dim: Input dimension size (int)\n",
        "                - nhead: Number of attention heads (int)\n",
        "                - num_layers: Number of transformer layers (int, optional)\n",
        "                - dim_feedforward: Feedforward dimension (int, optional)\n",
        "                - dropout: Dropout probability (float, optional)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.max_seq_len = max_seq_len  # Add max length parameter\n",
        "\n",
        "        # Learnable positional embeddings\n",
        "        self.position_embedding = nn.Embedding(\n",
        "            num_embeddings=self.max_seq_len,\n",
        "            embedding_dim = self.emb_dim\n",
        "        )\n",
        "\n",
        "        # Transformer components\n",
        "        encoder_layer = TransformerEncoderLayer(\n",
        "            d_model = emb_dim,\n",
        "            nhead = nhead,\n",
        "            dim_feedforward = self.emb_dim * multiple_ff,\n",
        "            dropout = dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, input: Tuple) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Process variable-length sequences through Transformer\n",
        "\n",
        "        Args:\n",
        "            padded_sequences: (batch_size, max_seq_len, emb_dim) padded sequences\n",
        "            sequence_lengths: (batch_size,) lengths of valid sequences\n",
        "\n",
        "        Returns:\n",
        "            output_sequences: (batch_size, max_seq_len, emb_dim) processed sequences\n",
        "        \"\"\"\n",
        "        padded_sequences, sequence_lengths = input\n",
        "\n",
        "        # Validate inputs\n",
        "        self._validate_inputs(padded_sequences, sequence_lengths)\n",
        "\n",
        "        # Add positional encodings\n",
        "        x = self._add_positional_encoding(padded_sequences)\n",
        "\n",
        "        # Create padding mask (True indicates padding positions)\n",
        "        padding_mask = self._create_padding_mask(sequence_lengths,\n",
        "                                                padded_sequences.size(1))\n",
        "        # Process through transformer\n",
        "        output_sequences = self.transformer_encoder(\n",
        "            x,\n",
        "            src_key_padding_mask=padding_mask\n",
        "        )\n",
        "\n",
        "        return output_sequences\n",
        "\n",
        "    def _add_positional_encoding(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Adds learned positional embeddings\"\"\"\n",
        "        batch_size, seq_len, emb_dim = x.size()\n",
        "\n",
        "        # Create position indices\n",
        "        positions = torch.arange(seq_len, device=x.device)\\\n",
        "                      .expand(batch_size, seq_len)\n",
        "\n",
        "        # Get embeddings (batch_size, seq_len, emb_dim)\n",
        "        pos_embeddings = self.position_embedding(positions)\n",
        "\n",
        "        return x + pos_embeddings\n",
        "\n",
        "    def _create_padding_mask(self,\n",
        "                            lengths: torch.Tensor,\n",
        "                            max_len: int) -> torch.Tensor:\n",
        "        \"\"\"Creates padding mask for transformer input\"\"\"\n",
        "        batch_size = lengths.size(0)\n",
        "        mask = torch.arange(max_len, device=lengths.device)\\\n",
        "               .expand(batch_size, max_len) >= lengths.unsqueeze(1)\n",
        "        return mask\n",
        "\n",
        "    def _validate_inputs(self,\n",
        "                        padded_sequences: torch.Tensor,\n",
        "                        sequence_lengths: torch.Tensor):\n",
        "        \"\"\"Validates input dimensions and types\"\"\"\n",
        "        if padded_sequences.dim() != 3:\n",
        "            raise ValueError(f\"Input sequences must be 3D tensor (batch, seq, features), \"\n",
        "                           f\"got {padded_sequences.dim()}D\")\n",
        "\n",
        "        if sequence_lengths.dim() != 1:\n",
        "            raise ValueError(f\"Lengths must be 1D tensor, got {sequence_lengths.dim()}D\")\n",
        "\n",
        "        if padded_sequences.size(0) != sequence_lengths.size(0):\n",
        "            raise ValueError(\"Batch size mismatch between sequences and lengths\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnFnYnl1E2IH"
      },
      "outputs": [],
      "source": [
        "\n",
        "class HorseRacingModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # self.__dict__.update(cfg)\n",
        "        self.sequence_model_cfg = cfg.sequence_model\n",
        "        self.graph_conv_model_cfg = cfg.graph_conv_model\n",
        "        self.input_features = cfg.input_features\n",
        "        self.input_embedding = self.sequence_model_cfg['input_embedding']\n",
        "        self.logits_size = cfg.logits_size\n",
        "        self.emb_dim = cfg.emb_dim\n",
        "\n",
        "        # Save the embeddings for horse states\n",
        "        self.horse_embeddings = CustomEmbeddingManager(\n",
        "            embedding_dim = self.sequence_model_cfg[\"emb_dim\"],\n",
        "            max_nodes = self.sequence_model_cfg[\"max_nodes\"],\n",
        "            max_depth_nodes = self.sequence_model_cfg[\"max_depth_nodes\"],\n",
        "            max_sequence_length = self.sequence_model_cfg[\"max_seq_depth\"]\n",
        "        )\n",
        "\n",
        "        # Embedding for horses that have no previous races\n",
        "        self.first_horse_state = nn.Parameter(torch.zeros(self.emb_dim))\n",
        "\n",
        "\n",
        "        # Input projection logits\n",
        "        self.logit_projection = nn.Linear(\n",
        "            len(self.input_features[\"races\"]) + len(self.input_features[\"race_horse\"]) + self.emb_dim,\n",
        "            self.emb_dim\n",
        "            )\n",
        "        # Input projection embeddings\n",
        "        if self.input_embedding:\n",
        "            self.embedding_projection = nn.Linear(\n",
        "                self.emb_dim + len(self.input_features[\"races\"]) + len(self.input_features[\"race_horse\"]) + len(self.input_features[\"results\"]),\n",
        "                self.graph_conv_model_cfg['emb_dim']\n",
        "            )\n",
        "        else:\n",
        "            self.embedding_projection = nn.Linear(\n",
        "                len(self.input_features[\"races\"]) + len(self.input_features[\"race_horse\"]) + len(self.input_features[\"results\"]),\n",
        "                self.graph_conv_model_cfg['emb_dim']\n",
        "            )\n",
        "\n",
        "        # Graph Neural Networks\n",
        "        if self.graph_conv_model_cfg['layers_type'] == 'GCNII':\n",
        "              self.logit_graph_nn = GCNII(\n",
        "                    nlayers = self.graph_conv_model_cfg['n_layers'],\n",
        "                    emb_dim = self.graph_conv_model_cfg['emb_dim'],\n",
        "                    dropout = self.graph_conv_model_cfg['dropout'],\n",
        "                    lamda = self.graph_conv_model_cfg['GCNII_config']['lamda'],\n",
        "                    alpha = self.graph_conv_model_cfg['GCNII_config']['alpha'],\n",
        "                    variant = self.graph_conv_model_cfg['GCNII_config']['variant'],\n",
        "                    residual = self.graph_conv_model_cfg['GCNII_config']['residual'],\n",
        "                    self_loop = self.graph_conv_model_cfg['GCNII_config']['self_loop']\n",
        "                )\n",
        "              self.embedding_graph_nn = GCNII(\n",
        "                    nlayers = self.graph_conv_model_cfg['n_layers'],\n",
        "                    emb_dim = self.graph_conv_model_cfg['emb_dim'],\n",
        "                    dropout = self.graph_conv_model_cfg['dropout'],\n",
        "                    lamda = self.graph_conv_model_cfg['GCNII_config']['lamda'],\n",
        "                    alpha = self.graph_conv_model_cfg['GCNII_config']['alpha'],\n",
        "                    variant = self.graph_conv_model_cfg['GCNII_config']['variant'],\n",
        "                    residual = self.graph_conv_model_cfg['GCNII_config']['residual'],\n",
        "                    self_loop = self.graph_conv_model_cfg['GCNII_config']['self_loop']\n",
        "                )\n",
        "\n",
        "        elif self.graph_conv_model_cfg['layers_type'] == 'GCN':\n",
        "              self.logit_graph_nn = nn.Sequential(*[\n",
        "                  GCNBlock(self.graph_conv_model_cfg['emb_dim'],\n",
        "                           1,\n",
        "                           self.graph_conv_model_cfg['dropout'],\n",
        "                           self.graph_conv_model_cfg['GCN_config']['ff_layer'],\n",
        "                           self.graph_conv_model_cfg['GCN_config']['multiple_ff'])\n",
        "                  for _ in range(self.graph_conv_model_cfg['n_layers'])\n",
        "              ])\n",
        "              self.embedding_graph_nn = nn.Sequential(*[\n",
        "                  GCNBlock(self.graph_conv_model_cfg['emb_dim'],\n",
        "                           1,\n",
        "                           self.graph_conv_model_cfg['dropout'])\n",
        "                  for _ in range(self.graph_conv_model_cfg['n_layers'])\n",
        "              ])\n",
        "\n",
        "        elif self.graph_conv_model_cfg['layers_type'] == 'Transformer':\n",
        "              self.logit_graph_nn = GNN_Transformer(\n",
        "                emb_dim = self.emb_dim,\n",
        "                nhead = self.graph_conv_model_cfg['GNN_Transformer_config']['n_heads'],\n",
        "                num_layers = self.graph_conv_model_cfg['GNN_Transformer_config']['n_layers'],\n",
        "                multiple_ff = self.graph_conv_model_cfg['GNN_Transformer_config']['multiple_ff'],\n",
        "                dropout = self.graph_conv_model_cfg['GNN_Transformer_config']['dropout'],\n",
        "            )\n",
        "\n",
        "              self.embedding_graph_nn = GNN_Transformer(\n",
        "                emb_dim = self.emb_dim,\n",
        "                nhead = self.graph_conv_model_cfg['GNN_Transformer_config']['n_heads'],\n",
        "                num_layers = self.graph_conv_model_cfg['GNN_Transformer_config']['n_layers'],\n",
        "                multiple_ff = self.graph_conv_model_cfg['GNN_Transformer_config']['multiple_ff'],\n",
        "                dropout = self.graph_conv_model_cfg['GNN_Transformer_config']['dropout'],\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported Graph Neural Network layers type: {self.layers_type}\")\n",
        "\n",
        "        # Output layers\n",
        "        self.final_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.final_projection = nn.Linear(self.emb_dim, self.logits_size)\n",
        "\n",
        "        # Sequence modeling\n",
        "        self.sequence_norm = nn.LayerNorm(self.emb_dim)\n",
        "        if self.sequence_model_cfg['sequence_model_type'] == 'Transformer':\n",
        "\n",
        "            self.sequence_model = TransformerModel(\n",
        "                emb_dim = self.emb_dim,\n",
        "                nhead = self.sequence_model_cfg['Transformer_config']['n_heads'],\n",
        "                num_layers = self.sequence_model_cfg['Transformer_config']['n_layers'],\n",
        "                multiple_ff = self.sequence_model_cfg['Transformer_config']['multiple_ff'],\n",
        "                dropout = self.sequence_model_cfg['Transformer_config']['dropout'],\n",
        "                max_seq_len = self.sequence_model_cfg['max_seq_depth']\n",
        "            )\n",
        "\n",
        "        elif self.sequence_model_cfg['sequence_model_type'] == 'LSTM':\n",
        "            self.sequence_model = LSTM_Model(\n",
        "                input_size = self.emb_dim,\n",
        "                hidden_size = self.sequence_model_cfg['LSTM_config']['hidden_dim'],\n",
        "                num_layers = self.sequence_model_cfg['LSTM_config']['n_layers'],\n",
        "                dropout = self.sequence_model_cfg['LSTM_config']['dropout'])\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported Sequence Neural Network layers type: {self.layers_type}\")\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        race_features = inputs['race_data']\n",
        "        horse_features = inputs['horse_data']\n",
        "        results = inputs['results_data']\n",
        "        hids = inputs['hids']\n",
        "        crids = inputs['crid']\n",
        "\n",
        "        if self.training:\n",
        "          race_features.requires_grad = True\n",
        "          horse_features.requires_grad = True\n",
        "          results.requires_grad = True\n",
        "\n",
        "        # Retrieve horse embeddings\n",
        "        embeddings, seq_lengths, hid_order, node_counts = self.horse_embeddings.get_embeddings(hids)\n",
        "\n",
        "        # Process through sequence neural network\n",
        "        if embeddings is not None:\n",
        "            lstm_output = self._process_sequence(self.sequence_model, embeddings, seq_lengths)\n",
        "        else:\n",
        "            lstm_output = None\n",
        "\n",
        "        # Create embedding matrix using vectorized operations\n",
        "        batch_size, num_horses = hids.shape\n",
        "        horse_embeddings = self._create_horse_embeddings_matrix(\n",
        "            hids, lstm_output, hid_order, seq_lengths, batch_size, num_horses\n",
        "        )\n",
        "\n",
        "        # Prepare Graph Neural Network inputs\n",
        "        logit_input = torch.cat([race_features, horse_features, horse_embeddings], dim=-1)\n",
        "        if self.input_embedding:\n",
        "            embed_input = torch.cat([race_features, horse_features, horse_embeddings, results], dim=-1)\n",
        "        else:\n",
        "            embed_input = torch.cat([race_features, horse_features, results], dim=-1)\n",
        "\n",
        "        # Process through Graph Neural Networks\n",
        "        logit_input = self._process_layer(self.logit_projection, logit_input)\n",
        "        embed_input = self._process_layer(self.embedding_projection, embed_input)\n",
        "\n",
        "        logit_output = self._process_GNN(self.logit_graph_nn, logit_input, hids)\n",
        "        embed_output = self._process_GNN(self.embedding_graph_nn, embed_input, hids)\n",
        "\n",
        "        # Generate final predictions\n",
        "        logit_output = self.final_norm(logit_output)\n",
        "        logits = self.final_projection(logit_output)\n",
        "\n",
        "        self.horse_embeddings.update_embeddings(hids, embed_output, node_counts)\n",
        "\n",
        "        return logits, node_counts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _create_horse_embeddings_matrix(self, hids, lstm_output, hid_order, seq_lengths,\n",
        "                                      batch_size, num_horses):\n",
        "        device = hids.device\n",
        "        embeddings = torch.zeros((batch_size, num_horses, self.emb_dim), device=device)\n",
        "\n",
        "        # Create lookup for valid hids\n",
        "        valid_hids = (hids != -1)\n",
        "        hid_lookup = {hid: idx for idx, hid in enumerate(hid_order)}\n",
        "\n",
        "        # Vectorized embedding assignment\n",
        "        with torch.no_grad():\n",
        "            index_hid_order = 0\n",
        "            hid_indices = torch.full_like(hids, -1, dtype=torch.long, device=device)\n",
        "            for i in range(batch_size):\n",
        "                for j in range(num_horses):\n",
        "                    if valid_hids[i,j] and hids[i,j].item() in hid_lookup:\n",
        "                        hid_indices[i,j] = hid_lookup[hids[i,j].item()]\n",
        "\n",
        "        valid_mask = hid_indices != -1\n",
        "        if valid_mask.any():\n",
        "            seq_lengths.to(device)\n",
        "\n",
        "            seq_indices = seq_lengths[hid_indices[valid_mask]].to(device) - 1\n",
        "            embeddings[valid_mask] = lstm_output[hid_indices[valid_mask], seq_indices]\n",
        "\n",
        "        # Handle first-time hids\n",
        "        first_time_mask = valid_hids & ~valid_mask\n",
        "        embeddings[first_time_mask] = self.first_horse_state\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def _process_layer(self, layer, x):\n",
        "        return checkpoint(layer, x, use_reentrant=True) if self.training else layer(x)\n",
        "\n",
        "    def _process_sequence(self, sequence_model, embeddings, lengths):\n",
        "        return checkpoint(sequence_model, ((embeddings, lengths)), use_reentrant=False) if self.training else sequence_model((embeddings, lengths))\n",
        "\n",
        "    def _process_GNN(self, transformer, x, hids):\n",
        "        output, hids = checkpoint(transformer, ((x, hids)), use_reentrant=False) if self.training else transformer((x, hids))\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXXRFHpFfU6o"
      },
      "source": [
        "### Training helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7pYrqK0k2D3"
      },
      "outputs": [],
      "source": [
        "class GradientMonitor:\n",
        "    \"\"\"Monitors and reports gradient statistics\"\"\"\n",
        "    def __init__(self):\n",
        "        self.max_gradients = {}\n",
        "        self.avg_gradients = {}\n",
        "\n",
        "    def update(self, model: nn.Module):\n",
        "        \"\"\"Update gradient statistics\"\"\"\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad = param.grad.detach()\n",
        "                self.max_gradients[name] = torch.max(torch.abs(grad)).item()\n",
        "                self.avg_gradients[name] = torch.mean(torch.abs(grad)).item()\n",
        "\n",
        "    def report(self, frequency: int = 100):\n",
        "        \"\"\"Print gradient summary\"\"\"\n",
        "        if not self.max_gradients:\n",
        "            return\n",
        "\n",
        "        max_grad = max(self.max_gradients.values())\n",
        "        avg_grad = sum(self.avg_gradients.values()) / len(self.avg_gradients)\n",
        "        print(f\"\\nGradient Summary:\")\n",
        "        print(f\"Max Gradient: {max_grad:.4e}, Max Gradients per layer: {self.max_gradients}\")\n",
        "        print(f\"Avg Gradient: {avg_grad:.4e}, Avg Gradient per layer: {self.avg_gradients}\")\n",
        "\n",
        "\n",
        "class TrainingConfig:\n",
        "    \"\"\"Validated training configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.__dict__.update(config)\n",
        "\n",
        "        self.optimizer = OptimConfig(config['optimizer'])\n",
        "        self.data = DataConfig(config['data'])\n",
        "        self.model = ModelConfig(config['model'])\n",
        "        self.train_follow = TrainingIllustration(config['training_follow'])\n",
        "\n",
        "class DataConfig:\n",
        "    \"\"\"Validated data configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.__dict__.update(config)\n",
        "\n",
        "    def get_dataloader(self):\n",
        "\n",
        "        dataset = HorseRacingDataset(self)\n",
        "        self.features = dataset.features\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size = self.batch_size,\n",
        "            shuffle = self.shuffle,\n",
        "            collate_fn = lambda b: collate_fn(b, dataset),\n",
        "            num_workers = self.num_workers,\n",
        "            drop_last = self.drop_last\n",
        "        )\n",
        "        return dataloader\n",
        "    def get_datafeatures(self):\n",
        "        return self.features\n",
        "\n",
        "class TrainingStats:\n",
        "    \"\"\"Handles training statistics collection and reporting\"\"\"\n",
        "    def __init__(self):\n",
        "        self.data = defaultdict(list)\n",
        "        self.metrics = [\n",
        "            'loss', 'total_winnings', 'total_betted',\n",
        "            'predicted_earnings', 'predicted_earnings_greedy',\n",
        "            'hrn', 'accuracy', 'spearman', 'batch_time', 'n_nodes'\n",
        "        ]\n",
        "\n",
        "    def update(self, epoch: int, batch_idx: int, model: nn.Module, outputs: torch.Tensor,\n",
        "                batch_time: float, nodes_depth: int, lr: float, batch: Dict, loss: torch.Tensor):\n",
        "        \"\"\" Compute metrics \"\"\"\n",
        "        with torch.no_grad():\n",
        "            device = outputs.device\n",
        "            positions_arrival = batch['positions']\n",
        "            decimal_prices = batch['prices']\n",
        "            hrn = batch['hrn']\n",
        "            hids = batch['hids']\n",
        "            crids = batch['crid']\n",
        "\n",
        "            batch_size, num_horses, n_ranks = outputs.shape\n",
        "            if n_ranks > 1:\n",
        "              outputs = outputs[:,:,0]\n",
        "            outputs = outputs.squeeze(-1)\n",
        "            # Mask out logits for horses with position -1 (excluded from softmax)\n",
        "            mask_out = (positions_arrival == -1.0)\n",
        "            outputs = outputs.masked_fill(mask_out, -1e9)\n",
        "\n",
        "            # Races with exactly one valid winner\n",
        "            winner_mask = (positions_arrival == 1.0)\n",
        "            valid_races = winner_mask.sum(dim=1) == 1\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Accuracy calculation\n",
        "            valid_winner_races = winner_mask.any(dim=1)\n",
        "\n",
        "            if valid_winner_races.any():\n",
        "                # Accuracy\n",
        "                pred_winners = probs.argmax(dim=1)\n",
        "                correct = winner_mask[torch.arange(batch_size), pred_winners]\n",
        "                accuracy = correct[valid_winner_races].float().mean()\n",
        "\n",
        "                # Probabilistic betting. Bet the probability of winning on the horse.\n",
        "                selected_probs = probs[winner_mask]\n",
        "                selected_decimalprices = decimal_prices[winner_mask]\n",
        "                winnings_prob = (selected_probs / selected_decimalprices).sum()\n",
        "                total_betted_prob = batch_size\n",
        "\n",
        "                # Value betting. Bet on the horse with a probability higher than the decimal price\n",
        "                valid_horses_mask = (positions_arrival != -1.0) & (positions_arrival != 40.0)\n",
        "                value_bets_mask = (probs > decimal_prices) & valid_horses_mask\n",
        "                won_value_bets = value_bets_mask & (positions_arrival == 1.0)\n",
        "                winnings_value = (won_value_bets.float() / decimal_prices).sum()\n",
        "                total_betted_value = value_bets_mask.sum().float()\n",
        "\n",
        "                # Greedy betting. Bet full on the horse with the highest probability\n",
        "                pred_winners = probs.argmax(dim=1)\n",
        "                winnings_greedy = (positions_arrival[torch.arange(batch_size), pred_winners] == 1.0)\n",
        "                decimalPrice_greedy = decimal_prices[torch.arange(batch_size), pred_winners]\n",
        "                winnings_greedy = (winnings_greedy.float() / decimalPrice_greedy).sum()\n",
        "                total_betted_greedy = batch_size\n",
        "\n",
        "                # Kelly Criterion betting\n",
        "                # Create valid horses mask (adjust based on your data conventions)\n",
        "                valid_horses_mask = (positions_arrival != -1.0) & (positions_arrival != 40.0)\n",
        "\n",
        "                # Value betting mask (where our probability estimate is better than market)\n",
        "                value_bets_mask = (probs > decimal_prices) & valid_horses_mask\n",
        "\n",
        "                # Calculate Kelly criterion fractions\n",
        "                b = 1 / decimal_prices  # Net odds received on the wager (profit per unit bet if you win)\n",
        "                edge = probs * (b + 1) - 1  # Simplified Kelly numerator\n",
        "                kelly_fractions = edge / b  # Kelly formula: (bp - q)/b\n",
        "\n",
        "                # Apply masks and clamp values\n",
        "                kelly_fractions = torch.where(value_bets_mask, kelly_fractions, 0.0)\n",
        "                kelly_fractions = torch.clamp(kelly_fractions, min=0.0, max=1.0)\n",
        "\n",
        "                # Calculate actual winnings and losses\n",
        "                won_bets = (positions_arrival == 1.0) & value_bets_mask\n",
        "                returns = kelly_fractions * won_bets / decimal_prices\n",
        "                losses = kelly_fractions * (~won_bets & value_bets_mask)\n",
        "\n",
        "                # Aggregate results\n",
        "                total_betted_kelly = kelly_fractions.sum()\n",
        "                total_return_kelly = returns.sum()\n",
        "                net_profit_kelly = total_return_kelly - total_betted_kelly\n",
        "                roi_kelly = net_profit_kelly / total_betted_kelly if total_betted_kelly > 0 else 0.0\n",
        "\n",
        "            else:\n",
        "                accuracy = torch.tensor(0.0, device=device)\n",
        "\n",
        "                winnings_prob = torch.tensor(0.0, device=device)\n",
        "                total_betted_prob = torch.tensor(0.0, device=device)\n",
        "\n",
        "                winnings_greedy = torch.tensor(0.0, device=device)\n",
        "                total_betted_greedy = torch.tensor(0.0, device=device)\n",
        "\n",
        "                total_betted_value = torch.tensor(0.0, device=device)\n",
        "                winnings_value = torch.tensor(0.0, device=device)\n",
        "\n",
        "                roi_kelly = torch.tensor(0.0, device=device)\n",
        "\n",
        "            predicted_earnings_prob = (winnings_prob - total_betted_prob) / (total_betted_prob + 1e-8)\n",
        "            predicted_earnings_greedy = (winnings_greedy - total_betted_greedy)/ (total_betted_greedy + 1e-8)\n",
        "            predicted_earnings_value = (winnings_value - total_betted_value)/(total_betted_value + 1e-8)\n",
        "\n",
        "            # Ranking correlation\n",
        "            spearman = self.spearman_rank_correlation(outputs.detach().clone(), positions_arrival)\n",
        "            n_nodes_computation_graph =  self.number_nodes_computation_graph(model, loss)\n",
        "\n",
        "            \"\"\"Update statistics\"\"\"\n",
        "            self.data['epoch'].append(epoch)\n",
        "            self.data['batch'].append(batch_idx)\n",
        "            self.data['batch_time'].append(batch_time)\n",
        "            self.data['computation_depth'].append(nodes_depth)\n",
        "            self.data['lr'].append(lr)\n",
        "            self.data['hrn'].append(hrn[hrn != -1].float().mean().item())\n",
        "            self.data['n_nodes'].append(n_nodes_computation_graph)\n",
        "\n",
        "            self.data['accuracy'].append(accuracy.item())\n",
        "            self.data['predicted_earnings'].append(predicted_earnings_prob.item())\n",
        "            self.data['predicted_earnings_greedy'].append(predicted_earnings_greedy.item())\n",
        "            self.data['predicted_earnings_value'].append(predicted_earnings_value.item())\n",
        "            self.data['roi_kelly'].append(roi_kelly.item())\n",
        "            self.data['spearman'].append(spearman.item())\n",
        "            self.data['avg_race_participants'].append(mask_out.sum().item()/batch_size)\n",
        "\n",
        "            self.data['loss'].append(loss.clone().item())\n",
        "\n",
        "    def number_nodes_computation_graph(self, model: nn.Module, loss: float) -> int:\n",
        "        try:\n",
        "            # dot = make_dot(loss, params=dict(model.named_parameters()),\n",
        "            #              show_attrs=False, show_saved=False)\n",
        "            # return len(dot.source)\n",
        "            return 0\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to count number of nodes in computation graph: {e}\")\n",
        "\n",
        "    def spearman_rank_correlation(self, logits: torch.Tensor,\n",
        "                             positions_arrival: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes Spearman's rank correlation between predicted logits and actual positions,\n",
        "        ignoring invalid entries (-1 or 40). Handles variable participant counts per race.\n",
        "\n",
        "        Args:\n",
        "            logits: Tensor of shape [batch_size, num_horses] with prediction scores\n",
        "            positions_arrival: Tensor of shape [batch_size, num_horses] with actual positions\n",
        "\n",
        "        Returns:\n",
        "            Mean Spearman's rho across batch (valid races only) as torch.Tensor\n",
        "        \"\"\"\n",
        "        device = logits.device\n",
        "        batch_size = logits.size(0)\n",
        "        correlations = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Filter valid entries for this race\n",
        "            mask = (positions_arrival[i] != -1.0) & (positions_arrival[i] != 40.0)\n",
        "            race_logits = logits[i][mask].detach().cpu().numpy()\n",
        "            race_positions = positions_arrival[i][mask].cpu().numpy()\n",
        "\n",
        "            # Skip races with <2 valid participants\n",
        "            if len(race_logits) < 2:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Generate predicted ranks from logits (higher logit = better rank)\n",
        "                pred_ranks = (-race_logits).argsort().argsort()  # Double argsort for rank\n",
        "\n",
        "                # Calculate Spearman correlation\n",
        "                rho, _ = spearmanr(pred_ranks, race_positions)\n",
        "\n",
        "                # Handle NaN/edge cases\n",
        "                if np.isnan(rho):\n",
        "                    rho = 0.0\n",
        "            except:\n",
        "                rho = 0.0\n",
        "\n",
        "            correlations.append(rho)\n",
        "\n",
        "        # Return average across valid races\n",
        "        if not correlations:\n",
        "            return torch.tensor(0.0, device=device)\n",
        "        return torch.tensor(np.mean(correlations), device=device)\n",
        "\n",
        "    def report(self, window_size: int = 100):\n",
        "        \"\"\"Print formatted training statistics\"\"\"\n",
        "        if len(self.data['loss']) < window_size:\n",
        "            return\n",
        "        print(f\"Epoch: {self.data['epoch'][-1]}, Batch: {self.data['batch'][-1]}, Loss:{round(np.mean(self.data['loss'][-window_size:]),2)}, Betting:{round(np.mean(self.data['predicted_earnings'][-window_size:]),2)}, Greedy Betting:{round(np.mean(self.data['predicted_earnings_greedy'][-window_size:]),2)}, HRN:{round(np.mean(self.data['hrn'][-window_size:]),2)} \")\n",
        "\n",
        "\n",
        "class ModelConfig:\n",
        "    \"\"\"Validated model configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.__dict__.update(config)\n",
        "\n",
        "class OptimConfig:\n",
        "    \"\"\"Validated optimizer configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.__dict__.update(config)\n",
        "        self.lr = 0\n",
        "        self.iterations_since_last_lr_update = 0\n",
        "\n",
        "    def init_optimizer(self, model: nn.Module) -> torch.optim.Optimizer:\n",
        "        if self.optimizer_name == 'Adam':\n",
        "            optim = torch.optim.Adam(params = model.parameters(),\n",
        "                            lr=self.lr,\n",
        "                            betas=self.Adam_cfg['betas'],\n",
        "                            eps=self.Adam_cfg['eps'],\n",
        "                            weight_decay=self.Adam_cfg['weight_decay'])\n",
        "            return optim\n",
        "        elif self.optimizer_name == 'SGD':\n",
        "            optim = torch.optim.SGD(params = model.parameters(),\n",
        "                            lr=self.lr,\n",
        "                            momentum=self.SGD_cfg['momentum'],\n",
        "                            weight_decay=self.SGD_cfg['weight_decay'])\n",
        "            return optim\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer: {cfg.optimizer}\")\n",
        "\n",
        "\n",
        "    def update_learning_rate(self, optimizer: torch.optim.Optimizer, stats: TrainingStats, batch_idx: int):\n",
        "      \"\"\"\n",
        "      Updates the learning rate based on the training loss.\n",
        "\n",
        "      Args:\n",
        "          optimizer: The optimizer to update.\n",
        "          stats: The TrainingStats object containing the training loss history.\n",
        "          patience: The number of epochs to wait before reducing the learning rate.\n",
        "          factor: The factor by which to reduce the learning rate.\n",
        "          min_lr: The minimum learning rate.\n",
        "          window_size: The size of the window to consider for the average loss.\n",
        "      \"\"\"\n",
        "      self.iterations_since_last_lr_update += 1\n",
        "\n",
        "      # Ensure cold start\n",
        "      if self.n_steps_cold_start >= batch_idx:\n",
        "          self.lr = (self.initial_lr / self.n_steps_cold_start) * batch_idx\n",
        "          for param_group in optimizer.param_groups:\n",
        "              param_group['lr'] = self.lr\n",
        "          self.iterations_since_last_lr_update = 0\n",
        "          return\n",
        "      # Check if enough data is available\n",
        "      if len(stats.data['loss']) < self.window_size + self.patience:\n",
        "          return\n",
        "\n",
        "      # Calculate average loss over the last 'window_size' epochs\n",
        "      current_loss = np.mean(stats.data['loss'][-self.window_size:])\n",
        "\n",
        "      # Calculate average loss 'patience' epochs ago\n",
        "      previous_loss = np.mean(stats.data['loss'][-(self.window_size + self.patience):-self.patience])\n",
        "\n",
        "      # If loss has not improved, reduce learning rate\n",
        "      if current_loss >= previous_loss and self.iterations_since_last_lr_update >= self.patience:\n",
        "          self.iterations_since_last_lr_update = 0\n",
        "          for param_group in optimizer.param_groups:\n",
        "              param_group['lr'] = max(param_group['lr'] * self.factor, self.min_lr)\n",
        "              self.lr = max(param_group['lr'] * self.factor, self.min_lr)\n",
        "              print(f\"Learning rate reduced to: {param_group['lr']:.6f}\")\n",
        "\n",
        "    def handle_gradients(self, model: nn.Module, optimizer: torch.optim.Optimizer, batch_idx: int):\n",
        "        \"\"\"Handle gradient updates and clipping\"\"\"\n",
        "\n",
        "\n",
        "        if batch_idx % self.n_steps_temporal_gradient_accumulation == 0:\n",
        "          # Gradient clipping\n",
        "          torch.nn.utils.clip_grad_norm_(\n",
        "              model.parameters(),\n",
        "              max_norm=self.gradient_clipping_max_norm,\n",
        "              error_if_nonfinite=True\n",
        "          )\n",
        "\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "    def update(self, model: nn.Module):\n",
        "        \"\"\"Update gradient statistics\"\"\"\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad = param.grad.detach()\n",
        "                self.max_gradients[name] = torch.max(torch.abs(grad)).item()\n",
        "                self.avg_gradients[name] = torch.mean(torch.abs(grad)).item()\n",
        "\n",
        "    def report(self, model):\n",
        "        \"\"\"Print gradient summary\"\"\"\n",
        "        max_gradients = {}\n",
        "        avg_gradients = {}\n",
        "        std_gradients = {}\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad = param.grad.detach()\n",
        "                max_gradients[name] = torch.max(torch.abs(grad)).item()\n",
        "                avg_gradients[name] = torch.mean(torch.abs(grad)).item()\n",
        "                std_gradients[name] = torch.std(torch.abs(grad)).item()\n",
        "\n",
        "\n",
        "        max_grad = max(max_gradients.values())\n",
        "        avg_grad = sum(avg_gradients.values()) / len(avg_gradients)\n",
        "        std_grad = sum(std_gradients.values()) / len(std_gradients)\n",
        "\n",
        "        print(f\"\\nGradient Summary:\")\n",
        "        print(f\"Max Gradient: {max_grad:.4e}, Max Gradients per layer: {max_gradients}\")\n",
        "        print(f\"Avg Gradient: {avg_grad:.4e}, Avg Gradient per layer: {avg_gradients}\")\n",
        "        print(f\"Std Gradient: {std_grad:.4e}, Avg Gradient per layer: {std_gradients}\")\n",
        "\n",
        "\n",
        "class TrainingIllustration:\n",
        "    \"\"\"Handles training visualization and reporting\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.__dict__.update(config)\n",
        "        # Plot metrics\n",
        "        self.metrics_to_plot = [\n",
        "                                ('loss', 'Training Loss'),\n",
        "                                ('predicted_earnings', 'Predicted Earnings (Probabilistic)'),\n",
        "                                ('predicted_earnings_greedy', 'Predicted Earnings (Greedy)'),\n",
        "                                ('predicted_earnings_value', 'Predicted Earnings (Value)'),\n",
        "                                ('roi_kelly', 'ROI Kelly Betting Method'),\n",
        "                                ('accuracy', 'Accuracy'),\n",
        "                                ('hrn', 'HRN'),\n",
        "                                ('spearman', 'Spearman Correlation'),\n",
        "                                # ('computation_depth', 'Computation Depth'),\n",
        "                                # ('n_nodes', 'Number of Nodes'),\n",
        "                                ('batch_time', 'Batch Time'),\n",
        "                                ('lr', 'Learning Rate'),\n",
        "                                ('avg_race_participants', 'Number of participants per race')\n",
        "                                ]\n",
        "\n",
        "    def handle_batch_operations(self, batch_idx: int, model: nn.Module,\n",
        "                          optimizer: torch.optim.Optimizer, stats: TrainingStats,\n",
        "                          cfg_optimizer: OptimConfig, model_folder: str, loss: torch.Tensor):\n",
        "        \"\"\"Handle periodic batch operations\"\"\"\n",
        "        # Visualization Computation Graph\n",
        "        if batch_idx % self.n_steps_graph == 0:\n",
        "            visualize_computation_graph(model, loss, model_folder, batch_idx)\n",
        "\n",
        "        # Reporting\n",
        "        if batch_idx % self.n_steps_gradient == 0:\n",
        "            cfg_optimizer.report(model)\n",
        "\n",
        "        # Display Plots\n",
        "        if batch_idx % self.n_steps_plot == 0 and batch_idx > self.window_size:\n",
        "            plot_training_stats(training_stats = stats.data,\n",
        "                                metrics = self.metrics_to_plot,\n",
        "                                window_size = self.window_size,\n",
        "                                plots_dir = None,\n",
        "                                config = None)\n",
        "\n",
        "        # Checkpointing\n",
        "        if batch_idx % self.n_steps_checkpoint == 0 and batch_idx > 0:\n",
        "            model_path = os.path.join(model_folder, f\"model_{batch_idx}.pt\")\n",
        "            save_training_checkpoint(model_folder, batch_idx, model, optimizer, stats.data)\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK_Fj-2ala54"
      },
      "outputs": [],
      "source": [
        "def model_forward(model: nn.Module, batch: Dict):\n",
        "    \"\"\"Forward pass through the model\"\"\"\n",
        "    race_data, horse_data, results_data, position, decimalPrice, crids, hids, hrn, date = batch\n",
        "    return model(batch)\n",
        "\n",
        "\n",
        "def handle_batch_operations(batch_idx: int, model: nn.Module,\n",
        "                          optimizer: torch.optim.Optimizer, stats: TrainingStats,\n",
        "                          grad_monitor: GradientMonitor, model_folder: str, loss: torch.Tensor):\n",
        "    \"\"\"Handle periodic batch operations\"\"\"\n",
        "    # Visualization\n",
        "    if batch_idx % 100 == 0 and batch_idx < -1 and False:\n",
        "        visualize_computation_graph(model, loss, model_folder, batch_idx)\n",
        "\n",
        "    # Reporting\n",
        "    if batch_idx % 10 == 0 and batch_idx > 0 and False:\n",
        "        stats.report()\n",
        "\n",
        "    # Display Graphs\n",
        "    if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "        plot_training_stats(stats.data)\n",
        "\n",
        "    # Checkpointing\n",
        "    if batch_idx % 1000 == 0 and batch_idx > 0:\n",
        "        model_path = os.path.join(model_folder, f\"model_{batch_idx}.pt\")\n",
        "        save_training_checkpoint(model_folder, batch_idx, model, optimizer, stats.data)\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "def visualize_computation_graph(model: nn.Module, loss: float,\n",
        "                              save_path: str, batch_idx: int):\n",
        "    \"\"\"Save computation graph visualization\"\"\"\n",
        "    try:\n",
        "\n",
        "        dot = make_dot(loss, params=dict(model.named_parameters()),\n",
        "                     show_attrs=False, show_saved=False)\n",
        "        dot.render(os.path.join(save_path, f\"graph_{batch_idx}\"), format=\"png\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save computation graph: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def create_directory_training_session(cfg: Any = None) -> str:\n",
        "    \"\"\"\n",
        "    Creates a new directory for storing training checkpoints and logs.\n",
        "    Also saves the training configuration as a JSON file.\n",
        "\n",
        "    Args:\n",
        "        path: Base path where the directory should be created\n",
        "        cfg: Configuration object (class) containing training settings\n",
        "\n",
        "    Returns:\n",
        "        The full path of the created directory\n",
        "    \"\"\"\n",
        "    try:\n",
        "      # path = cfg.train_follow['model_folder']\n",
        "      # suffix = cfg.train_follow['suffix_model_folder']\n",
        "      path = cfg.train_follow.model_folder\n",
        "      suffix = cfg.train_follow.suffix_model_folder\n",
        "\n",
        "      # Create a timestamped directory name\n",
        "      timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "      session_dir = os.path.join(path, f\"training_session_{timestamp}_{suffix}\")\n",
        "\n",
        "      # Create the directory\n",
        "      os.makedirs(session_dir, exist_ok=True)\n",
        "\n",
        "      # Create subdirectories\n",
        "      os.makedirs(os.path.join(session_dir, \"checkpoints\"), exist_ok=True)\n",
        "      os.makedirs(os.path.join(session_dir, \"logs\"), exist_ok=True)\n",
        "\n",
        "      # Save configuration if provided\n",
        "      if cfg is not None:\n",
        "          # Convert cfg to dictionary if it's not already\n",
        "          if not isinstance(cfg, dict):\n",
        "              try:\n",
        "                  cfg_dict = vars(cfg)  # Try to convert class to dict\n",
        "\n",
        "                  # Replace the function with its name\n",
        "                  cfg_dict['loss_function'] = cfg_dict['loss_function'].__name__\n",
        "\n",
        "                  # Convert DataConfig and ModelConfig to dictionaries\n",
        "                  cfg_dict['data'] = vars(cfg_dict['data'])\n",
        "                  cfg_dict['model'] = vars(cfg_dict['model'])\n",
        "                  cfg_dict['optimizer'] = vars(cfg_dict['optimizer'])\n",
        "                  cfg_dict['train_follow'] = vars(cfg_dict['train_follow'])\n",
        "\n",
        "              except TypeError:\n",
        "                  cfg_dict = {k: getattr(cfg, k) for k in dir(cfg) if not k.startswith('_')}\n",
        "          else:\n",
        "              cfg_dict = cfg\n",
        "\n",
        "          # Save as JSON\n",
        "          config_path = os.path.join(session_dir, \"training_config.json\")\n",
        "          with open(config_path, 'w') as f:\n",
        "              json.dump(cfg_dict, f, indent=4)\n",
        "\n",
        "      return session_dir\n",
        "\n",
        "    except OSError as e:\n",
        "        print(f\"Error creating directory: {e}\")\n",
        "        raise\n",
        "\n",
        "def move_to_device(batch: Dict, device: torch.device) -> Tuple:\n",
        "    \"\"\"Move batch tensors to specified device\"\"\"\n",
        "    return{k: batch[k].to(device) if isinstance(batch[k], torch.Tensor) else batch[k] for k in batch}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8SxtYc4fZ0h"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs40jDxSTDbG"
      },
      "outputs": [],
      "source": [
        "def training(training_cfg: Dict[str, Any]):\n",
        "    \"\"\"Enhanced training procedure with robust error handling\"\"\"\n",
        "    set_seed(seed = 42)\n",
        "\n",
        "    cfg = TrainingConfig(training_cfg)\n",
        "    device = get_device()\n",
        "\n",
        "    # Data loading\n",
        "    dataloader = cfg.data.get_dataloader()\n",
        "\n",
        "\n",
        "    # Model initialization\n",
        "    cfg.model.input_features = cfg.data.get_datafeatures()\n",
        "    model = HorseRacingModel(cfg.model).to(device)\n",
        "\n",
        "    # Optimizer initialization\n",
        "    optimizer = cfg.optimizer.init_optimizer(model)\n",
        "\n",
        "    # Load model and optimizer and initialise stats follower\n",
        "    if cfg.continue_training_from_checkpoint:\n",
        "        batch_idx_start, prev_stats = load_training_checkpoint( checkpoint_path = cfg.checkpoint_path, model = model, optimizer = optimizer)\n",
        "    else:\n",
        "        batch_idx_start = 0\n",
        "    stats = TrainingStats()\n",
        "\n",
        "    # Loss function initialization\n",
        "    criterion = cfg.loss_function\n",
        "\n",
        "    # Create directory of training session\n",
        "    cfg.model_folder = create_directory_training_session(copy.deepcopy(cfg))\n",
        "\n",
        "\n",
        "    for epoch in range(cfg.data.n_epochs):\n",
        "        model.train()\n",
        "        model.horse_embeddings.reset_embeddings()\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training Epoch\"), start = batch_idx_start):\n",
        "            start_time = time.time()\n",
        "\n",
        "            batch = move_to_device(batch, device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, nodes_depth = model(batch)\n",
        "            loss = criterion(outputs, batch)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward(retain_graph = True)\n",
        "\n",
        "            # Update statistics\n",
        "            stats.update(epoch, batch_idx, model, outputs, time.time() - start_time, nodes_depth, cfg.optimizer.lr, batch, loss)\n",
        "\n",
        "            # Update learning rate\n",
        "            cfg.optimizer.update_learning_rate(optimizer, stats, batch_idx)\n",
        "\n",
        "            # Batch operations\n",
        "            cfg.train_follow.handle_batch_operations(batch_idx, model, optimizer, stats, cfg.optimizer, cfg.model_folder, loss)\n",
        "\n",
        "            # Update weights\n",
        "            cfg.optimizer.handle_gradients(model, optimizer, batch_idx)\n",
        "\n",
        "\n",
        "data_config = {\n",
        "    'batch_size': 8,\n",
        "    'num_workers': 2,\n",
        "    'shuffle': False,\n",
        "    'drop_last': True,\n",
        "    'data_folder': \"/content/drive/My Drive/HorseRacing/Horse riding/data/clean data/\",\n",
        "    'start_crid': 0,\n",
        "    'max_crids': 100000000,\n",
        "    'input_solution': False,\n",
        "    'n_epochs': 1,\n",
        "    'cols_races_to_drop': [],\n",
        "    'cols_results_to_drop': [],\n",
        "    'cols_horses_to_drop': ['decimalPrice', 'isFav']\n",
        "}\n",
        "sequence_model_cfg = {\n",
        "    'emb_dim': 512,\n",
        "    'max_seq_depth': 12,\n",
        "    'max_nodes': np.inf,\n",
        "    'max_depth_nodes': np.inf,\n",
        "    'input_embedding': False,\n",
        "    'sequence_model_type': 'Transformer',\n",
        "    'Transformer_config':{\n",
        "            'n_heads': 8,\n",
        "            'n_layers': 4,\n",
        "            'dropout': 0.2,\n",
        "            'multiple_ff': 4\n",
        "            },\n",
        "    'LSTM_config':{\n",
        "            'n_layers': 4,\n",
        "            'dropout': 0.2,\n",
        "            'hidden_dim': 128\n",
        "            }\n",
        "    }\n",
        "\n",
        "graph_conv_model_cfg = {\n",
        "    'emb_dim': 512,\n",
        "    'dropout': 0.2,\n",
        "    'n_layers': 4,\n",
        "    'layers_type': 'GCN', # GCN, Transformer, GCNII\n",
        "    'GCNII_config':{\n",
        "            'lamda':0.5,\n",
        "            'alpha': 0.1,\n",
        "            'variant': True,\n",
        "            'residual': True,\n",
        "            'self_loop': True\n",
        "            },\n",
        "    'GNN_Transformer_config':{\n",
        "            'n_heads': 8,\n",
        "            'n_layers': 4,\n",
        "            'dropout': 0.2,\n",
        "            'multiple_ff': 4\n",
        "            },\n",
        "    'GCN_config':{\n",
        "            'ff_layer' : True,\n",
        "            'multiple_ff': 4\n",
        "            }\n",
        "          }\n",
        "\n",
        "model_cfg = {\n",
        "    'emb_dim': 512,\n",
        "    'logits_size': 1,\n",
        "    'node_embedding_input': False,\n",
        "    'sequence_model': sequence_model_cfg,\n",
        "    'graph_conv_model': graph_conv_model_cfg,\n",
        "}\n",
        "\n",
        "\n",
        "optimizer_cfg = {\n",
        "    'optimizer_name': 'Adam',\n",
        "    'Adam_cfg':\n",
        "    {\n",
        "        'betas': (0.9, 0.999),\n",
        "        'eps': 1e-8,\n",
        "        'weight_decay': 0.0001,\n",
        "    },\n",
        "    'SGD_cfg':\n",
        "    {\n",
        "        'momentum': 0.9,\n",
        "        'weight_decay': 0.0001\n",
        "    },\n",
        "    'initial_lr': 0.0001,\n",
        "    'amsgrad': False,\n",
        "    'patience': 1000,\n",
        "    'factor': 0.5,\n",
        "    'min_lr': 1e-6,\n",
        "    'window_size': 300,\n",
        "    'n_steps_temporal_gradient_accumulation': 4,\n",
        "    'gradient_clipping_max_norm': 2,\n",
        "    'n_steps_cold_start': 300\n",
        "    }\n",
        "\n",
        "training_follow_config = {\n",
        "    'n_steps_gradient': 100,\n",
        "    'n_steps_graph': 100000000,\n",
        "    'n_steps_report': 100000000,\n",
        "    'n_steps_plot': 100,\n",
        "    'window_size': 50,\n",
        "    'n_steps_checkpoint': 100,\n",
        "    'model_folder': \"/content/drive/My Drive/HorseRacing/Horse riding/new_models/\",\n",
        "    'suffix_model_folder': 'test_gcn_big_1_continue',\n",
        "}\n",
        "\n",
        "training_config = {\n",
        "    \"optimizer\": optimizer_cfg,\n",
        "    \"loss_function\": loss_function_first_horse_classification,\n",
        "    \"data\":data_config,\n",
        "    \"model\":model_cfg,\n",
        "    \"training_follow\": training_follow_config,\n",
        "    \"continue_training_from_checkpoint\": True,\n",
        "    \"checkpoint_path\": \"/content/drive/My Drive/HorseRacing/Horse riding/new_models/training_session_20250407_130333_test_gcn_big_1/checkpoints/checkpoint_4400.pt\",\n",
        "}\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "# with torch.autograd.set_detect_anomaly(False):\n",
        "  # training(training_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUJm0nOAcayZ"
      },
      "source": [
        "### TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q7PBzpWRG1s"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class TestingConfig:\n",
        "    \"\"\"Validated training configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.__dict__.update(config)\n",
        "\n",
        "        self.data = DataConfig(config['data'])\n",
        "        self.model = ModelConfig(config['model'])\n",
        "        self.test_follow = TestingIllustration(config['testing_follow_config'])\n",
        "\n",
        "class DataConfig:\n",
        "    \"\"\"Validated data configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.__dict__.update(config)\n",
        "\n",
        "    def get_dataloader(self):\n",
        "\n",
        "        dataset = HorseRacingDataset(self)\n",
        "        self.features = dataset.features\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size = self.batch_size,\n",
        "            shuffle = self.shuffle,\n",
        "            collate_fn = lambda b: collate_fn(b, dataset),\n",
        "            num_workers = self.num_workers,\n",
        "            drop_last = self.drop_last\n",
        "        )\n",
        "        return dataloader\n",
        "    def get_datafeatures(self):\n",
        "        return self.features\n",
        "\n",
        "class TestingStats:\n",
        "    \"\"\"Handles training statistics collection and reporting\"\"\"\n",
        "    def __init__(self):\n",
        "        self.data = defaultdict(list)\n",
        "        self.metrics = [\n",
        "            'loss', 'total_winnings', 'total_betted',\n",
        "            'predicted_earnings', 'predicted_earnings_greedy',\n",
        "            'hrn', 'accuracy', 'spearman', 'batch_time', 'n_nodes'\n",
        "        ]\n",
        "        self.df_outputs = pd.DataFrame(columns = ['crid', 'rid', 'hrn', 'position', 'prices', 'prediction'])\n",
        "    def update(self, epoch: int, batch_idx: int, model: nn.Module, outputs: torch.Tensor,\n",
        "                batch_time: float, nodes_depth: int, batch: Dict, loss: torch.Tensor):\n",
        "        \"\"\" Compute metrics \"\"\"\n",
        "        with torch.no_grad():\n",
        "            device = outputs.device\n",
        "            positions_arrival = batch['positions']\n",
        "            decimal_prices = batch['prices']\n",
        "            hrn = batch['hrn']\n",
        "            hids = batch['hids']\n",
        "            crids = batch['crid']\n",
        "\n",
        "            batch_size, num_horses, n_ranks = outputs.shape\n",
        "            if n_ranks > 1:\n",
        "              outputs = outputs[:,:,0]\n",
        "            outputs = outputs.squeeze(-1)\n",
        "            # Mask out logits for horses with position -1 (excluded from softmax)\n",
        "            mask_out = (positions_arrival == -1.0)\n",
        "            outputs = outputs.masked_fill(mask_out, -1e9)\n",
        "\n",
        "            # Races with exactly one valid winner\n",
        "            winner_mask = (positions_arrival == 1.0)\n",
        "            valid_races = winner_mask.sum(dim=1) == 1\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Accuracy calculation\n",
        "            valid_winner_races = winner_mask.any(dim=1)\n",
        "\n",
        "            if valid_winner_races.any():\n",
        "                # Accuracy\n",
        "                pred_winners = probs.argmax(dim=1)\n",
        "                correct = winner_mask[torch.arange(batch_size), pred_winners]\n",
        "                accuracy = correct[valid_winner_races].float().mean()\n",
        "\n",
        "                # Probabilistic betting. Bet the probability of winning on the horse.\n",
        "                selected_probs = probs[winner_mask]\n",
        "                selected_decimalprices = decimal_prices[winner_mask]\n",
        "                winnings_prob = (selected_probs / selected_decimalprices).sum()\n",
        "                total_betted_prob = batch_size\n",
        "\n",
        "                # Value betting. Bet on the horse with a probability higher than the decimal price\n",
        "                valid_horses_mask = (positions_arrival != -1.0) & (positions_arrival != 40.0)\n",
        "                value_bets_mask = (probs > decimal_prices) & valid_horses_mask\n",
        "                won_value_bets = value_bets_mask & (positions_arrival == 1.0)\n",
        "                winnings_value = (won_value_bets.float() / decimal_prices).sum()\n",
        "                total_betted_value = value_bets_mask.sum().float()\n",
        "\n",
        "                # Greedy betting. Bet full on the horse with the highest probability\n",
        "                pred_winners = probs.argmax(dim=1)\n",
        "                winnings_greedy = (positions_arrival[torch.arange(batch_size), pred_winners] == 1.0)\n",
        "                decimalPrice_greedy = decimal_prices[torch.arange(batch_size), pred_winners]\n",
        "                winnings_greedy = (winnings_greedy.float() / decimalPrice_greedy).sum()\n",
        "                total_betted_greedy = batch_size\n",
        "\n",
        "                # Kelly Criterion betting\n",
        "                # Create valid horses mask (adjust based on your data conventions)\n",
        "                valid_horses_mask = (positions_arrival != -1.0) & (positions_arrival != 40.0)\n",
        "\n",
        "                # Value betting mask (where our probability estimate is better than market)\n",
        "                value_bets_mask = (probs > decimal_prices) & valid_horses_mask\n",
        "\n",
        "                # Calculate Kelly criterion fractions\n",
        "                b = 1 / decimal_prices  # Net odds received on the wager (profit per unit bet if you win)\n",
        "                edge = probs * (b + 1) - 1  # Simplified Kelly numerator\n",
        "                kelly_fractions = edge / b  # Kelly formula: (bp - q)/b\n",
        "\n",
        "                # Apply masks and clamp values\n",
        "                kelly_fractions = torch.where(value_bets_mask, kelly_fractions, 0.0)\n",
        "                kelly_fractions = torch.clamp(kelly_fractions, min=0.0, max=1.0)\n",
        "\n",
        "                # Calculate actual winnings and losses\n",
        "                won_bets = (positions_arrival == 1.0) & value_bets_mask\n",
        "                returns = kelly_fractions * won_bets / decimal_prices\n",
        "                losses = kelly_fractions * (~won_bets & value_bets_mask)\n",
        "\n",
        "                # Aggregate results\n",
        "                total_betted_kelly = kelly_fractions.sum()\n",
        "                total_return_kelly = returns.sum()\n",
        "                net_profit_kelly = total_return_kelly - total_betted_kelly\n",
        "                roi_kelly = net_profit_kelly / total_betted_kelly if total_betted_kelly > 0 else 0.0\n",
        "\n",
        "            else:\n",
        "                accuracy = torch.tensor(0.0, device=device)\n",
        "\n",
        "                winnings_prob = torch.tensor(0.0, device=device)\n",
        "                total_betted_prob = torch.tensor(0.0, device=device)\n",
        "\n",
        "                winnings_greedy = torch.tensor(0.0, device=device)\n",
        "                total_betted_greedy = torch.tensor(0.0, device=device)\n",
        "\n",
        "                total_betted_value = torch.tensor(0.0, device=device)\n",
        "                winnings_value = torch.tensor(0.0, device=device)\n",
        "\n",
        "                roi_kelly = torch.tensor(0.0, device=device)\n",
        "\n",
        "            predicted_earnings_prob = (winnings_prob - total_betted_prob) / (total_betted_prob + 1e-8)\n",
        "            predicted_earnings_greedy = (winnings_greedy - total_betted_greedy)/ (total_betted_greedy + 1e-8)\n",
        "            predicted_earnings_value = (winnings_value - total_betted_value)/(total_betted_value + 1e-8)\n",
        "\n",
        "            # Ranking correlation\n",
        "            spearman = self.spearman_rank_correlation(outputs.detach().clone(), positions_arrival)\n",
        "            n_nodes_computation_graph =  self.number_nodes_computation_graph(model, loss)\n",
        "\n",
        "            \"\"\"Update statistics\"\"\"\n",
        "            self.data['epoch'].append(epoch)\n",
        "            self.data['batch'].append(batch_idx)\n",
        "            self.data['batch_time'].append(batch_time)\n",
        "            self.data['computation_depth'].append(nodes_depth)\n",
        "            self.data['hrn'].append(hrn[hrn != -1].float().mean().item())\n",
        "            self.data['n_nodes'].append(n_nodes_computation_graph)\n",
        "\n",
        "            self.data['accuracy'].append(accuracy.item())\n",
        "            self.data['predicted_earnings'].append(predicted_earnings_prob.item())\n",
        "            self.data['predicted_earnings_greedy'].append(predicted_earnings_greedy.item())\n",
        "            self.data['predicted_earnings_value'].append(predicted_earnings_value.item())\n",
        "            self.data['roi_kelly'].append(roi_kelly.item())\n",
        "            self.data['spearman'].append(spearman.item())\n",
        "            self.data['avg_race_participants'].append(mask_out.sum().item()/batch_size)\n",
        "\n",
        "            self.data['loss'].append(loss.clone().item())\n",
        "\n",
        "            # Extract batch data with device awareness\n",
        "            hids = batch['hids'].cpu().numpy()\n",
        "            crids = batch['crid'].cpu().numpy()\n",
        "            positions = batch['positions'].cpu().numpy()\n",
        "            prices = batch['prices'].cpu().numpy()\n",
        "            predictions = outputs.detach().cpu().numpy()\n",
        "\n",
        "            # Reshape arrays\n",
        "            predictions_flat = predictions.reshape(-1)\n",
        "            hids_flat = hids.reshape(-1)\n",
        "            crids_flat = crids.reshape(-1)\n",
        "            positions_flat = positions.reshape(-1)\n",
        "            prices_flat = prices.reshape(-1)\n",
        "\n",
        "            # Create mask for valid hids\n",
        "            valid_mask = hids_flat != -1\n",
        "\n",
        "            # Create temporary dataframe\n",
        "            temp_df = pd.DataFrame({\n",
        "                'hid': hids_flat[valid_mask],\n",
        "                'crid': crids_flat[valid_mask],\n",
        "                'position': positions_flat[valid_mask],\n",
        "                'decimalPrice': prices_flat[valid_mask],\n",
        "                'prediction': predictions_flat[valid_mask]\n",
        "            })\n",
        "\n",
        "            # Update main dataframe\n",
        "            self.df_outputs = pd.concat(\n",
        "                [self.df_outputs, temp_df],\n",
        "                ignore_index=True\n",
        "            )\n",
        "\n",
        "    def save_results_df(self, folder: str, filename):\n",
        "        \"\"\"Save predictions dataframe to CSV\"\"\"\n",
        "        path = os.path.join(folder, f\"df_predictions_{filename}.csv\")\n",
        "        self.df_outputs.to_csv(path, index=False)\n",
        "        print(f\"Predictions saved to {path}\")\n",
        "\n",
        "\n",
        "    def number_nodes_computation_graph(self, model: nn.Module, loss: float) -> int:\n",
        "        try:\n",
        "            # dot = make_dot(loss, params=dict(model.named_parameters()),\n",
        "            #              show_attrs=False, show_saved=False)\n",
        "            # return len(dot.source)\n",
        "            return 0\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to count number of nodes in computation graph: {e}\")\n",
        "\n",
        "    def spearman_rank_correlation(self, logits: torch.Tensor,\n",
        "                             positions_arrival: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes Spearman's rank correlation between predicted logits and actual positions,\n",
        "        ignoring invalid entries (-1 or 40). Handles variable participant counts per race.\n",
        "\n",
        "        Args:\n",
        "            logits: Tensor of shape [batch_size, num_horses] with prediction scores\n",
        "            positions_arrival: Tensor of shape [batch_size, num_horses] with actual positions\n",
        "\n",
        "        Returns:\n",
        "            Mean Spearman's rho across batch (valid races only) as torch.Tensor\n",
        "        \"\"\"\n",
        "        device = logits.device\n",
        "        batch_size = logits.size(0)\n",
        "        correlations = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Filter valid entries for this race\n",
        "            mask = (positions_arrival[i] != -1.0) & (positions_arrival[i] != 40.0)\n",
        "            race_logits = logits[i][mask].detach().cpu().numpy()\n",
        "            race_positions = positions_arrival[i][mask].cpu().numpy()\n",
        "\n",
        "            # Skip races with <2 valid participants\n",
        "            if len(race_logits) < 2:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Generate predicted ranks from logits (higher logit = better rank)\n",
        "                pred_ranks = (-race_logits).argsort().argsort()  # Double argsort for rank\n",
        "\n",
        "                # Calculate Spearman correlation\n",
        "                rho, _ = spearmanr(pred_ranks, race_positions)\n",
        "\n",
        "                # Handle NaN/edge cases\n",
        "                if np.isnan(rho):\n",
        "                    rho = 0.0\n",
        "            except:\n",
        "                rho = 0.0\n",
        "\n",
        "            correlations.append(rho)\n",
        "\n",
        "        # Return average across valid races\n",
        "        if not correlations:\n",
        "            return torch.tensor(0.0, device=device)\n",
        "        return torch.tensor(np.mean(correlations), device=device)\n",
        "\n",
        "    def report(self, window_size: int = 100):\n",
        "        \"\"\"Print formatted training statistics\"\"\"\n",
        "        if len(self.data['loss']) < window_size:\n",
        "            return\n",
        "        print(f\"Epoch: {self.data['epoch'][-1]}, Batch: {self.data['batch'][-1]}, Loss:{round(np.mean(self.data['loss'][-window_size:]),2)}, Betting:{round(np.mean(self.data['predicted_earnings'][-window_size:]),2)}, Greedy Betting:{round(np.mean(self.data['predicted_earnings_greedy'][-window_size:]),2)}, HRN:{round(np.mean(self.data['hrn'][-window_size:]),2)} \")\n",
        "\n",
        "\n",
        "class ModelConfig:\n",
        "    \"\"\"Validated model configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.__dict__.update(config)\n",
        "\n",
        "\n",
        "class TestingIllustration:\n",
        "    \"\"\"Handles training visualization and reporting\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.__dict__.update(config)\n",
        "        # Plot metrics\n",
        "        self.metrics_to_plot = [\n",
        "                                ('loss', 'Training Loss'),\n",
        "                                ('predicted_earnings', 'Predicted Earnings (Probabilistic)'),\n",
        "                                ('predicted_earnings_greedy', 'Predicted Earnings (Greedy)'),\n",
        "                                ('predicted_earnings_value', 'Predicted Earnings (Value)'),\n",
        "                                ('roi_kelly', 'ROI Kelly Betting Method'),\n",
        "                                ('accuracy', 'Accuracy'),\n",
        "                                ('hrn', 'HRN'),\n",
        "                                ('spearman', 'Spearman Correlation'),\n",
        "                                # ('computation_depth', 'Computation Depth'),\n",
        "                                # ('n_nodes', 'Number of Nodes'),\n",
        "                                ('batch_time', 'Batch Time'),\n",
        "                                ('avg_race_participants', 'Number of participants per race')\n",
        "                                ]\n",
        "\n",
        "    def handle_batch_operations(self, batch_idx: int, model: nn.Module,\n",
        "                          stats: TrainingStats):\n",
        "        \"\"\"Handle periodic batch operations\"\"\"\n",
        "\n",
        "        # Display Plots\n",
        "        if batch_idx % self.n_steps_plot == 0 and batch_idx > self.window_size:\n",
        "            plot_training_stats(training_stats = stats.data,\n",
        "                                metrics = self.metrics_to_plot,\n",
        "                                window_size = self.window_size,\n",
        "                                plots_dir = None,\n",
        "                                config = None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egNPKRy_MsJg",
        "outputId": "249ce34d-9737-4221-87f2-dc6158e5df25"
      },
      "outputs": [],
      "source": [
        "def list_files_in_folder(folder_path):\n",
        "    try:\n",
        "        # List all files in the given folder\n",
        "        files = os.listdir(folder_path)\n",
        "        pt_files = [os.path.join(folder_path, f) for f in files if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.pt')]\n",
        "        return pt_files\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "def testing(training_cfg: Dict[str, Any]):\n",
        "    \"\"\"Enhanced training procedure with robust error handling\"\"\"\n",
        "    set_seed(seed = 42)\n",
        "\n",
        "    cfg = TestingConfig(training_cfg)\n",
        "    device = get_device()\n",
        "\n",
        "    list_checkpoints_to_test = list_files_in_folder(cfg.checkpoints_path)\n",
        "    print('list ch', list_checkpoints_to_test)\n",
        "    # Data loading\n",
        "    dataloader = cfg.data.get_dataloader()\n",
        "\n",
        "    # Model initialization\n",
        "    cfg.model.input_features = cfg.data.get_datafeatures()\n",
        "    model = HorseRacingModel(cfg.model).to(device)\n",
        "\n",
        "    # Loss function initialization\n",
        "    criterion = cfg.loss_function\n",
        "\n",
        "\n",
        "\n",
        "    for file_ckp in list_checkpoints_to_test:\n",
        "        print(f'Starting to test: {file_ckp}')\n",
        "        # Load model\n",
        "        batch_idx_model, stats_model = load_training_checkpoint( checkpoint_path = file_ckp, model = model, optimizer = None)\n",
        "\n",
        "        model.eval()\n",
        "        model.horse_embeddings.reset_embeddings()\n",
        "        with torch.no_grad():\n",
        "          batch_idx_start = 0\n",
        "          stats = TestingStats()\n",
        "          for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training Epoch\"), start = batch_idx_start):\n",
        "              start_time = time.time()\n",
        "\n",
        "              batch = move_to_device(batch, device)\n",
        "\n",
        "              # Forward pass\n",
        "              outputs, nodes_depth = model(batch)\n",
        "              loss = criterion(outputs, batch)\n",
        "\n",
        "              # Update statistics\n",
        "              stats.update(0, batch_idx, model, outputs, time.time() - start_time, nodes_depth, batch, loss)\n",
        "\n",
        "              # Batch operations\n",
        "              cfg.test_follow.handle_batch_operations(batch_idx, model, stats)\n",
        "        filename = os.path.basename(file_ckp)\n",
        "        stats.save_results_df(folder = cfg.checkpoints_path, filename = filename)\n",
        "\n",
        "\n",
        "data_config = {\n",
        "    'batch_size': 8,\n",
        "    'num_workers': 2,\n",
        "    'shuffle': False,\n",
        "    'drop_last': True,\n",
        "    'data_folder': \"/content/drive/My Drive/HorseRacing/Horse riding/data/clean data/\",\n",
        "    'start_crid': 0,\n",
        "    'max_crids': 50000,\n",
        "    'input_solution': False,\n",
        "    'n_epochs': 1,\n",
        "    'cols_races_to_drop': [],\n",
        "    'cols_results_to_drop': [],\n",
        "    'cols_horses_to_drop': ['decimalPrice', 'isFav']\n",
        "}\n",
        "sequence_model_cfg = {\n",
        "    'emb_dim': 512,\n",
        "    'max_seq_depth': 12,\n",
        "    'max_nodes': np.inf,\n",
        "    'max_depth_nodes': np.inf,\n",
        "    'input_embedding': False,\n",
        "    'sequence_model_type': 'Transformer',\n",
        "    'Transformer_config':{\n",
        "            'n_heads': 8,\n",
        "            'n_layers': 4,\n",
        "            'dropout': 0.2,\n",
        "            'multiple_ff': 4\n",
        "            },\n",
        "    'LSTM_config':{\n",
        "            'n_layers': 4,\n",
        "            'dropout': 0.2,\n",
        "            'hidden_dim': 128\n",
        "            }\n",
        "    }\n",
        "\n",
        "graph_conv_model_cfg = {\n",
        "    'emb_dim': 512,\n",
        "    'dropout': 0.2,\n",
        "    'n_layers': 4,\n",
        "    'layers_type': 'GCN', # GCN, Transformer, GCNII\n",
        "    'GCNII_config':{\n",
        "            'lamda':0.5,\n",
        "            'alpha': 0.1,\n",
        "            'variant': True,\n",
        "            'residual': True,\n",
        "            'self_loop': True\n",
        "            },\n",
        "    'GNN_Transformer_config':{\n",
        "            'n_heads': 8,\n",
        "            'n_layers': 4,\n",
        "            'dropout': 0.2,\n",
        "            'multiple_ff': 4\n",
        "            },\n",
        "    'GCN_config':{\n",
        "            'ff_layer' : True,\n",
        "            'multiple_ff': 4\n",
        "            }\n",
        "          }\n",
        "\n",
        "model_cfg = {\n",
        "    'emb_dim': 512,\n",
        "    'logits_size': 1,\n",
        "    'node_embedding_input': False,\n",
        "    'sequence_model': sequence_model_cfg,\n",
        "    'graph_conv_model': graph_conv_model_cfg,\n",
        "}\n",
        "\n",
        "\n",
        "testing_follow_config = {\n",
        "    'n_steps_gradient': 100,\n",
        "    'n_steps_graph': 100000000,\n",
        "    'n_steps_report': 100000000,\n",
        "    'n_steps_plot': 1000,\n",
        "    'window_size': 50,\n",
        "    'n_steps_checkpoint': 100,\n",
        "    'model_folder': \"/content/drive/My Drive/HorseRacing/Horse riding/new_models/\",\n",
        "    'suffix_model_folder': 'test_gcn_big_1_continue',\n",
        "}\n",
        "\n",
        "testing_config = {\n",
        "    \"loss_function\": loss_function_first_horse_classification,\n",
        "    \"data\":data_config,\n",
        "    \"model\":model_cfg,\n",
        "    \"testing_follow_config\": testing_follow_config,\n",
        "    \"continue_training_from_checkpoint\": True,\n",
        "    \"checkpoints_path\": \"/content/drive/My Drive/HorseRacing/Horse riding/new_models/training_session_20250407_130333_test_gcn_big_1/checkpoints\",\n",
        "    \"start_crid_id\": 0,\n",
        "    \"end_crid_id\": 50000\n",
        "}\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "with torch.autograd.set_detect_anomaly(False):\n",
        "  testing(testing_config)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
