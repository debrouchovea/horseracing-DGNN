# horseracing-DGNN
In this personnal research project one tries to predict the results of horse races based on the previous performances of each horse. To do this, we use a special type of dynamic graph neural networks, made especially for this type of dataset. We assume the horses being nodes, of which the states change after each race he participates in. We assume the edges to be the races. Before each race, the horses are represented by embeddings that are made of a concatenation of the the saved state of the model, information about the horse at the begin of the race and information about the race. Those embeddings are then compared with each other using CGNN layers. The output of these CGNN layers are used for two things. 1) To predict the position of arrival of the horse during this race. 2) To update the state of the horse, using an LSTM. Each race is considered as a snapshot of the Dynamic Graph, since only the nodes (horses) participating in the race are being updated, only the horses participating in the specific race are loaded and participate in the backward pass for each training step. Each node, is represented by its performances in the previous races he participated in, using a "horse embedding". Since each "horse embedding" is created by comparing the performances of the horse with other horses during his previous races, the computation graph increases O($n^2$) with n being the amount of races the model was trained on. This leads during training to rapidly increasing memory usage and computation time per race. Checkpoints were used to address the issue of memory usage, even though using checkpoints increases slightly the computation time by repassing through the network when performing backward passes. Checkpoints however don't solve the issue of increasing computation time, since the computation still grows rapidly. To solve this, we cut the backward pass at specific points in the graph. By having independent subgraphs, the model tends to have a stable iteration time during training. The computation graph is divided in subgraphs, by detaching some specifically chosen "horse embeddings" from their original computation graph. This method shows that information between races can successfully be passed. The project is still in progress, however the results are promissing. 

This paper presents a novel approach to predicting horse race outcomes by modeling historical performance data using a dynamic graph neural network (DyGNN). Horses are represented as nodes in a temporal graph, where edges correspond to races, and node states evolve dynamically as horses participate in successive races. Each horse’s embedding is constructed by concatenating three components: (1) a learned state vector updated via an LSTM after each race, (2) features of the horse at the time of the race (e.g., age, breed), and (3) dynamic race-specific conditions (e.g., track surface, distance). These embeddings are processed through convolutional graph neural network (CGNN) layers to model interactions between horses in a race, enabling two key tasks:

1) Rank prediction: Estimating the finishing position of each horse.

2) State propagation: Updating the horse’s hidden state via the LSTM to capture performance trends. The race results are concatenated the the LSTM's input, in order to propagate the race results.

To address computational challenges inherent in dynamic graphs, we propose a snapshot-based training framework where only participating horses and their recent history are loaded for each race. However, since the model’s computational graph grows quadratically (O($n^2$)) with the number of historical races (due to comparisons), we introduce two optimizations:

1) Selective gradient checkpointing: Reducing memory overhead by recomputing intermediate states during backpropagation.

2) Computation graph partitioning: Isolating subgraphs by strategically detaching embeddings from earlier races, thereby truncating backward passes and stabilizing training time.

Although this partitioning limits long-term dependency learning, empirical results demonstrate that the truncated gradients still enable effective state propagation across races. Preliminary experiments shows the training time per epoch stabilized at O(1) after partitioning. While the project is ongoing, this work highlights the potential of DyGNNs for modeling sequential, interaction-heavy systems like competitive sports. 
