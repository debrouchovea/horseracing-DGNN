{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnmmudHdhqBX",
        "outputId": "93181078-fee9-4307-a4d8-4e77b5516534"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math as math\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Optional, Tuple, Any, List, Union\n",
        "import copy\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import logging\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass, field\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import spearmanr\n",
        "import sys as sys\n",
        "import pickle as pkl\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !pip install torchviz\n",
        "# from torchviz import make_dot\n",
        "\n",
        "# !pip install memory_profiler\n",
        "# %load_ext memory_profiler\n",
        "\n",
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.13-cp311-cp311-linux_x86_64.whl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr3ikEdNjOzq"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jszvF6QC2ulN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_device(prefer_device: Optional[str] = None) -> torch.device:\n",
        "    \"\"\"\n",
        "    Detect and return available compute device with priority order:\n",
        "    TPU > GPU > CPU\n",
        "\n",
        "    Args:\n",
        "        prefer_device: Optional preferred device ('tpu', 'gpu', 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        torch.device: Best available device\n",
        "    \"\"\"\n",
        "    device_order = []\n",
        "\n",
        "    # Determine detection order based on preference\n",
        "    if prefer_device:\n",
        "        device_order.append(prefer_device.lower())\n",
        "    device_order += ['tpu', 'cuda', 'mps', 'cpu']\n",
        "\n",
        "    for device_type in device_order:\n",
        "        try:\n",
        "            if device_type == 'tpu':\n",
        "                import torch_xla\n",
        "                import torch_xla.core.xla_model as xm\n",
        "\n",
        "                device = xm.xla_device()\n",
        "                print(f\"Using TPU: {device}\")\n",
        "                return device\n",
        "\n",
        "            elif device_type == 'cuda' and torch.cuda.is_available():\n",
        "                device = torch.device(\"cuda\")\n",
        "                print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
        "                return device\n",
        "\n",
        "            elif device_type == 'mps' and torch.backends.mps.is_available():\n",
        "                device = torch.device(\"mps\")\n",
        "                print(\"Using Apple MPS\")\n",
        "                return device\n",
        "\n",
        "            elif device_type == 'cpu':\n",
        "                device = torch.device(\"cpu\")\n",
        "                print(\"Using CPU\")\n",
        "                return device\n",
        "\n",
        "        except ImportError:\n",
        "            continue\n",
        "\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "def set_seed(seed: int, deterministic: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Set random seeds for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        seed: Random seed value\n",
        "        deterministic: Enable deterministic algorithms (may impact performance)\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        if deterministic:\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Seeds initialized to {seed} with {'deterministic' if deterministic else 'normal'} mode\")\n",
        "\n",
        "def save_training_checkpoint(\n",
        "    session_dir: str,\n",
        "    batch_idx: int,\n",
        "    model: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    stats: Dict[str, Any],\n",
        "):\n",
        "    \"\"\"\n",
        "    Saves training checkpoint, statistics, and generates plots.\n",
        "\n",
        "    Args:\n",
        "        session_dir: Directory created by create_directory_training_session\n",
        "        batch_idx: Current batch_idx number\n",
        "        model: Model to save\n",
        "        optimizer: Optimizer to save\n",
        "        stats: Dictionary containing training statistics\n",
        "    \"\"\"\n",
        "    checkpoint_dir = os.path.join(session_dir, \"checkpoints\")\n",
        "    plots_dir = os.path.join(session_dir, \"plots\")\n",
        "    os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Save model checkpoint\n",
        "    checkpoint = {\n",
        "        'batch_idx': batch_idx,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'stats': stats,\n",
        "        'model_type': type(model).__name__,\n",
        "    }\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_{batch_idx}.pt\")\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # 2. Save statistics to JSON\n",
        "    stats_path = os.path.join(session_dir, \"training_stats.json\")\n",
        "    with open(stats_path, 'w') as f:\n",
        "        json.dump(stats, f, indent=4)\n",
        "\n",
        "    # 3. Generate and save plots\n",
        "    plot_path = os.path.join(plots_dir, f\"training_plots_{batch_idx}.png\")\n",
        "    plot_training_stats(training_stats = stats,\n",
        "                       window_size = 50,\n",
        "                       config = None,\n",
        "                       plots_dir = plot_path)\n",
        "    print('Successfully saved model')\n",
        "\n",
        "\n",
        "def load_training_checkpoint(\n",
        "    checkpoint_path: str,\n",
        "    model: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        ") -> Tuple[int, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Loads a training checkpoint from a specific file path.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Full path to the checkpoint file\n",
        "        model: Model instance to load weights into\n",
        "        optimizer: Optimizer instance to load state into\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - int: The batch index of the loaded checkpoint\n",
        "        - dict: The statistics dictionary from the checkpoint\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If specified checkpoint doesn't exist\n",
        "        TypeError: If saved model type doesn't match current model type\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
        "\n",
        "    # Determine device to load on\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:  # Model has no parameters\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Verify model compatibility\n",
        "    saved_model_type = checkpoint.get(\"model_type\")\n",
        "    current_model_type = type(model).__name__\n",
        "    if saved_model_type != current_model_type:\n",
        "        raise TypeError(\n",
        "            f\"Model type mismatch: Saved model '{saved_model_type}', \"\n",
        "            f\"Current model '{current_model_type}'\"\n",
        "        )\n",
        "\n",
        "    # Load states\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "    return checkpoint[\"batch_idx\"], checkpoint[\"stats\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxwaAifUVbKg"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class PlotConfig:\n",
        "    \"\"\"Configuration for plotting training statistics\"\"\"\n",
        "    figsize: tuple = (10, 6)\n",
        "    linewidth: float = 2.0\n",
        "    fontsize: int = 12\n",
        "    dpi: int = 100\n",
        "    colors: List[str] = field(default_factory=lambda: ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
        "                                                       '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']) # Use default_factory to create a new list for each instance\n",
        "\n",
        "def smooth_data(data: np.ndarray, window_size: int = 20) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Smooth data using a simple moving average with proper edge handling.\n",
        "\n",
        "    Args:\n",
        "        data: Input data array\n",
        "        window_size: Size of the moving average window\n",
        "\n",
        "    Returns:\n",
        "        Smoothed data array\n",
        "    \"\"\"\n",
        "    if not isinstance(data, np.ndarray):\n",
        "      data = np.array(data, dtype=np.float64) # Ensure data is numeric\n",
        "\n",
        "    if window_size > len(data):\n",
        "        raise ValueError(\"Window size cannot be larger than data length\")\n",
        "\n",
        "    # Create a masked array where NaNs are masked\n",
        "    masked_data = np.ma.masked_array(data, np.isnan(data))\n",
        "\n",
        "    # Perform convolution on the masked array\n",
        "    result = np.convolve(masked_data.filled(0), np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "    # Adjust the result to account for the masked values\n",
        "    mask = np.convolve(~masked_data.mask, np.ones(window_size), mode='valid')\n",
        "    result = np.ma.masked_array(result, mask == 0)\n",
        "\n",
        "    return result\n",
        "\n",
        "def plot_metric(ax: plt.Axes, x: np.ndarray, y: np.ndarray,\n",
        "                label: str, color: str, config: PlotConfig) -> None:\n",
        "    \"\"\"\n",
        "    Helper function to plot a single metric.\n",
        "    \"\"\"\n",
        "    ax.plot(x, y, label=label, color=color,\n",
        "            linewidth=config.linewidth)\n",
        "    ax.set_xlabel('Training Step', fontsize=config.fontsize)\n",
        "    ax.set_ylabel(label, fontsize=config.fontsize)\n",
        "    ax.set_title(f'{label} Over Training Steps', fontsize=config.fontsize+2)\n",
        "    ax.legend(fontsize=config.fontsize-2)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "def plot_training_stats(training_stats: Dict[str, List[float]],\n",
        "                       window_size: int = 50,\n",
        "                       config: Optional[PlotConfig] = None, plots_dir: str = None) -> None:\n",
        "    \"\"\"\n",
        "    Plot training statistics with proper smoothing and visualization.\n",
        "\n",
        "    Args:\n",
        "        training_stats: Dictionary containing training metrics\n",
        "        window_size: Smoothing window size\n",
        "        config: Plot configuration object\n",
        "    \"\"\"\n",
        "\n",
        "    if config is None:\n",
        "        config = PlotConfig()\n",
        "\n",
        "    # Smooth all metrics\n",
        "    smoothed_stats = {\n",
        "        key: smooth_data(values, window_size)\n",
        "        for key, values in training_stats.items()\n",
        "    }\n",
        "\n",
        "    # Create training steps array\n",
        "    training_steps = np.arange(len(smoothed_stats['loss']))\n",
        "\n",
        "    # Plot metrics\n",
        "    metrics = [\n",
        "        ('loss', 'Training Loss'),\n",
        "        ('predicted_earnings', 'Predicted Earnings'),\n",
        "        ('predicted_earnings_greedy', 'Predicted Earnings (Greedy)'),\n",
        "        ('accuracy', 'Accuracy'),\n",
        "        ('hrn', 'HRN'),\n",
        "        ('spearman', 'Spearman Correlation'),\n",
        "        ('computation_depth', 'Computation Depth'),\n",
        "        ('n_nodes', 'Number of Nodes'),\n",
        "        ('batch_time', 'Batch Time'),\n",
        "        ('lr', 'Learning Rate')\n",
        "    ]\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(len(metrics)%2 + len(metrics)//2, 2, figsize=(16, 22), dpi=config.dpi)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    id = 0\n",
        "    for idx, (metric, label) in enumerate(metrics):\n",
        "        if metric in smoothed_stats:\n",
        "            plot_metric(axes[id], np.arange(len(smoothed_stats[metric])),\n",
        "                       smoothed_stats[metric], label,\n",
        "                       config.colors[idx % len(config.colors)], config)\n",
        "            id+=1\n",
        "    # Adjust layout and show\n",
        "    plt.tight_layout()\n",
        "    if plots_dir:\n",
        "        plt.savefig(plots_dir)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKHj-XwgecPj"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3c8Qguc7TMT"
      },
      "outputs": [],
      "source": [
        "class HorseRacingDataset(Dataset):\n",
        "    \"\"\"Dataset class for horse racing prediction tasks\"\"\"\n",
        "\n",
        "    def __init__(self, data_config):\n",
        "        \"\"\"\n",
        "        Initialize dataset with configuration\n",
        "\n",
        "        Args:\n",
        "            data_config: Data configuration parameters\n",
        "        \"\"\"\n",
        "        self.config = data_config\n",
        "        self.embedding_dict: Dict = {}\n",
        "\n",
        "        # Load and preprocess data\n",
        "        self._load_data()\n",
        "        self._preprocess_data()\n",
        "        self._validate_data()\n",
        "\n",
        "        logger.info(f\"Dataset initialized with {len(self)} races\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Total number of races in dataset\"\"\"\n",
        "        return len(self.df_races)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple:\n",
        "        \"\"\"Get item with robust error handling and validation\"\"\"\n",
        "        for attempt in range(100):  # Max 100 attempts to find valid sample\n",
        "            try:\n",
        "                i = self._get_item(idx)\n",
        "                return i\n",
        "            except (KeyError, IndexError, ValueError) as e:\n",
        "                logger.warning(f\"Error processing index {idx}: {str(e)}\")\n",
        "                idx = (idx + 1) % len(self)\n",
        "\n",
        "        raise RuntimeError(f\"Failed to find valid sample after 100 attempts starting from index {idx}\")\n",
        "\n",
        "    def _load_data(self) -> None:\n",
        "        \"\"\"Load and filter raw data files\"\"\"\n",
        "        try:\n",
        "            # self.df_races = self._load_csv(\"df_races_input.csv\", sort_by = ['crid']).drop(columns = self.config.cols_races_to_drop)\n",
        "            # self.df_results = self._load_csv(\"df_results.csv\", sort_by = ['crid', 'hid']).drop(columns = self.config.cols_results_to_drop)\n",
        "            # self.df_race_horse = self._load_csv(\"df_race_horse_input.csv\", sort_by = ['crid', 'hid']).drop(columns = self.config.cols_horses_to_drop)\n",
        "            self.df_races = self._load_csv(\"df_races_input_10000.csv\", sort_by = ['crid']).drop(columns = self.config.cols_races_to_drop)\n",
        "            self.df_results = self._load_csv(\"df_results_10000.csv\", sort_by = ['crid', 'hid']).drop(columns = self.config.cols_results_to_drop)\n",
        "            self.df_race_horse = self._load_csv(\"df_race_horse_input_10000.csv\", sort_by = ['crid', 'hid']).drop(columns = self.config.cols_horses_to_drop)\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logger.error(f\"Data loading failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _load_csv(self, filename: str, sort_by: list) -> pd.DataFrame:\n",
        "        \"\"\"Load and filter CSV file with memory optimization\"\"\"\n",
        "        filepath = os.path.join(self.config.data_folder, filename)\n",
        "        df = pd.read_csv(filepath, sep=\";\",\n",
        "                        dtype={'crid': 'int32', 'hid': 'int32'})\n",
        "        # display(df)\n",
        "        return df.sort_values(sort_by).reset_index(drop=True)\n",
        "\n",
        "    def _preprocess_data(self) -> None:\n",
        "        \"\"\"Preprocess and merge datasets\"\"\"\n",
        "        if self.config.input_solution:\n",
        "          # Merge results to test the functionning of the model\n",
        "          cols_to_merge = ['hrn', 'hid', 'res_win'] + [f\"position_{i}\" for i in range(1, 41)]\n",
        "          self.df_results['hrn'] = self.df_results['hrn'] - 5\n",
        "          self.df_race_horse = self.df_race_horse.merge(\n",
        "              self.df_results[cols_to_merge],\n",
        "              on=['hrn', 'hid'],\n",
        "              how='left',\n",
        "              validate='one_to_one'\n",
        "          ).fillna(0)\n",
        "          self.df_results['hrn'] = self.df_results['hrn'] + 5\n",
        "\n",
        "        # Get dataframe, that is not standardized\n",
        "        self.df_target = self.df_results.copy()\n",
        "\n",
        "        # Prepare feature columns\n",
        "        self._setup_feature_columns()\n",
        "        self._standardize_features()\n",
        "\n",
        "        # Create lookup indices\n",
        "        self.crid_to_race_horse = self._create_crid_groups(self.df_race_horse)\n",
        "        self.crid_to_results = self._create_crid_groups(self.df_results)\n",
        "\n",
        "    def _setup_feature_columns(self) -> None:\n",
        "        \"\"\"Identify feature columns for each dataframe\"\"\"\n",
        "        id_columns = [\"rid\", \"hid\", \"crid\"]\n",
        "\n",
        "        self.features = {\n",
        "            'races': [c for c in self.df_races if c not in id_columns],\n",
        "            'race_horse': [c for c in self.df_race_horse if c not in id_columns],\n",
        "            'results': [c for c in self.df_results if c not in id_columns]\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Feature counts - Races: {len(self.features['races'])}, \"\n",
        "                   f\"Race Horses: {len(self.features['race_horse'])}, \"\n",
        "                   f\"Results: {len(self.features['results'])}\")\n",
        "\n",
        "    def _standardize_features(self) -> None:\n",
        "        \"\"\"Apply z-score standardization to feature columns\"\"\"\n",
        "        self.df_races = self._zscore_standardize(self.df_races, self.features['races'])\n",
        "        self.df_race_horse = self._zscore_standardize(self.df_race_horse, self.features['race_horse'])\n",
        "        self.df_results = self._zscore_standardize(self.df_results, self.features['results'])\n",
        "\n",
        "    @staticmethod\n",
        "    def _zscore_standardize(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Safe z-score standardization with error handling\"\"\"\n",
        "        df = df.copy()\n",
        "        for col in columns:\n",
        "            try:\n",
        "                mean = df[col].mean()\n",
        "                std = df[col].std(ddof=0)\n",
        "                df[col] = (df[col] - mean) / (std + 1e-8)\n",
        "            except TypeError:\n",
        "                logger.error(f\"Non-numeric data in column {col}\")\n",
        "                raise\n",
        "        return df.fillna(0)\n",
        "\n",
        "    def _create_crid_groups(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Create efficient crid to indices mapping\"\"\"\n",
        "        return df.groupby('crid', sort=False).indices\n",
        "\n",
        "    def _validate_data(self) -> None:\n",
        "        \"\"\"Validate dataset consistency\"\"\"\n",
        "        if len(self.df_races) == 0:\n",
        "            raise ValueError(\"No races loaded in dataset\")\n",
        "\n",
        "        if not all(c in self.df_race_horse for c in ['hrn', 'hid']):\n",
        "            raise ValueError(\"Missing required columns in race horse data\")\n",
        "\n",
        "    def _get_item(self, idx: int) -> Tuple:\n",
        "        \"\"\"Core item retrieval logic\"\"\"\n",
        "        crid = self.df_races.iloc[idx]['crid']\n",
        "\n",
        "        # Get race features\n",
        "        race_features = self.df_races.iloc[idx][self.features['races']].values.astype(np.float32)\n",
        "        # Get horse data\n",
        "        horse_data, results_data = self._get_horse_data(crid)\n",
        "\n",
        "        # Get target information\n",
        "        target_data = self.df_target.loc[self.crid_to_results.get(crid, [])]\n",
        "\n",
        "        targets = {\n",
        "            'position': target_data['position'].values.astype(np.float32),\n",
        "            'decimalPrice': target_data['decimalPrice'].values.astype(np.float32),\n",
        "            'hids': target_data['hid'].values.astype(np.int32),\n",
        "            'hrn': target_data['hrn'].values.astype(np.int32),\n",
        "            'crid': target_data['crid'].values.astype(np.int32)\n",
        "        }\n",
        "\n",
        "        return (race_features, horse_data, results_data) + tuple(targets.values())\n",
        "\n",
        "    def _get_horse_data(self, crid: int) -> Tuple:\n",
        "        \"\"\"Retrieve horse data for a given CRID\"\"\"\n",
        "        horse_indices = self.crid_to_race_horse.get(crid, np.array([], dtype=int))\n",
        "        results_indices = self.crid_to_results.get(crid, np.array([], dtype=int))\n",
        "\n",
        "        if len(horse_indices) != len(results_indices):\n",
        "            logger.warning(f\"Mismatched data lengths for CRID {crid}: \"\n",
        "                          f\"{len(horse_indices)} horses vs {len(results_indices)} results\")\n",
        "            return np.empty((0, len(self.features['race_horse']))), np.empty((0, len(self.features['results'])))\n",
        "\n",
        "        return (\n",
        "            self.df_race_horse.iloc[horse_indices][self.features['race_horse']].values.astype(np.float32),\n",
        "            self.df_results.iloc[results_indices][self.features['results']].values.astype(np.float32)\n",
        "        )\n",
        "\n",
        "def collate_fn(batch: List, dataset: HorseRacingDataset) -> Dict:\n",
        "    \"\"\"Efficient batch collation with padding and masking\"\"\"\n",
        "\n",
        "    def pad_array(arr: np.ndarray, target_length: int, pad_value: float = 0) -> np.ndarray:\n",
        "      \"\"\"Pad array to target length\"\"\"\n",
        "      pad_width = (0, target_length - len(arr))\n",
        "      return np.pad(arr, (pad_width, (0, 0)) if arr.ndim == 2 else pad_width,\n",
        "                  constant_values=pad_value)\n",
        "\n",
        "    batch_elements = len(batch)\n",
        "    max_horses = max(len(item[1]) for item in batch)\n",
        "\n",
        "    # Initialize storage\n",
        "    batch_dict = {\n",
        "        'race_data': [],\n",
        "        'horse_data': [],\n",
        "        'results_data': [],\n",
        "        'positions': [],\n",
        "        'prices': [],\n",
        "        'hids': [],\n",
        "        'hrn': [],\n",
        "        'crid': []\n",
        "    }\n",
        "\n",
        "    # Process each sample\n",
        "    for sample in batch:\n",
        "        race, horses, results, pos, price, hids, hrn, crid = sample\n",
        "        num_horses = len(horses)\n",
        "\n",
        "        # Pad features\n",
        "        batch_dict['race_data'].append(pad_array(np.tile(race, (num_horses, 1)), max_horses))\n",
        "        batch_dict['horse_data'].append(pad_array(horses, max_horses))\n",
        "        batch_dict['results_data'].append(pad_array(results, max_horses))\n",
        "\n",
        "        # Pad targets\n",
        "        batch_dict['positions'].append(pad_array(pos, max_horses, -1))\n",
        "        batch_dict['prices'].append(pad_array(price, max_horses, -1))\n",
        "        batch_dict['hids'].append(pad_array(hids, max_horses, -1))\n",
        "        batch_dict['hrn'].append(pad_array(hrn, max_horses, -1))\n",
        "        batch_dict['crid'].append(pad_array(crid, max_horses, -1))\n",
        "\n",
        "    # Convert to tensors\n",
        "    tensor_batch = {\n",
        "        k: torch.tensor(np.stack(v), dtype=torch.float32)\n",
        "        for k, v in batch_dict.items()\n",
        "    }\n",
        "    tensors_with_nan = []\n",
        "    for name, tensor in tensor_batch.items():\n",
        "        if torch.isnan(tensor).any():\n",
        "            tensors_with_nan.append(name)\n",
        "    if tensors_with_nan:\n",
        "        print(f\"Tensors with NaNs in {tensors_with_nan}\")\n",
        "    return tensor_batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b3jxHZjeyZ_"
      },
      "source": [
        "### Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47d_QgTHFgiD"
      },
      "outputs": [],
      "source": [
        "def spearman_rank_correlation(logits: torch.Tensor,\n",
        "                             positions_arrival: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Spearman's rank correlation between predicted logits and actual positions,\n",
        "    ignoring invalid entries (-1 or 40). Handles variable participant counts per race.\n",
        "\n",
        "    Args:\n",
        "        logits: Tensor of shape [batch_size, num_horses] with prediction scores\n",
        "        positions_arrival: Tensor of shape [batch_size, num_horses] with actual positions\n",
        "\n",
        "    Returns:\n",
        "        Mean Spearman's rho across batch (valid races only) as torch.Tensor\n",
        "    \"\"\"\n",
        "    device = logits.device\n",
        "    batch_size = logits.size(0)\n",
        "    correlations = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Filter valid entries for this race\n",
        "        mask = (positions_arrival[i] != -1.0) & (positions_arrival[i] != 40.0)\n",
        "        race_logits = logits[i][mask].detach().cpu().numpy()\n",
        "        race_positions = positions_arrival[i][mask].cpu().numpy()\n",
        "\n",
        "        # Skip races with <2 valid participants\n",
        "        if len(race_logits) < 2:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Generate predicted ranks from logits (higher logit = better rank)\n",
        "            pred_ranks = (-race_logits).argsort().argsort()  # Double argsort for rank\n",
        "\n",
        "            # Calculate Spearman correlation\n",
        "            rho, _ = spearmanr(pred_ranks, race_positions)\n",
        "\n",
        "            # Handle NaN/edge cases\n",
        "            if np.isnan(rho):\n",
        "                rho = 0.0\n",
        "        except:\n",
        "            rho = 0.0\n",
        "\n",
        "        correlations.append(rho)\n",
        "\n",
        "    # Return average across valid races\n",
        "    if not correlations:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "    return torch.tensor(np.mean(correlations), device=device)\n",
        "\n",
        "def loss_function_classificationV2(\n",
        "    logits: torch.Tensor,\n",
        "    decimal_prices: torch.Tensor,\n",
        "    positions_arrival: torch.Tensor,\n",
        "    crids: torch.Tensor,\n",
        "    penalty_weight: float = 1,\n",
        "    avg_bet_per_race: float = 0.5,\n",
        "    printit: bool = False\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Loss function with vectorized operations and reduced memory footprint.\n",
        "\n",
        "    Args:\n",
        "        logits: Model outputs (batch_size, num_tokens, 40)\n",
        "        decimal_prices: Decimal odds (batch_size, num_tokens)\n",
        "        positions_arrival: Target positions (batch_size, num_tokens)\n",
        "        crids: Race identifiers (batch_size)\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing loss tensor and various metrics\n",
        "    \"\"\"\n",
        "    device = logits.device\n",
        "    batch_size, num_tokens, n_rankings = logits.shape\n",
        "\n",
        "    # Mask invalid positions and calculate outputs\n",
        "    valid_mask = (positions_arrival != -1.0) & (positions_arrival != 40.0)\n",
        "    logits = logits.masked_fill(~valid_mask.unsqueeze(-1), -1e6)\n",
        "    output = torch.sigmoid(logits)\n",
        "\n",
        "    # Create indices tensor using vectorized operations\n",
        "    positions = positions_arrival.clamp(min=1, max=n_rankings).long() - 1\n",
        "\n",
        "    positions_expanded = positions.unsqueeze(-1)  # (batch_size, num_tokens, 1)\n",
        "    rankings_range = torch.arange(n_rankings, device=device).view(1, 1, -1)  # (1, 1, n_rankings)\n",
        "    indices = (rankings_range >= positions_expanded).float()  # (batch_size, num_tokens, n_rankings)\n",
        "    indices[valid_mask == False] = 1.0  # (batch_size, num_tokens, n_rankings\n",
        "\n",
        "    # Calculate position weights, to put as much attention on each race, independently of the amount of racers.\n",
        "    weights_by_position = torch.arange(n_rankings, 0, -1, device=device).float()\n",
        "    weights_position = valid_mask.unsqueeze(-1) * weights_by_position.view(1, 1, -1)\n",
        "    weights_position = (n_rankings**2 / 2) * weights_position / (weights_position.sum(dim=(1,2), keepdim=True) + 1e-5)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = torch.nn.functional.binary_cross_entropy(\n",
        "        output, indices, weight=weights_position\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Calculate accuracy\n",
        "        valid_positions = valid_mask & (positions_arrival == 1.0)\n",
        "        race_output_max = output[..., 0].masked_fill(~valid_mask, -float('inf'))\n",
        "        predicted_winners = race_output_max.argmax(dim=1)\n",
        "\n",
        "        # Create mask of correct targets\n",
        "        correct_target_mask = (positions_arrival == 1.0) & valid_mask\n",
        "\n",
        "        # Check if predictions match any correct target\n",
        "        batch_indices = torch.arange(batch_size, device=device)\n",
        "        correct_predictions = correct_target_mask[batch_indices, predicted_winners]\n",
        "\n",
        "        # Filter for batches with valid positions\n",
        "        valid_batches = valid_positions.any(dim=1)\n",
        "        accuracy = correct_predictions[valid_batches].float().mean()\n",
        "\n",
        "        # Total winnings\n",
        "        winnings = (output[valid_positions][:,0] / decimal_prices[valid_positions]).sum()\n",
        "\n",
        "        # Total betted amount\n",
        "        total_betted = output[valid_mask][:, 0].sum()\n",
        "\n",
        "        # Predicted earnings\n",
        "        predicted_earnings = winnings - total_betted\n",
        "\n",
        "        # Greedy earnings\n",
        "        greedy_winnings = ((positions_arrival.gather(1, predicted_winners.unsqueeze(1))==1) / decimal_prices.gather(1, predicted_winners.unsqueeze(1))).sum()\n",
        "        greedy_earnings = greedy_winnings - batch_size  # Subtract total bets\n",
        "\n",
        "        # Calculate Spearman correlation\n",
        "        spearman_results = spearmanr_kpi(output.detach().clone(), positions_arrival)\n",
        "\n",
        "    return (\n",
        "        loss,\n",
        "        accuracy.detach(),\n",
        "        predicted_earnings / batch_size,\n",
        "        greedy_earnings / batch_size,\n",
        "        total_betted / batch_size,\n",
        "        winnings / batch_size,\n",
        "        torch.tensor(spearman_results, device=device)\n",
        "    )\n",
        "\n",
        "def loss_function_first_horse_classification(\n",
        "    logits: torch.Tensor,\n",
        "    decimal_prices: torch.Tensor,\n",
        "    positions_arrival: torch.Tensor,\n",
        "    crids: torch.Tensor,\n",
        "    penalty_weight: float = 1,\n",
        "    avg_bet_per_race: float = 0.5,\n",
        "    printit: bool = False\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Focused on predicting first-place finishes using softmax cross-entropy.\n",
        "    Maintains betting metrics while optimizing for winner prediction.\n",
        "\n",
        "    Args:\n",
        "        logits: Winner prediction scores (batch_size, num_horses)\n",
        "        decimal_prices: Decimal odds (batch_size, num_horses)\n",
        "        positions_arrival: Target positions (batch_size, num_horses)\n",
        "        crids: Race identifiers (batch_size)\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing loss tensor and metrics\n",
        "    \"\"\"\n",
        "    device = logits.device\n",
        "    batch_size, num_horses, n_ranks = logits.shape\n",
        "    logits = logits.squeeze(-1)\n",
        "\n",
        "    # Mask for valid horses and races with exactly one winner\n",
        "    valid_mask = (positions_arrival != -1.0) & (positions_arrival != 40.0)\n",
        "    winner_mask = (positions_arrival == 1.0) & valid_mask\n",
        "    valid_races = winner_mask.sum(dim=1) == 1  # Races with exactly one valid winner\n",
        "\n",
        "    # Convert to class indices\n",
        "    winner_indices = winner_mask.float().argmax(dim=1)  # (batch_size,)\n",
        "\n",
        "    # Cross-entropy loss only for valid races\n",
        "    if valid_races.any():\n",
        "        logits_valid = logits[valid_races]\n",
        "        targets_valid = winner_indices[valid_races].long()\n",
        "        loss = F.cross_entropy(logits_valid, targets_valid)\n",
        "    else:\n",
        "        loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Prediction metrics (similar to original)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        winner_mask = (positions_arrival == 1.0) & valid_mask\n",
        "\n",
        "        # Accuracy calculation\n",
        "        valid_winner_races = winner_mask.any(dim=1)\n",
        "        if valid_winner_races.any():\n",
        "            pred_winners = probs.argmax(dim=1)\n",
        "            correct = winner_mask[torch.arange(batch_size), pred_winners]\n",
        "            accuracy = correct[valid_winner_races].float().mean()\n",
        "        else:\n",
        "            accuracy = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # Betting metrics\n",
        "        if valid_winner_races.any():\n",
        "            # Probabilistic betting\n",
        "            selected_probs = probs[winner_mask]\n",
        "            selected_decimalprices = decimal_prices[winner_mask]\n",
        "            winnings_prob = (selected_probs / selected_decimalprices).sum()\n",
        "            total_betted_prob = batch_size\n",
        "\n",
        "            # Greedy betting\n",
        "            pred_winners = probs.argmax(dim=1)\n",
        "            winnings_greedy = (positions_arrival[torch.arange(batch_size), pred_winners] == 1.0)\n",
        "            decimalPrice_greedy = decimal_prices[torch.arange(batch_size), pred_winners]\n",
        "            winnings_greedy = (winnings_greedy.float() / decimalPrice_greedy).sum()\n",
        "            total_betted_greedy = batch_size\n",
        "        else:\n",
        "            winnings_prob = torch.tensor(0.0, device=device)\n",
        "            total_betted_prob = torch.tensor(0.0, device=device)\n",
        "            winnings_greedy = torch.tensor(0.0, device=device)\n",
        "            total_betted_greedy = torch.tensor(0.0, device=device)\n",
        "\n",
        "        predicted_earnings_prob = winnings_prob - total_betted_prob\n",
        "        predicted_earnings_greedy = winnings_greedy - total_betted_greedy\n",
        "\n",
        "        # Ranking correlation\n",
        "        spearman = spearman_rank_correlation(logits, positions_arrival)\n",
        "\n",
        "    return (\n",
        "        loss,\n",
        "        accuracy.detach(),\n",
        "        predicted_earnings_prob / batch_size,\n",
        "        predicted_earnings_greedy / batch_size,\n",
        "        torch.tensor(1.0, device = device),\n",
        "        winnings_prob / batch_size,\n",
        "        spearman.to(device)\n",
        "    )\n",
        "\n",
        "def loss_function_plackett_luce(\n",
        "    logits: torch.Tensor,\n",
        "    decimal_prices: torch.Tensor,\n",
        "    positions_arrival: torch.Tensor,\n",
        "    crids: torch.Tensor,\n",
        "    penalty_weight: float = 1,\n",
        "    avg_bet_per_race: float = 0.5,\n",
        "    printit: bool = False\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Plackett-Luce loss implementation for horse racing predictions.\n",
        "    Handles up to 40 positions with dynamic computation graph optimization.\n",
        "    \"\"\"\n",
        "    device = logits.device\n",
        "    batch_size, num_horses, n_ranks = logits.shape\n",
        "    logits = logits.squeeze(-1)\n",
        "    # Mask and prepare valid rankings\n",
        "    valid_mask = (positions_arrival != -1.0) & (positions_arrival != 40.0)\n",
        "    positions = positions_arrival.clamp(min=1, max=40).long()\n",
        "\n",
        "    # ====================== Plackett-Luce Loss Core ========================\n",
        "    # Sort logits by actual positions for each race\n",
        "    adjusted_positions = torch.where(valid_mask, positions, torch.full_like(positions, 41))\n",
        "    sorted_indices = adjusted_positions.argsort(dim=1)\n",
        "\n",
        "    # Prepare sorted tensors with valid masking\n",
        "    sorted_logits = logits.gather(1, sorted_indices)\n",
        "    sorted_valid = valid_mask.gather(1, sorted_indices)\n",
        "    sorted_logits_masked = sorted_logits.masked_fill(~sorted_valid, -float('inf'))\n",
        "\n",
        "    # Compute reverse cumulative logsumexp for stability\n",
        "    reversed_logits = torch.flip(sorted_logits_masked, dims=[1])\n",
        "    reverse_cumsum = torch.logcumsumexp(reversed_logits, dim=1)\n",
        "    cum_logsumexp = torch.flip(reverse_cumsum, dims=[1])\n",
        "\n",
        "    # Calculate per-position log probabilities\n",
        "    log_probs = sorted_logits_masked - cum_logsumexp\n",
        "    valid_log_probs = log_probs * sorted_valid.float()\n",
        "\n",
        "    # Normalize by number of participants per race\n",
        "    participants_per_race = valid_mask.sum(dim=1, dtype=torch.float)  # (batch_size,)\n",
        "    race_log_likelihood = torch.nansum(valid_log_probs, dim=1) / participants_per_race\n",
        "\n",
        "    # Filter valid races (handle potential 0/0 from empty races)\n",
        "    valid_races = (valid_mask.any(dim=1)) & (participants_per_race > 0)\n",
        "    loss = -race_log_likelihood[valid_races].mean() if valid_races.any() else torch.tensor(0.0, device=device)\n",
        "    # ========================================================================\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Prediction metrics (similar to original)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        winner_mask = (positions_arrival == 1.0) & valid_mask\n",
        "\n",
        "        # Accuracy calculation\n",
        "        valid_winner_races = winner_mask.any(dim=1)\n",
        "        if valid_winner_races.any():\n",
        "            pred_winners = probs.argmax(dim=1)\n",
        "            correct = winner_mask[torch.arange(batch_size), pred_winners]\n",
        "            accuracy = correct[valid_winner_races].float().mean()\n",
        "        else:\n",
        "            accuracy = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # Betting metrics\n",
        "        if valid_winner_races.any():\n",
        "            # Probabilistic betting\n",
        "            selected_probs = probs[winner_mask]\n",
        "            selected_decimalprices = decimal_prices[winner_mask]\n",
        "            winnings_prob = (selected_probs / selected_decimalprices).sum()\n",
        "            total_betted_prob = batch_size\n",
        "\n",
        "            # Greedy betting\n",
        "            pred_winners = probs.argmax(dim=1)\n",
        "            winnings_greedy = (positions_arrival[torch.arange(batch_size), pred_winners] == 1.0)\n",
        "            decimalPrice_greedy = decimal_prices[torch.arange(batch_size), pred_winners]\n",
        "            winnings_greedy = (winnings_greedy.float() / decimalPrice_greedy).sum()\n",
        "            total_betted_greedy = batch_size\n",
        "        else:\n",
        "            winnings_prob = torch.tensor(0.0, device=device)\n",
        "            total_betted_prob = torch.tensor(0.0, device=device)\n",
        "            winnings_greedy = torch.tensor(0.0, device=device)\n",
        "            total_betted_greedy = torch.tensor(0.0, device=device)\n",
        "\n",
        "        predicted_earnings_prob = winnings_prob - total_betted_prob\n",
        "        predicted_earnings_greedy = winnings_greedy - total_betted_greedy\n",
        "\n",
        "        # Ranking correlation\n",
        "        spearman = spearman_rank_correlation(logits, positions_arrival)\n",
        "\n",
        "    return (\n",
        "        loss,\n",
        "        accuracy.detach(),\n",
        "        predicted_earnings_prob / batch_size,\n",
        "        predicted_earnings_greedy / batch_size,\n",
        "        torch.tensor(1.0, device = device),\n",
        "        winnings_prob / batch_size,\n",
        "        spearman.to(device)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo-J62NOes4d"
      },
      "source": [
        "### Embedding Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClSmn-r3iAms"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "@dataclass\n",
        "class EmbeddingState:\n",
        "    embeddings: deque\n",
        "    node_counts: deque\n",
        "\n",
        "class CustomEmbeddingManager(nn.Module):\n",
        "    \"\"\"Manages dynamic embeddings with computation graph optimization\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int = 128,\n",
        "                 max_nodes: int = 100,\n",
        "                 max_depth_nodes: int = 10,\n",
        "                 max_sequence_length: int = 5):\n",
        "        super().__init__()\n",
        "\n",
        "        # State management\n",
        "        self.embedding_states: Dict[int, EmbeddingState] = defaultdict(\n",
        "            lambda: EmbeddingState(embeddings=deque(maxlen=max_sequence_length),\n",
        "                                   node_counts=deque(maxlen=max_sequence_length)))\n",
        "\n",
        "        # Configuration\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_nodes = max_nodes\n",
        "        self.max_depth_nodes = max_depth_nodes\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "    def update_embeddings(self,\n",
        "                         horse_ids: torch.Tensor,\n",
        "                         new_embeddings: torch.Tensor,\n",
        "                         additional_nodes: int) -> None:\n",
        "        \"\"\"\n",
        "        Update embeddings in a vectorized manner\n",
        "        Args:\n",
        "            horse_ids: (batch_size, num_tokens) tensor of horse IDs\n",
        "            new_embeddings: (batch_size, num_tokens, embedding_dim) tensor\n",
        "            additional_nodes: (batch_size, num_tokens) tensor of node counts\n",
        "        \"\"\"\n",
        "        self.device = new_embeddings.device\n",
        "        batch_size, num_tokens = horse_ids.shape\n",
        "\n",
        "        # Flatten and filter valid IDs\n",
        "        mask = horse_ids != -1\n",
        "        valid_ids = horse_ids[mask].long()\n",
        "        valid_embeddings = new_embeddings[mask]\n",
        "\n",
        "        # Vectorized update\n",
        "        for hid, emb in zip(valid_ids, valid_embeddings):\n",
        "            state = self.embedding_states[hid.item()]\n",
        "            if len(state.embeddings) == self.max_sequence_length:\n",
        "              # Detach and remove reference to popped embedding\n",
        "              popped_emb = state.embeddings.popleft()\n",
        "              popped_emb = popped_emb.detach()\n",
        "              state.node_counts.popleft()\n",
        "\n",
        "            state.embeddings.append(emb)\n",
        "            state.node_counts.append(additional_nodes + 1)\n",
        "\n",
        "    def get_detach_flags(self, horse_ids: torch.Tensor) -> Tuple[List[bool], int, List[int]]:\n",
        "        \"\"\"\n",
        "        Calculate detachment flags considering:\n",
        "        1. Each individual node_count <= max_depth_nodes\n",
        "        2. Total of selected node_counts <= max_nodes\n",
        "        3. Prioritizes smallest node_counts first\n",
        "        \"\"\"\n",
        "        batch_size, num_tokens = horse_ids.shape\n",
        "        device = horse_ids.device\n",
        "\n",
        "        # Initialize\n",
        "        detach_flags = []\n",
        "        node_counts = []\n",
        "        hid_location = []\n",
        "\n",
        "        # Collect all valid candidates with their positions\n",
        "        valid_mask = horse_ids != -1\n",
        "        valid_indices = torch.nonzero(valid_mask, as_tuple=False)\n",
        "\n",
        "        for idx in valid_indices:\n",
        "            i, j = idx.tolist()\n",
        "            hid = horse_ids[i, j].item()\n",
        "            state = self.embedding_states.get(hid)\n",
        "\n",
        "            if not state:\n",
        "                continue\n",
        "\n",
        "            # Collect individual node counts with their positions\n",
        "            for count in state.node_counts:\n",
        "              node_counts.append(count)\n",
        "              detach_flags.append(True)\n",
        "              hid_location.append(hid)\n",
        "\n",
        "        # Sort candidates by node count (smallest first)\n",
        "        sorted_indices = sorted(range(len(node_counts)), key=lambda i: node_counts[i])\n",
        "\n",
        "        # Select candidates until we reach max_nodes\n",
        "        total = 0\n",
        "        selected = set()\n",
        "        for i, index in enumerate(sorted_indices):\n",
        "            count = node_counts[index]\n",
        "            if total + count > self.max_nodes or count > self.max_depth_nodes:\n",
        "                break\n",
        "            total += count\n",
        "            detach_flags[index] = False\n",
        "\n",
        "        return detach_flags, total, hid_location\n",
        "\n",
        "    def get_embeddings(self,\n",
        "                      horse_ids: torch.Tensor) -> Tuple[Optional[torch.Tensor],\n",
        "                                                       Optional[torch.Tensor],\n",
        "                                                       List[int]]:\n",
        "        \"\"\"\n",
        "        Retrieve embeddings with optimized detachment\n",
        "        Args:\n",
        "            horse_ids: (batch_size, num_tokens) tensor of horse IDs\n",
        "        Returns:\n",
        "            embeddings: (total_sequences, seq_len, embedding_dim) padded embeddings\n",
        "            lengths: (total_sequences,) tensor of sequence lengths\n",
        "            valid_ids: List of valid horse IDs\n",
        "        \"\"\"\n",
        "        detach_flags, total_nodes, hid_location = self.get_detach_flags(horse_ids)\n",
        "        valid_mask = horse_ids != -1\n",
        "        valid_ids = horse_ids[valid_mask].tolist()\n",
        "\n",
        "        # Batch retrieval of embeddings\n",
        "        sequences = []\n",
        "        integrated_ids = []\n",
        "        i = 0\n",
        "        for hid in valid_ids:\n",
        "            state = self.embedding_states.get(hid)\n",
        "            if state and len(state.embeddings) > 0:\n",
        "                intermediate_sequence = []\n",
        "                for inter_state in state.embeddings:\n",
        "                    if detach_flags[i]:\n",
        "                        inter_state = inter_state.detach()\n",
        "                    if hid_location[i] != hid:\n",
        "                        logger.warning(f\"In get_embeddings crid sequence {hid} doesn't coincide with detach_flags sequence {hid_location[i]} \")\n",
        "                    i += 1\n",
        "                    intermediate_sequence.append(inter_state)\n",
        "                sequences.append(torch.stack(intermediate_sequence))\n",
        "                integrated_ids.append(hid)\n",
        "        if not sequences:\n",
        "            return None, None, [], 0\n",
        "\n",
        "        # Pad sequences efficiently\n",
        "        lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long, device=self.device)\n",
        "        padded = torch.nn.utils.rnn.pad_sequence(sequences,\n",
        "                                                batch_first=True,\n",
        "                                                padding_value=0.0)\n",
        "\n",
        "        return padded, lengths, integrated_ids, total_nodes\n",
        "\n",
        "    def reset_embeddings(self) -> None:\n",
        "        \"\"\"Safely reset all embeddings and associated computation graphs\"\"\"\n",
        "        # First detach and clear gradients\n",
        "        for hid, state in self.embedding_states.items():\n",
        "            with torch.no_grad():\n",
        "                # Detach all embeddings from computation graph\n",
        "                state.embeddings = [t.detach() for t in state.embeddings]\n",
        "\n",
        "                # Remove gradient information\n",
        "                for t in state.embeddings:\n",
        "                    t.grad = None\n",
        "                    t.requires_grad_(False)\n",
        "\n",
        "                # Clear lists\n",
        "                state.embeddings.clear()\n",
        "                state.node_counts.clear()\n",
        "\n",
        "        # Then clear the dictionary\n",
        "        self.embedding_states.clear()\n",
        "\n",
        "        # Finally force CUDA cleanup\n",
        "        if torch.cuda.is_initialized():\n",
        "            torch.cuda.synchronize()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijyZl9FVeo3p"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad-NiLDECK2S"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise feed-forward network with GELU activation and configurable hidden dimension.\n",
        "\n",
        "    Args:\n",
        "        config: Dictionary containing:\n",
        "            - emb_dim: Input/output dimension (int)\n",
        "            - ff_dim: Hidden dimension multiplier (int, default 4)\n",
        "    \"\"\"\n",
        "    def __init__(self, config: dict):\n",
        "        super().__init__()\n",
        "        hidden_dim = config.get(\"ff_dim\", 4) * config[\"emb_dim\"]\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config[\"emb_dim\"], hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, config[\"emb_dim\"])\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"Graph convolution layer with input/output transformations and neighbor aggregation.\n",
        "\n",
        "    Args:\n",
        "        emb_dim: Dimension of node embeddings (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear_self = nn.Linear(emb_dim, emb_dim)\n",
        "        self.linear_neigh = nn.Linear(emb_dim, emb_dim)\n",
        "        self.eps = 1e-6  # For numerical stability\n",
        "\n",
        "    def forward(self, x: torch.Tensor, token_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Node features (batch_size, num_nodes, emb_dim)\n",
        "            token_ids: Token ids (batch_size, max_number_tokens)\n",
        "                          -1 indicates padding/no token\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes, emb_dim = x.shape\n",
        "\n",
        "        # Self transformation\n",
        "        self_emb = self.linear_self(x)  # (B, N, D)\n",
        "\n",
        "        # Neighbor transformation and aggregation\n",
        "        hids_mask = token_ids != -1\n",
        "        valid_counts = hids_mask.sum(dim=-1, keepdim=True)  # (B, N, 1)\n",
        "        transformed_neigh = self.linear_neigh(x)  # (B, N, D)\n",
        "\n",
        "        # Masked sum and normalize\n",
        "        summed_neigh = (transformed_neigh * hids_mask[..., None]).sum(dim=-2)\n",
        "        normalized_neigh = summed_neigh / (valid_counts + self.eps)\n",
        "\n",
        "        # Combine and activate\n",
        "        return F.gelu(self_emb + normalized_neigh.unsqueeze(1))\n",
        "\n",
        "class GCNBlock(nn.Module):\n",
        "    \"\"\"Multi-head graph convolution block with residual connection and layer normalization.\n",
        "\n",
        "    Args:\n",
        "        config: Dictionary containing:\n",
        "            - emb_dim: Embedding dimension (int)\n",
        "            - n_heads: Number of attention heads (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, config: dict):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([\n",
        "            GraphConvolution(config.emb_dim)\n",
        "            for _ in range(config.n_heads)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config.emb_dim)\n",
        "        self.output_proj = nn.Linear(config.emb_dim * config.n_heads, config.emb_dim)\n",
        "\n",
        "    def forward(self, data: tuple) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: Tuple containing: (x, token_ids)\n",
        "            x: Node features (B, N, D)\n",
        "            token_ids: Token ids (B, N, K)\n",
        "        \"\"\"\n",
        "\n",
        "        x, token_ids = data\n",
        "\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Process all heads in parallel\n",
        "        head_outputs = [head(x, token_ids) for head in self.heads]\n",
        "\n",
        "        if len(self.heads)>1:\n",
        "          combined = torch.cat(head_outputs, dim=-1)  # (B, N, D*H)\n",
        "\n",
        "          # Project back to original dimension\n",
        "          return self.output_proj(combined) + residual\n",
        "        else:\n",
        "          return head_outputs[0] + residual, token_ids\n",
        "\n",
        "class LSTM_Model(nn.Module):\n",
        "    \"\"\"LSTM Model with padded sequence handling and configurable parameters\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config: Dictionary containing model parameters:\n",
        "                - emb_dim: Input and hidden dimension size (int)\n",
        "                - num_layers: Number of LSTM layers (int, optional)\n",
        "                - dropout: Dropout probability (float, optional)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=config.emb_dim,\n",
        "            hidden_size=config.emb_dim,\n",
        "            num_layers=config.__dict__.get(\"num_layers\", 1),\n",
        "            dropout=config.__dict__.get(\"dropout\", 0.0),\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "                padded_sequences: torch.Tensor,\n",
        "                sequence_lengths: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Process variable-length sequences through LSTM\n",
        "\n",
        "        Args:\n",
        "            padded_sequences: (batch_size, max_seq_len, emb_dim) padded sequences\n",
        "            sequence_lengths: (batch_size,) lengths of valid sequences\n",
        "\n",
        "        Returns:\n",
        "            output_sequences: (batch_size, max_seq_len, emb_dim) processed sequences\n",
        "        \"\"\"\n",
        "        # Validate inputs\n",
        "        self._validate_inputs(padded_sequences, sequence_lengths)\n",
        "\n",
        "        # Convert lengths to CPU tensor for packing\n",
        "        lengths_cpu = sequence_lengths.cpu()\n",
        "\n",
        "        # Pack padded sequences\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(\n",
        "            input=padded_sequences,\n",
        "            lengths=lengths_cpu,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Process through LSTM\n",
        "        packed_output, _ = self.lstm(packed_input)\n",
        "\n",
        "        # Unpack sequences\n",
        "        output_sequences, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "            packed_output,\n",
        "            batch_first=True,\n",
        "            total_length=padded_sequences.size(1))\n",
        "\n",
        "        return output_sequences\n",
        "\n",
        "    def _validate_inputs(self,\n",
        "                        sequences: torch.Tensor,\n",
        "                        lengths: torch.Tensor) -> None:\n",
        "        \"\"\"Validate input dimensions and lengths\"\"\"\n",
        "        if sequences.dim() != 3:\n",
        "            raise ValueError(f\"Input sequences must be 3D tensor (batch, seq, features), got {sequences.shape}\")\n",
        "\n",
        "        if lengths.dim() != 1:\n",
        "            raise ValueError(f\"Sequence lengths must be 1D tensor, got {lengths.shape}\")\n",
        "\n",
        "        if sequences.size(0) != lengths.size(0):\n",
        "            raise ValueError(f\"Batch size mismatch between sequences ({sequences.size(0)}) and lengths ({lengths.size(0)})\")\n",
        "\n",
        "        if (lengths < 0).any() or (lengths > sequences.size(1)).any():\n",
        "            raise ValueError(\"Invalid sequence lengths detected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnFnYnl1E2IH"
      },
      "outputs": [],
      "source": [
        "class HorseRacingModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        required_keys = [\n",
        "            \"emb_dim\", \"max_nodes\", \"max_depth_nodes\", \"max_lstm_depth\",\n",
        "            \"n_heads\", \"input_features\", \"n_layers\"\n",
        "        ]\n",
        "\n",
        "        for key in required_keys:\n",
        "            if key not in cfg.__dict__.keys():\n",
        "                raise ValueError(f\"Config missing required key: {key}\")\n",
        "\n",
        "        # Embeddings for horse states\n",
        "        self.horse_embeddings = CustomEmbeddingManager(\n",
        "            embedding_dim=cfg.emb_dim,\n",
        "            max_nodes=cfg.max_nodes,\n",
        "            max_depth_nodes=cfg.max_depth_nodes,\n",
        "            max_sequence_length=cfg.max_lstm_depth\n",
        "        )\n",
        "        # self.register_parameter('initial_embedding', nn.Parameter(torch.zeros(cfg[\"emb_dim\"])))\n",
        "        self.initial_embedding = nn.Parameter(torch.zeros(cfg.emb_dim))\n",
        "\n",
        "        # Projection layers\n",
        "        self.logit_projection = nn.Linear(\n",
        "            len(cfg.input_features[\"races\"]) + len(cfg.input_features[\"race_horse\"]) + cfg.emb_dim,\n",
        "            cfg.emb_dim\n",
        "            )\n",
        "        self.embedding_projection = nn.Linear(\n",
        "            cfg.emb_dim + len(cfg.input_features[\"races\"]) + len(cfg.input_features[\"race_horse\"]) + len(cfg.input_features[\"results\"]),\n",
        "            cfg.emb_dim\n",
        "        )\n",
        "\n",
        "        # Graph Convolutional Layers blocks\n",
        "        self.logit_transformers = nn.Sequential(*[\n",
        "            GCNBlock(cfg) for _ in range(cfg.n_layers)\n",
        "        ])\n",
        "        self.embedding_transformers = nn.Sequential(*[\n",
        "            GCNBlock(cfg) for _ in range(cfg.n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layers\n",
        "        self.final_norm = nn.LayerNorm(cfg.emb_dim)\n",
        "        self.final_projection = nn.Linear(cfg.emb_dim, cfg.logits_size)\n",
        "\n",
        "        # Sequence modeling\n",
        "        self.lstm_norm = nn.LayerNorm(cfg.emb_dim)\n",
        "        self.lstm = LSTM_Model(cfg)\n",
        "\n",
        "        self.cfg = cfg\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        race_features = inputs['race_data']\n",
        "        horse_features = inputs['horse_data']\n",
        "        results = inputs['results_data']\n",
        "        hids = inputs['hids']\n",
        "        crids = inputs['crid']\n",
        "\n",
        "        # Retrieve and update horse embeddings\n",
        "        embeddings, seq_lengths, hid_order, node_counts = self.horse_embeddings.get_embeddings(hids)\n",
        "\n",
        "        lstm_output = None\n",
        "\n",
        "        if embeddings is not None:\n",
        "            if self.training:\n",
        "                lstm_output = checkpoint(self._process_lstm, embeddings, seq_lengths, use_reentrant=False)\n",
        "            else:\n",
        "                lstm_output = self._process_lstm(embeddings, seq_lengths)\n",
        "\n",
        "        # Create embedding matrix using vectorized operations\n",
        "        batch_size, num_horses = hids.shape\n",
        "        horse_embeddings = self._create_horse_embeddings_matrix(\n",
        "            hids, lstm_output, hid_order, seq_lengths, batch_size, num_horses\n",
        "        )\n",
        "\n",
        "        # Prepare transformer inputs\n",
        "        logit_input = torch.cat([race_features, horse_features, horse_embeddings], dim=-1)\n",
        "        embed_input = torch.cat([race_features, horse_features, horse_embeddings, results], dim=-1)\n",
        "\n",
        "        # Process through transformers\n",
        "        logit_input = self._process_layer(self.logit_projection, logit_input)\n",
        "        embed_input = self._process_layer(self.embedding_projection, embed_input)\n",
        "\n",
        "        logit_output, _ = self._process_transformers(self.logit_transformers, logit_input, hids)\n",
        "        embed_output, _ = self._process_transformers(self.embedding_transformers, embed_input, hids)\n",
        "\n",
        "        # Generate final predictions\n",
        "        logits = self.final_projection(self.final_norm(logit_output))\n",
        "        self.horse_embeddings.update_embeddings(hids, embed_output, node_counts)\n",
        "\n",
        "        return logits, node_counts\n",
        "\n",
        "    def _process_lstm(self, embeddings, lengths):\n",
        "        return self.lstm(self.lstm_norm(embeddings), lengths)\n",
        "\n",
        "    def _create_horse_embeddings_matrix(self, hids, lstm_output, hid_order, seq_lengths,\n",
        "                                      batch_size, num_horses):\n",
        "        device = hids.device\n",
        "        embeddings = torch.zeros((batch_size, num_horses, self.cfg.emb_dim), device=device)\n",
        "\n",
        "        # Create lookup for valid hids\n",
        "        valid_hids = (hids != -1)\n",
        "        hid_lookup = {hid: idx for idx, hid in enumerate(hid_order)}\n",
        "\n",
        "        # Vectorized embedding assignment\n",
        "        with torch.no_grad():\n",
        "            index_hid_order = 0\n",
        "            hid_indices = torch.full_like(hids, -1, dtype=torch.long, device=device)\n",
        "            for i in range(batch_size):\n",
        "                for j in range(num_horses):\n",
        "                    if valid_hids[i,j] and hids[i,j].item() in hid_lookup:\n",
        "                        hid_indices[i,j] = hid_lookup[hids[i,j].item()]\n",
        "\n",
        "        valid_mask = hid_indices != -1\n",
        "        if valid_mask.any():\n",
        "            seq_lengths.to(device)\n",
        "\n",
        "            seq_indices = seq_lengths[hid_indices[valid_mask]].to(device) - 1\n",
        "            embeddings[valid_mask] = lstm_output[hid_indices[valid_mask], seq_indices]\n",
        "\n",
        "        # Handle first-time hids\n",
        "        first_time_mask = valid_hids & ~valid_mask\n",
        "        embeddings[first_time_mask] = self.initial_embedding\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def _process_layer(self, layer, x):\n",
        "        return checkpoint(layer, x, use_reentrant=True) if self.training else layer(x)\n",
        "\n",
        "    def _process_transformers(self, transformer, x, hids):\n",
        "        return checkpoint(transformer, ((x, hids)), use_reentrant=False) if self.training else transformer((x, hids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXXRFHpFfU6o"
      },
      "source": [
        "### Training helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7pYrqK0k2D3"
      },
      "outputs": [],
      "source": [
        "class TrainingConfig:\n",
        "    \"\"\"Validated training configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        required_keys = {\n",
        "            'optimizer', 'loss_function', 'model_folder',\n",
        "            'initial_lr', 'data', 'model'\n",
        "        }\n",
        "        missing = required_keys - set(config.keys())\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing config keys: {missing}\")\n",
        "\n",
        "        self.__dict__.update(config)\n",
        "\n",
        "        self._validate_config(config)\n",
        "        self.optimizer = config['optimizer']\n",
        "        self.loss_function = config['loss_function']\n",
        "        self.model_folder = config['model_folder']\n",
        "        self.initial_lr = config['initial_lr']\n",
        "        self.data = DataConfig(config['data'])\n",
        "        self.model = ModelConfig(config['model'])\n",
        "\n",
        "\n",
        "class DataConfig:\n",
        "    \"\"\"Validated data configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        required_keys = {\n",
        "            'batch_size', 'num_workers', 'shuffle', 'drop_last',\n",
        "            'data_folder', 'n_epochs', 'input_solution'\n",
        "        }\n",
        "        missing = required_keys - set(config.keys())\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing config keys: {missing}\")\n",
        "        self.__dict__.update(config)\n",
        "\n",
        "\n",
        "class ModelConfig:\n",
        "    \"\"\"Validated model configuration container\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        required_keys = {\n",
        "            'emb_dim', 'n_heads',\n",
        "            'n_layers', 'max_nodes', 'max_depth_nodes',\n",
        "            'max_lstm_depth'\n",
        "        }\n",
        "        missing = required_keys - set(config.keys())\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing config keys: {missing}\")\n",
        "        self.__dict__.update(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK_Fj-2ala54"
      },
      "outputs": [],
      "source": [
        "class GradientMonitor:\n",
        "    \"\"\"Monitors and reports gradient statistics\"\"\"\n",
        "    def __init__(self):\n",
        "        self.max_gradients = {}\n",
        "        self.avg_gradients = {}\n",
        "\n",
        "    def update(self, model: nn.Module):\n",
        "        \"\"\"Update gradient statistics\"\"\"\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad = param.grad.detach()\n",
        "                self.max_gradients[name] = torch.max(torch.abs(grad)).item()\n",
        "                self.avg_gradients[name] = torch.mean(torch.abs(grad)).item()\n",
        "\n",
        "    def report(self, frequency: int = 100):\n",
        "        \"\"\"Print gradient summary\"\"\"\n",
        "        if not self.max_gradients:\n",
        "            return\n",
        "\n",
        "        max_grad = max(self.max_gradients.values())\n",
        "        avg_grad = sum(self.avg_gradients.values()) / len(self.avg_gradients)\n",
        "        print(f\"\\nGradient Summary:\")\n",
        "        print(f\"Max Gradient: {max_grad:.4e}, Max Gradients per layer: {self.max_gradients}\")\n",
        "        print(f\"Avg Gradient: {avg_grad:.4e}, Avg Gradient per layer: {self.avg_gradients}\")\n",
        "\n",
        "class TrainingStats:\n",
        "    \"\"\"Handles training statistics collection and reporting\"\"\"\n",
        "    def __init__(self):\n",
        "        self.data = defaultdict(list)\n",
        "        self.metrics = [\n",
        "            'loss', 'total_winnings', 'total_betted',\n",
        "            'predicted_earnings', 'predicted_earnings_greedy',\n",
        "            'hrn', 'accuracy', 'spearman', 'batch_time', 'n_nodes'\n",
        "        ]\n",
        "\n",
        "    def update(self, epoch: int, batch_idx: int,\n",
        "             metrics: Dict[str, float], batch_time: float, nodes_depth: int, lr: float):\n",
        "\n",
        "        \"\"\"Update statistics\"\"\"\n",
        "        self.data['epoch'].append(epoch)\n",
        "        self.data['batch'].append(batch_idx)\n",
        "        self.data['batch_time'].append(batch_time)\n",
        "        self.data['computation_depth'].append(nodes_depth)\n",
        "        self.data['lr'].append(lr)\n",
        "        for metric in self.metrics:\n",
        "            self.data[metric].append(metrics.get(metric, 0.0))\n",
        "\n",
        "    def report(self, window_size: int = 100):\n",
        "        \"\"\"Print formatted training statistics\"\"\"\n",
        "        if len(self.data['loss']) < window_size:\n",
        "            return\n",
        "        print(f\"Epoch: {self.data['epoch'][-1]}, Batch: {self.data['batch'][-1]}, Loss:{round(np.mean(self.data['loss'][-window_size:]),2)}, Betting:{round(np.mean(self.data['predicted_earnings'][-window_size:]),2)}, Greedy Betting:{round(np.mean(self.data['predicted_earnings_greedy'][-window_size:]),2)}, HRN:{round(np.mean(self.data['hrn'][-window_size:]),2)} \")\n",
        "\n",
        "def get_learning_rate(optimizer):\n",
        "    \"\"\"  Get the current learning rate from the optimizer.\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def update_learning_rate(optimizer: torch.optim.Optimizer, stats: TrainingStats,\n",
        "                          patience: int = 10, factor: float = 0.5,\n",
        "                          min_lr: float = 1e-6, window_size: int = 100):\n",
        "    \"\"\"\n",
        "    Updates the learning rate based on the training loss.\n",
        "\n",
        "    Args:\n",
        "        optimizer: The optimizer to update.\n",
        "        stats: The TrainingStats object containing the training loss history.\n",
        "        patience: The number of epochs to wait before reducing the learning rate.\n",
        "        factor: The factor by which to reduce the learning rate.\n",
        "        min_lr: The minimum learning rate.\n",
        "        window_size: The size of the window to consider for the average loss.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if enough data is available\n",
        "    if len(stats.data['loss']) < window_size + patience:\n",
        "        return\n",
        "\n",
        "    # Calculate average loss over the last 'window_size' epochs\n",
        "    current_loss = np.mean(stats.data['loss'][-window_size:])\n",
        "\n",
        "    # Calculate average loss 'patience' epochs ago\n",
        "    previous_loss = np.mean(stats.data['loss'][-(window_size + patience):-patience])\n",
        "\n",
        "    # If loss has not improved, reduce learning rate\n",
        "    if current_loss >= previous_loss:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = max(param_group['lr'] * factor, min_lr)\n",
        "            print(f\"Learning rate reduced to: {param_group['lr']:.6f}\")\n",
        "\n",
        "\n",
        "def model_forward(model: nn.Module, batch: Dict):\n",
        "    \"\"\"Forward pass through the model\"\"\"\n",
        "    race_data, horse_data, results_data, position, decimalPrice, crids, hids, hrn = batch\n",
        "    return model(batch)\n",
        "\n",
        "def compute_loss(model: nn.Module, criterion: nn.Module, outputs: Tuple, batch: Tuple):\n",
        "    \"\"\"Compute loss and metrics\"\"\"\n",
        "\n",
        "    # race_data, horse_data, results_data, position, decimalPrice, crids, hids, hrn = batch\n",
        "    position = batch['positions']\n",
        "    decimalPrice = batch['prices']\n",
        "    hrn = batch['hrn']\n",
        "    hids = batch['hids']\n",
        "    crids = batch['crid']\n",
        "\n",
        "    loss, accuracy, avg_pred_earnings, avg_greedy_earnings, avg_betted, avg_winnings, avg_spearman = criterion(outputs, decimalPrice, position, crids)\n",
        "\n",
        "    n_nodes_computation_graph =  number_nodes_computation_graph(model, loss)\n",
        "\n",
        "    metrics = {\n",
        "        'loss': loss.item(),\n",
        "        'total_winnings': avg_winnings.item(),\n",
        "        'total_betted': avg_betted.item(),\n",
        "        'predicted_earnings': avg_pred_earnings.item(),\n",
        "        'predicted_earnings_greedy': avg_greedy_earnings.item(),\n",
        "        'accuracy': accuracy.item(),\n",
        "        'spearman': avg_spearman.item(),\n",
        "        'n_nodes': n_nodes_computation_graph,\n",
        "        'hrn': hrn[hrn != -1].float().mean().item(),\n",
        "    }\n",
        "\n",
        "    return loss, metrics\n",
        "\n",
        "def create_optimizer(model: nn.Module, cfg: TrainingConfig) -> torch.optim.Optimizer:\n",
        "    \"\"\"Factory function for optimizers\"\"\"\n",
        "    optimizers = {\n",
        "        'Adam': torch.optim.Adam,\n",
        "        'SGD': torch.optim.SGD\n",
        "    }\n",
        "    try:\n",
        "        return optimizers[cfg.optimizer](model.parameters(), lr=cfg.initial_lr)\n",
        "    except KeyError:\n",
        "        raise ValueError(f\"Unsupported optimizer: {cfg.optimizer}\")\n",
        "\n",
        "\n",
        "def handle_gradients(model: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "                   grad_monitor: GradientMonitor, batch_idx: int):\n",
        "    \"\"\"Handle gradient updates and clipping\"\"\"\n",
        "    grad_monitor.update(model)\n",
        "\n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(\n",
        "        model.parameters(),\n",
        "        max_norm=2.0,\n",
        "        error_if_nonfinite=True\n",
        "    )\n",
        "\n",
        "    # Gradient reporting\n",
        "    if batch_idx % 500 == 0:\n",
        "        grad_monitor.report(100)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "def handle_batch_operations(batch_idx: int, model: nn.Module,\n",
        "                          optimizer: torch.optim.Optimizer, stats: TrainingStats,\n",
        "                          grad_monitor: GradientMonitor, model_folder: str, loss: torch.Tensor):\n",
        "    \"\"\"Handle periodic batch operations\"\"\"\n",
        "    # Visualization\n",
        "    if batch_idx % 100 == 0 and batch_idx < -1 and False:\n",
        "        visualize_computation_graph(model, loss, model_folder, batch_idx)\n",
        "\n",
        "    # Reporting\n",
        "    if batch_idx % 10 == 0 and batch_idx > 0 and False:\n",
        "        stats.report()\n",
        "\n",
        "    # Display Graphs\n",
        "    if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "        plot_training_stats(stats.data)\n",
        "\n",
        "    # Checkpointing\n",
        "    if batch_idx % 1000 == 0 and batch_idx > 0:\n",
        "        model_path = os.path.join(model_folder, f\"model_{batch_idx}.pt\")\n",
        "        save_training_checkpoint(model_folder, batch_idx, model, optimizer, stats.data)\n",
        "        gc.collect()\n",
        "\n",
        "def number_nodes_computation_graph(model: nn.Module, loss: float) -> int:\n",
        "    try:\n",
        "        # dot = make_dot(loss, params=dict(model.named_parameters()),\n",
        "        #              show_attrs=False, show_saved=False)\n",
        "        # return len(dot.source)\n",
        "        return 0\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to count number of nodes in computation graph: {e}\")\n",
        "\n",
        "\n",
        "def visualize_computation_graph(model: nn.Module, loss: float,\n",
        "                              save_path: str, batch_idx: int):\n",
        "    \"\"\"Save computation graph visualization\"\"\"\n",
        "    try:\n",
        "\n",
        "        dot = make_dot(loss, params=dict(model.named_parameters()),\n",
        "                     show_attrs=False, show_saved=False)\n",
        "        dot.render(os.path.join(save_path, f\"graph_{batch_idx}\"), format=\"png\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save computation graph: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def create_directory_training_session(cfg: Any = None) -> str:\n",
        "    \"\"\"\n",
        "    Creates a new directory for storing training checkpoints and logs.\n",
        "    Also saves the training configuration as a JSON file.\n",
        "\n",
        "    Args:\n",
        "        path: Base path where the directory should be created\n",
        "        cfg: Configuration object (class) containing training settings\n",
        "\n",
        "    Returns:\n",
        "        The full path of the created directory\n",
        "    \"\"\"\n",
        "    try:\n",
        "      path = cfg.model_folder\n",
        "\n",
        "      # Create a timestamped directory name\n",
        "      timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "      session_dir = os.path.join(path, f\"training_session_{timestamp}\")\n",
        "\n",
        "      # Create the directory\n",
        "      os.makedirs(session_dir, exist_ok=True)\n",
        "\n",
        "      # Create subdirectories\n",
        "      os.makedirs(os.path.join(session_dir, \"checkpoints\"), exist_ok=True)\n",
        "      os.makedirs(os.path.join(session_dir, \"logs\"), exist_ok=True)\n",
        "\n",
        "      # Save configuration if provided\n",
        "      if cfg is not None:\n",
        "          # Convert cfg to dictionary if it's not already\n",
        "          if not isinstance(cfg, dict):\n",
        "              try:\n",
        "                  cfg_dict = vars(cfg)  # Try to convert class to dict\n",
        "\n",
        "                  # Replace the function with its name\n",
        "                  cfg_dict['loss_function'] = cfg_dict['loss_function'].__name__\n",
        "\n",
        "                  # Convert DataConfig and ModelConfig to dictionaries\n",
        "                  cfg_dict['data'] = vars(cfg_dict['data']) # Convert DataConfig to dictionary\n",
        "                  cfg_dict['model'] = vars(cfg_dict['model']) # Convert ModelConfig to dictionary\n",
        "\n",
        "              except TypeError:\n",
        "                  cfg_dict = {k: getattr(cfg, k) for k in dir(cfg) if not k.startswith('_')}\n",
        "          else:\n",
        "              cfg_dict = cfg\n",
        "\n",
        "          # Save as JSON\n",
        "          config_path = os.path.join(session_dir, \"training_config.json\")\n",
        "          with open(config_path, 'w') as f:\n",
        "              json.dump(cfg_dict, f, indent=4)\n",
        "\n",
        "      return session_dir\n",
        "\n",
        "    except OSError as e:\n",
        "        print(f\"Error creating directory: {e}\")\n",
        "        raise\n",
        "\n",
        "def move_to_device(batch: Dict, device: torch.device) -> Tuple:\n",
        "    \"\"\"Move batch tensors to specified device\"\"\"\n",
        "    return{k: batch[k].to(device) if isinstance(batch[k], torch.Tensor) else batch[k] for k in batch}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8SxtYc4fZ0h"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cs40jDxSTDbG",
        "outputId": "2a17bb92-5b33-4909-a61e-4d8b7a61972d"
      },
      "outputs": [],
      "source": [
        "def training(training_cfg: Dict[str, Any]):\n",
        "    \"\"\"Enhanced training procedure with robust error handling\"\"\"\n",
        "\n",
        "    cfg = TrainingConfig(training_cfg)\n",
        "    device = get_device()\n",
        "\n",
        "    # Data loading\n",
        "    dataset = HorseRacingDataset(cfg.data)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=cfg.data.batch_size,\n",
        "        shuffle=cfg.data.shuffle,\n",
        "        collate_fn=lambda b: collate_fn(b, dataset),\n",
        "        num_workers=cfg.data.num_workers,\n",
        "        drop_last=cfg.data.drop_last\n",
        "    )\n",
        "\n",
        "    # Model initialization\n",
        "    cfg.model.input_features = dataset.features\n",
        "    model = HorseRacingModel(cfg.model).to(device)\n",
        "    optimizer = create_optimizer(model, cfg)\n",
        "    criterion = cfg.loss_function\n",
        "\n",
        "    # Training state\n",
        "    stats = TrainingStats()\n",
        "    grad_monitor = GradientMonitor()\n",
        "    session_dir = create_directory_training_session(copy.deepcopy(cfg))\n",
        "    cfg.model_folder = session_dir\n",
        "    for epoch in range(cfg.data.n_epochs):\n",
        "        model.train()\n",
        "        model.horse_embeddings.reset_embeddings()\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training Epoch\")):\n",
        "\n",
        "            batch = move_to_device(batch, device)\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs, nodes_depth = model_forward(model, batch)\n",
        "            loss, metrics = compute_loss(model, criterion, outputs, batch)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward(retain_graph = True)\n",
        "            handle_gradients(model, optimizer, grad_monitor, batch_idx)\n",
        "\n",
        "            # Update statistics\n",
        "            stats.update(epoch, batch_idx, metrics, time.time() - start_time, nodes_depth, get_learning_rate(optimizer))\n",
        "            update_learning_rate(optimizer, stats)\n",
        "\n",
        "            # Batch operations\n",
        "            handle_batch_operations(batch_idx, model, optimizer, stats, grad_monitor, cfg.model_folder, loss)\n",
        "\n",
        "\n",
        "\n",
        "data_config = {\n",
        "    'batch_size': 8,\n",
        "    'num_workers': 0,\n",
        "    'shuffle': False,\n",
        "    'drop_last': True,\n",
        "    'data_folder': \"/content/drive/My Drive/HorseRacing/Horse riding/data/clean data/\",\n",
        "    'max_crids': 100000000,\n",
        "    'start_crid': 0,\n",
        "    'input_solution': False,\n",
        "    'n_epochs': 1,\n",
        "    'cols_races_to_drop': [],\n",
        "    'cols_results_to_drop': [],\n",
        "    'cols_horses_to_drop': ['decimalPrice', 'isFav']\n",
        "}\n",
        "model_cfg = {\n",
        "    'emb_dim': 256,\n",
        "    'n_heads': 1,\n",
        "    'n_layers': 2,\n",
        "    'max_lstm_depth': 10,\n",
        "    'max_nodes': 1000,\n",
        "    'max_depth_nodes': 100,\n",
        "    'logits_size': 1\n",
        " }\n",
        "\n",
        "training_config = {\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"loss_function\": loss_function_first_horse_classification,\n",
        "    \"model_folder\": \"/content/drive/My Drive/HorseRacing/Horse riding/new_models/\",\n",
        "    \"initial_lr\": 0.0001,\n",
        "    \"data\":data_config,\n",
        "    \"model\":model_cfg\n",
        "}\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "with torch.autograd.set_detect_anomaly(False):\n",
        "  training(training_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUJm0nOAcayZ"
      },
      "source": [
        "### TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "vhbH3mIStgIz",
        "outputId": "74509bb9-3a62-4448-939e-361e7eed8ae3"
      },
      "outputs": [],
      "source": [
        "class TestStats:\n",
        "    \"\"\"Track testing metrics and predictions\"\"\"\n",
        "    def __init__(self):\n",
        "        self.data = defaultdict(list)\n",
        "        self.metrics = [\n",
        "            'loss', 'total_winnings', 'total_betted',\n",
        "            'predicted_earnings', 'predicted_earnings_greedy',\n",
        "            'hrn', 'accuracy', 'spearman', 'batch_time', 'n_nodes'\n",
        "        ]\n",
        "        self.df_predictions = pd.DataFrame(columns=[\n",
        "            'hid', 'crid', 'position', 'decimalPrice'] +\n",
        "            [f'pred_{i+1}' for i in range(40)]\n",
        "        )\n",
        "\n",
        "\n",
        "    def update(self, batch_idx: int,\n",
        "             metrics: Dict[str, float], batch_time: float,\n",
        "             nodes_depth: int):\n",
        "        \"\"\"Update statistics with proper tensor handling\"\"\"\n",
        "        self.data['batch'].append(batch_idx)\n",
        "        self.data['batch_time'].append(batch_time)\n",
        "        self.data['n_nodes'].append(nodes_depth)\n",
        "\n",
        "        # Handle both tensor and float values\n",
        "        for metric in self.metrics:\n",
        "            value = metrics.get(metric, 0.0)\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                value = value.item() if value.dim() == 0 else value.cpu().numpy()\n",
        "            self.data[metric].append(float(value))\n",
        "\n",
        "    def report(self, window_size: int = 100):\n",
        "        \"\"\"Print formatted statistics with safety checks\"\"\"\n",
        "        if len(self.data['loss']) < window_size:\n",
        "            return\n",
        "\n",
        "        metrics = {\n",
        "            'loss': np.mean(self.data['loss'][-window_size:]),\n",
        "            'predicted_earnings': np.mean(self.data['predicted_earnings'][-window_size:]),\n",
        "            'predicted_earnings_greedy': np.mean(self.data['predicted_earnings_greedy'][-window_size:]),\n",
        "            'hrn': np.mean(self.data['hrn'][-window_size:])\n",
        "        }\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {self.data['epoch'][-1]}, \"\n",
        "            f\"Batch: {self.data['batch'][-1]}, \"\n",
        "            f\"Loss: {round(metrics['loss'], 2)}, \"\n",
        "            f\"Betting: {round(metrics['predicted_earnings'], 2)}, \"\n",
        "            f\"Greedy Betting: {round(metrics['predicted_earnings_greedy'], 2)}, \"\n",
        "            f\"HRN: {round(metrics['hrn'], 2)}\"\n",
        "        )\n",
        "\n",
        "    def update_results_df(self, predictions: torch.Tensor, batch: Dict[str, torch.Tensor]):\n",
        "        \"\"\"\n",
        "        Update predictions dataframe with proper tensor handling and masking\n",
        "        \"\"\"\n",
        "        # Convert tensors to numpy arrays\n",
        "        batch_size, num_tokens, _ = predictions.shape\n",
        "        device = predictions.device\n",
        "\n",
        "        # Extract batch data with device awareness\n",
        "        hids = batch['hids'].cpu().numpy()\n",
        "        crids = batch['crid'].cpu().numpy()\n",
        "        positions = batch['positions'].cpu().numpy()\n",
        "        prices = batch['prices'].cpu().numpy()\n",
        "        predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "        # Reshape arrays\n",
        "        predictions_flat = predictions.reshape(-1, 40)\n",
        "        hids_flat = hids.reshape(-1)\n",
        "        crids_flat = crids.reshape(-1)\n",
        "        positions_flat = positions.reshape(-1)\n",
        "        prices_flat = prices.reshape(-1)\n",
        "\n",
        "        # Create mask for valid hids\n",
        "        valid_mask = hids_flat != -1\n",
        "\n",
        "        # Create temporary dataframe\n",
        "        temp_df = pd.DataFrame({\n",
        "            'hid': hids_flat[valid_mask],\n",
        "            'crid': crids_flat[valid_mask],\n",
        "            'position': positions_flat[valid_mask],\n",
        "            'decimalPrice': prices_flat[valid_mask],\n",
        "        })\n",
        "\n",
        "        # Add prediction columns\n",
        "        pred_columns = [f'pred_{i+1}' for i in range(40)]\n",
        "        temp_df[pred_columns] = predictions_flat[valid_mask]\n",
        "\n",
        "        # Update main dataframe\n",
        "        self.df_predictions = pd.concat(\n",
        "            [self.df_predictions, temp_df],\n",
        "            ignore_index=True\n",
        "        )\n",
        "\n",
        "    def save_results_df(self, folder: str):\n",
        "        \"\"\"Save predictions dataframe to CSV\"\"\"\n",
        "        path = os.path.join(folder, f\"df_predictions_{len(self.df_predictions)}.csv\")\n",
        "        self.df_predictions.to_csv(path, index=False)\n",
        "        print(f\"Predictions saved to {path}\")\n",
        "\n",
        "def testing(test_cfg: Dict[str, Any], checkpoint_path: str):\n",
        "    \"\"\"Enhanced testing procedure with metrics tracking and model loading\"\"\"\n",
        "\n",
        "    cfg = TrainingConfig(test_cfg)\n",
        "    device = get_device()\n",
        "    os.makedirs(cfg.save_results_folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Data loading (should use test split in practice)\n",
        "        dataset = HorseRacingDataset(cfg.data)\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=cfg.data.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda b: collate_fn(b, dataset),\n",
        "            num_workers=cfg.data.num_workers,\n",
        "            drop_last=False\n",
        "        )\n",
        "\n",
        "        # Model initialization\n",
        "        cfg.model.input_features = dataset.features\n",
        "        model = HorseRacingModel(cfg.model).to(device)\n",
        "\n",
        "        # Load trained model\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            raise FileNotFoundError(f\"Checkpoint {checkpoint_path} not found\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        # Testing state\n",
        "        stats = TestStats()\n",
        "        criterion = cfg.loss_function\n",
        "\n",
        "        model.eval()\n",
        "        model.horse_embeddings.reset_embeddings()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Testing\")):\n",
        "                batch = move_to_device(batch, device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs, nodes_depth = model_forward(model, batch)\n",
        "\n",
        "                loss, metrics = compute_loss(model, criterion, outputs, batch)\n",
        "\n",
        "                stats.update_results_df(outputs, batch)\n",
        "\n",
        "                if batch_idx % 1000 == 0:\n",
        "                  stats.save_results_df(cfg.save_results_folder)\n",
        "\n",
        "        return stats\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Testing failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "data_config = {\n",
        "        \"batch_size\": 8,\n",
        "        \"num_workers\": 0,\n",
        "        \"shuffle\": False,\n",
        "        \"drop_last\": True,\n",
        "        \"data_folder\": \"/content/drive/My Drive/HorseRacing/Horse riding/data/clean data/\",\n",
        "        \"max_crids\": 100000000,\n",
        "        \"input_solution\": False,\n",
        "        \"n_epochs\": 1,\n",
        "        \"cols_races_to_drop\": [],\n",
        "        \"cols_results_to_drop\": [],\n",
        "        \"cols_horses_to_drop\": [\n",
        "            \"decimalPrice\",\n",
        "            \"isFav\"\n",
        "        ]\n",
        "}\n",
        "model_cfg = {\n",
        "        \"emb_dim\": 256,\n",
        "        \"n_heads\": 1,\n",
        "        \"n_layers\": 6,\n",
        "        \"max_lstm_depth\": 15,\n",
        "        \"max_nodes\": 1000,\n",
        "        \"max_depth_nodes\": 100,\n",
        "}\n",
        "test_config = {\n",
        "    \"loss_function\": loss_function_classificationV2,\n",
        "    \"data\":data_config,\n",
        "    \"model\":model_cfg,\n",
        "    \"save_results_folder\": \"/content/drive/My Drive/HorseRacing/Horse riding/save_results_testing/test_1\",\n",
        "}\n",
        "checkpoint_path=\"/content/drive/My Drive/HorseRacing/Horse riding/new_models/training_session_20250325_081150/checkpoints/checkpoint_3000.pt\"\n",
        "stats = testing_results = testing(\n",
        "    test_cfg=test_config,\n",
        "    checkpoint_path=checkpoint_path\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
